{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "effd2a7a",
   "metadata": {},
   "source": [
    "\n",
    "## Mixture of Gaussians Dataset\n",
    "\n",
    "Develop a toy dataset of $m=1000$ sample points for Mixture of Gaussians (MoG):\n",
    "\n",
    "- Feature dimensions: 2\n",
    "- Number of Gaussian components: 3\n",
    "- Means: random.\n",
    "- Covariance matrices: diagonal.\n",
    "- Create visualizations for  dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4405a249",
   "metadata": {},
   "source": [
    "\n",
    "## Gradient Formulas\n",
    "\n",
    "We consider a 2-component Mixture of Gaussians (MoG) with 1-dimensional data. Show that the gradients you need for solving the estimation problem are as follows. Use Latex math in markdown format for this task. \n",
    "\n",
    "The likelihood for a point $x_i$ is\n",
    "\n",
    "$$\n",
    "p(x_i  \\mid \\pi, \\mu, \\sigma^2) =\n",
    "\\pi_1 \\, \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2)\n",
    "+ \\pi_2 \\, \\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2).\n",
    "$$\n",
    "\n",
    "The log-likelihood for $m$ data points is\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{i=1}^m \\log \\left(\n",
    "\\sum_{k=1}^2 \\pi_k \\, \\mathcal{N}(x_i \\mid \\mu_k, \\sigma_k^2)\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "Define the responsibility of component $k$ for data point $i$:\n",
    "\n",
    "$$\n",
    "\\gamma_{ik} =\n",
    "\\frac{ \\pi_k \\, \\mathcal{N}(x_i \\mid \\mu_k, \\sigma_k^2) }\n",
    "     { \\sum_{j=1}^2 \\pi_j \\, \\mathcal{N}(x_i \\mid \\mu_j, \\sigma_j^2) }.\n",
    "$$\n",
    "\n",
    "Then the gradients are:\n",
    "\n",
    "- **Mean gradient**:\n",
    "  $$\n",
    "  \\frac{\\partial \\mathcal{L}}{\\partial \\mu_k}\n",
    "  = \\sum_{i=1}^m \\gamma_{ik} \\, \\frac{(x_i - \\mu_k)}{\\sigma_k^2}.\n",
    "  $$\n",
    "\n",
    "- **Variance gradient**:\n",
    "  $$\n",
    "  \\frac{\\partial \\mathcal{L}}{\\partial \\sigma_k^2}\n",
    "  = \\frac{1}{2} \\sum_{i=1}^m \\gamma_{ik}\n",
    "    \\left[ \\frac{(x_i - \\mu_k)^2}{\\sigma_k^4} - \\frac{1}{\\sigma_k^2} \\right].\n",
    "  $$\n",
    "\n",
    "- **Mixture weights gradient** (with constraint $\\sum_k \\pi_k = 1$):\n",
    "  $$\n",
    "  \\frac{\\partial \\mathcal{L}}{\\partial \\pi_k}\n",
    "  = \\sum_{i=1}^m \\frac{\\gamma_{ik}}{\\pi_k}.\n",
    "  $$\n",
    "\n",
    "## SGD from Scratch for 3-Component, 3-Feature MoG \n",
    "\n",
    "Implement Stochastic Gradient Descent (SGD) from scratch with the Negative Log-Likelihood (NLL) objective and analytic derivatives to optimize the parameters.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Initialize covariance matrices as diagonal.\n",
    "- For a mini-batch $B$, provide expressions for the gradients below.\n",
    "- Re-parameterize variance as $\\log \\sigma$ to keep the model stable and avoid invalid variances while applying SGD.\n",
    "\n",
    "Provide and use the following in your write-up/code:\n",
    "\n",
    "## Responsibility function\n",
    "\n",
    "The responsibility function for component $k$ and data point $i$:\n",
    "\n",
    "$$\n",
    "\\gamma_{ik} =\n",
    "\\frac{ \\pi_k \\, \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) }\n",
    "     { \\sum_{j=1}^K \\pi_j \\, \\mathcal{N}(x_i \\mid \\mu_j, \\Sigma_j) }.\n",
    "$$\n",
    "\n",
    "## Mean gradient\n",
    "\n",
    "For a mini-batch $B$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_B}{\\partial \\mu_k}\n",
    "= \\sum_{i \\in B} \\gamma_{ik} \\, \\Sigma_k^{-1} (x_i - \\mu_k).\n",
    "$$\n",
    "\n",
    "## Variance gradient\n",
    "\n",
    "For diagonal covariance matrices:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_B}{\\partial \\log \\sigma_{kd}^2}\n",
    "= \\frac{1}{2} \\sum_{i \\in B} \\gamma_{ik}\n",
    "  \\left[ \\frac{(x_{id} - \\mu_{kd})^2}{\\sigma_{kd}^2} - 1 \\right],\n",
    "$$\n",
    "\n",
    "where $d$ indexes the feature dimension.\n",
    "\n",
    "## Mixture weights gradient\n",
    "\n",
    "For the mixture weights (with softmax re-parameterization):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_B}{\\partial \\pi_k}\n",
    "= \\sum_{i \\in B} \\frac{\\gamma_{ik}}{\\pi_k}.\n",
    "$$\n",
    "\n",
    "# References and Review Materials\n",
    "\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/6gUdlygtscI\" frameBorder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowFullScreen></iframe>\n",
    "\n",
    "- [Gaussian Mixture Model](https://www.geeksforgeeks.org/machine-learning/gaussian-mixture-model/)\n",
    "- [Lecture notes](https://web.stanford.edu/~lmackey/stats306b/doc/stats306b-spring14-lecture2_scribed.pdf)\n",
    "- [Blog post overview](https://mpatacchiola.github.io/blog/2020/07/31/gaussian-mixture-models.html)\n",
    "\n",
    "\n",
    "# Deliverables Checklist\n",
    "\n",
    "- [ ] Environment screenshot.\n",
    "- [ ] Dataset generation code (2D with 3 components).\n",
    "- [ ] Visualizations of the dataset.\n",
    "- [ ] Derivations for the 2-component, 1-feature MoG gradients.\n",
    "- [ ] SGD implementation (NLL objective, diagonal covariance via log-std parameterization)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}