{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feaf3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the backup tree above and the vector form Bellman equation to understand this code.\n",
    "# Have them side by side while you are reading.\n",
    "# Each of the 11 rows of the \"matrix\" P[s][a] has 4 tuples - one for each of the allowed actions. Each tuple / action is written in the format (probability, s') and is associated with the 3 possible next states that the agent may end up despite its intention to go to the desired state. The states are numbered sequentially from top left to bottom right.\n",
    "import numpy as np\n",
    "from rich import print\n",
    "\n",
    "P = {\n",
    "    0: {\n",
    "        0: [(0.9, 0), (0.1, 1), (0, 4)],\n",
    "        1: [(0.8, 1), (0.1, 4), (0.1, 0)],\n",
    "        2: [(0.8, 4), (0.1, 1), (0.1, 0)],\n",
    "        3: [(0.9, 0), (0.1, 4)],\n",
    "    },\n",
    "    1: {\n",
    "        0: [(0.8, 1), (0.1, 2), (0.1, 0)],\n",
    "        1: [(0.8, 2), (0.2, 1)],\n",
    "        2: [(0.8, 1), (0.1, 0), (0.1, 2)],\n",
    "        3: [(0.8, 0), (0.2, 1)],\n",
    "    },\n",
    "    2: {\n",
    "        0: [(0.8, 2), (0.1, 3), (0.1, 1)],\n",
    "        1: [(0.8, 3), (0.1, 5), (0.1, 2)],\n",
    "        2: [(0.8, 5), (0.1, 1), (0.1, 3)],\n",
    "        3: [(0.8, 1), (0.1, 2), (0.1, 5)],\n",
    "    },\n",
    "    3: {\n",
    "        0: [(0.9, 3), (0.1, 2)],\n",
    "        1: [(0.9, 3), (0.1, 6)],\n",
    "        2: [(0.8, 6), (0.1, 2), (0.1, 3)],\n",
    "        3: [(0.8, 2), (0.1, 3), (0.1, 6)],\n",
    "    },\n",
    "    4: {\n",
    "        0: [(0.8, 0), (0.2, 4)],\n",
    "        1: [(0.8, 4), (0.1, 7), (0.1, 0)],\n",
    "        2: [(0.8, 7), (0.2, 4)],\n",
    "        3: [(0.8, 4), (0.1, 0), (0.1, 7)],\n",
    "    },\n",
    "    5: {\n",
    "        0: [(0.8, 2), (0.1, 6), (0.1, 5)],\n",
    "        1: [(0.8, 6), (0.1, 9), (0.1, 2)],\n",
    "        2: [(0.8, 9), (0.1, 5), (0.1, 6)],\n",
    "        3: [(0.8, 5), (0.1, 2), (0.1, 9)],\n",
    "    },\n",
    "    6: {\n",
    "        0: [(0.8, 3), (0.1, 6), (0.1, 5)],\n",
    "        1: [(0.8, 6), (0.1, 10), (0.1, 3)],\n",
    "        2: [(0.8, 10), (0.1, 5), (0.1, 6)],\n",
    "        3: [(0.8, 5), (0.1, 3), (0.1, 10)],\n",
    "    },\n",
    "    7: {\n",
    "        0: [(0.8, 4), (0.1, 8), (0.1, 7)],\n",
    "        1: [(0.8, 8), (0.1, 7), (0.1, 4)],\n",
    "        2: [(0.9, 7), (0.1, 8)],\n",
    "        3: [(0.9, 7), (0.1, 4)],\n",
    "    },\n",
    "    8: {\n",
    "        0: [(0.8, 8), (0.1, 9), (0.1, 7)],\n",
    "        1: [(0.8, 9), (0.2, 8)],\n",
    "        2: [(0.8, 8), (0.1, 7), (0.1, 9)],\n",
    "        3: [(0.8, 7), (0.2, 8)],\n",
    "    },\n",
    "    9: {\n",
    "        0: [(0.8, 5), (0.1, 10), (0.1, 8)],\n",
    "        1: [(0.8, 9), (0.1, 9), (0.1, 5)],\n",
    "        2: [(0.8, 9), (0.1, 8), (0.1, 10)],\n",
    "        3: [(0.8, 8), (0.1, 5), (0.1, 9)],\n",
    "    },\n",
    "    10: {\n",
    "        0: [(0.8, 6), (0.1, 10), (0.1, 9)],\n",
    "        1: [(0.9, 10), (0.1, 6)],\n",
    "        2: [(0.9, 10), (0.1, 9)],\n",
    "        3: [(0.8, 9), (0.1, 6), (0.1, 10)],\n",
    "    },\n",
    "}\n",
    "\n",
    "R = [0, 0, 0, 1, 0, 0, -1, 0, 0, 0, 0]\n",
    "gamma = 0.9\n",
    "\n",
    "States = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "Actions = [0, 1, 2, 3]  # [east, north, south, west]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7184f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [0] * 11\n",
    "\n",
    "# once v computed, we can calculate the optimal policy\n",
    "optPolicy = [0] * 11\n",
    "\n",
    "# value iteration\n",
    "for i in range(100):\n",
    "    for s in States:\n",
    "        # trans[1] = s'\n",
    "        # trans[0] = P_ss'\n",
    "        q_0 = sum(trans[0] * v[trans[1]] for trans in P[s][0])\n",
    "        q_1 = sum(trans[0] * v[trans[1]] for trans in P[s][1])\n",
    "        q_2 = sum(trans[0] * v[trans[1]] for trans in P[s][2])\n",
    "        q_3 = sum(trans[0] * v[trans[1]] for trans in P[s][3])\n",
    "\n",
    "        v[s] = R[s] + gamma * max(q_0, q_1, q_2, q_3)\n",
    "\n",
    "\n",
    "    for s in States:\n",
    "        optPolicy[s] = int(\n",
    "            np.argmax(\n",
    "                a=[sum([trans[0] * v[trans[1]] for trans in P[s][a]]) for a in Actions]\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(v)\n",
    "print(optPolicy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c87e657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Gridworld parameters \n",
    "rows, cols = 3, 4\n",
    "step_reward = -0.04\n",
    "gamma = 0.9\n",
    "\n",
    "# Terminal states and their rewards (row, col) using 0-indexed rows from top\n",
    "terminal_states = {\n",
    "    (0, 3): 1.0,  # top-right\n",
    "    (1, 3): -1.0,\n",
    "}  # just below top-right\n",
    "walls = {(1, 1)}  # wall at position (row=1, col=1)\n",
    "\n",
    "# Initialize value function\n",
    "V = np.zeros((rows, cols))\n",
    "for (i, j), r in terminal_states.items():\n",
    "    V[i, j] = r\n",
    "\n",
    "# Define actions\n",
    "actions = {\"up\": (-1, 0), \"down\": (1, 0), \"left\": (0, -1), \"right\": (0, 1)}\n",
    "action_list = list(actions.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f1f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_state_reward(state, action):\n",
    "    i, j = state\n",
    "    di, dj = action\n",
    "    ni, nj = i + di, j + dj\n",
    "    # If next state is off-grid or a wall, stay in place\n",
    "    if (ni < 0 or ni >= rows or nj < 0 or nj >= cols) or ((ni, nj) in walls):\n",
    "        return state, step_reward\n",
    "    # If next state is terminal, get its terminal reward\n",
    "    if (ni, nj) in terminal_states:\n",
    "        return (ni, nj), terminal_states[(ni, nj)]\n",
    "    # Otherwise, standard step reward\n",
    "    return (ni, nj), step_reward\n",
    "\n",
    "\n",
    "def transitions(state, action):\n",
    "    # Stochastic model: intended 0.8, perpendicular slips 0.1 each\n",
    "    if action == actions[\"up\"]:\n",
    "        slips = [actions[\"left\"], actions[\"right\"]]\n",
    "    elif action == actions[\"down\"]:\n",
    "        slips = [actions[\"right\"], actions[\"left\"]]\n",
    "    elif action == actions[\"left\"]:\n",
    "        slips = [actions[\"down\"], actions[\"up\"]]\n",
    "    else:  # right\n",
    "        slips = [actions[\"up\"], actions[\"down\"]]\n",
    "    # Gather transitions\n",
    "    results = []\n",
    "    # intended\n",
    "    s1, r1 = next_state_reward(state, action)\n",
    "    results.append((0.8, s1, r1))\n",
    "    # slips / sideways\n",
    "    for slip in slips:\n",
    "        s2, r2 = next_state_reward(state, slip)\n",
    "        results.append((0.1, s2, r2))\n",
    "    return results\n",
    "\n",
    "\n",
    "# Value Iteration\n",
    "theta = 1e-4\n",
    "while True:\n",
    "    delta = 0\n",
    "    newV = V.copy()\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if (i, j) in terminal_states or (i, j) in walls:\n",
    "                continue\n",
    "            q_values = []\n",
    "            for action in action_list:\n",
    "                q = 0\n",
    "                for prob, s_prime, reward in transitions((i, j), action):\n",
    "                    q += prob * (reward + gamma * V[s_prime])\n",
    "                q_values.append(q)\n",
    "            newV[i, j] = max(q_values)\n",
    "            delta = max(delta, abs(newV[i, j] - V[i, j]))\n",
    "    V = newV\n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "# Display the resulting value function\n",
    "df = pd.DataFrame(\n",
    "    V, index=[f\"row{r}\" for r in range(rows)], columns=[f\"col{c}\" for c in range(cols)]\n",
    ")\n",
    "\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}