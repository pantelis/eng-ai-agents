{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49da9da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from minigrid.wrappers import FullyObsWrapper\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "\n",
    "def build_minigrid_model(env):\n",
    "    \"\"\"\n",
    "    Enumerate all reachable fully-observable states in MiniGrid,\n",
    "    then build P[s,a,s'] and R[s,a]\n",
    "    \"\"\"\n",
    "    # wrap to get full grid observation (no partial obs)\n",
    "    env = FullyObsWrapper(env)\n",
    "    # helper to hash an obs dictionary\n",
    "    def obs_key(o):\n",
    "        if isinstance(o, dict):\n",
    "            # For dictionary observations, extract the 'image' part which is the grid\n",
    "            if 'image' in o:\n",
    "                return o['image'].tobytes()\n",
    "            else:\n",
    "                # If no 'image' key, convert dict to a stable string representation\n",
    "                return hashlib.md5(json.dumps(str(o), sort_keys=True).encode()).hexdigest()\n",
    "        else:\n",
    "            # For array observations (older versions)\n",
    "            return o.tobytes()\n",
    "\n",
    "    # BFS over states\n",
    "    state_dicts = []           # index -> env.__dict__ snapshot\n",
    "    obs_to_idx = {}            # obs_key -> index\n",
    "    transitions = {}           # (s,a) -> (s', r, done)\n",
    "\n",
    "    # init\n",
    "    obs, _ = env.reset(seed=0)\n",
    "    idx0 = 0\n",
    "    obs_to_idx[obs_key(obs)] = idx0\n",
    "    state_dicts.append(deepcopy(env.unwrapped.__dict__))\n",
    "    queue = deque([obs])\n",
    "\n",
    "    while queue:\n",
    "        obs = queue.popleft()\n",
    "        s = obs_to_idx[obs_key(obs)]\n",
    "        # restore that state\n",
    "        env.unwrapped.__dict__.update(deepcopy(state_dicts[s]))\n",
    "\n",
    "        for a in range(env.action_space.n):\n",
    "            obs2, r, terminated, truncated, _ = env.step(a)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            key2 = obs_key(obs2)\n",
    "            if key2 not in obs_to_idx:\n",
    "                obs_to_idx[key2] = len(state_dicts)\n",
    "                state_dicts.append(deepcopy(env.unwrapped.__dict__))\n",
    "                queue.append(obs2)\n",
    "\n",
    "            s2 = obs_to_idx[key2]\n",
    "            transitions[(s,a)] = (s2, r, done)\n",
    "\n",
    "            # restore before next action\n",
    "            env.unwrapped.__dict__.update(deepcopy(state_dicts[s]))\n",
    "\n",
    "    nS = len(state_dicts)\n",
    "    nA = env.action_space.n\n",
    "\n",
    "    # build P and R arrays\n",
    "    P = np.zeros((nS, nA, nS), dtype=np.float32)\n",
    "    R = np.zeros((nS, nA), dtype=np.float32)\n",
    "    for (s,a), (s2, r, done) in transitions.items():\n",
    "        if done:\n",
    "            # absorbing: stay in s\n",
    "            P[s,a,s] = 1.0\n",
    "        else:\n",
    "            P[s,a,s2] = 1.0\n",
    "        R[s,a] = r\n",
    "\n",
    "    return P, R, state_dicts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aee28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_policy_linear_minigrid(env, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Under uniform random policy, solve (I - \u03b3 P^\u03c0) v = R^\u03c0 exactly.\n",
    "    \"\"\"\n",
    "    P, R, state_dicts = build_minigrid_model(env)\n",
    "    nS, nA = R.shape\n",
    "\n",
    "    # uniform random policy\n",
    "    pi = np.ones((nS, nA)) / nA\n",
    "\n",
    "    # build P^\u03c0 and R^\u03c0\n",
    "    P_pi = np.einsum('sa,sab->sb', pi, P)  # shape (nS,nS) - fixed indices to avoid repeated output subscript\n",
    "    R_pi = (pi * R).sum(axis=1)            # shape (nS,)\n",
    "\n",
    "    # solve linear system\n",
    "    A = np.eye(nS) - gamma * P_pi\n",
    "    v = np.linalg.solve(A, R_pi)\n",
    "    return v, state_dicts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. create the Minigrid env\n",
    "    env = gym.make(\"MiniGrid-Empty-5x5-v0\")\n",
    "\n",
    "    # 2. solve for v under random policy\n",
    "    v, states = solve_policy_linear_minigrid(env, gamma=0.99)  # Changed from 1.0 to 0.99\n",
    "\n",
    "    # 3. print out values by (pos,dir)\n",
    "    print(\"State-value function for each (x,y,dir):\\n\")\n",
    "    for idx, sdict in enumerate(states):\n",
    "        pos = sdict['agent_pos']\n",
    "        d   = sdict['agent_dir']\n",
    "        print(f\"  s={idx:2d}, pos={pos}, dir={d}:  V = {v[idx]:6.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e164f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from minigrid.wrappers import FullyObsWrapper\n",
    "\n",
    "def policy_eval_iterative(env, policy, discount_factor=0.99, epsilon=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment using iterative policy evaluation.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: MiniGrid environment\n",
    "        discount_factor: Gamma discount factor.\n",
    "        epsilon: Threshold for convergence.\n",
    "    \n",
    "    Returns:\n",
    "        Vector representing the value function.\n",
    "    \"\"\"\n",
    "    # Build model to get state space size and transition probabilities\n",
    "    P, R, state_dicts = build_minigrid_model(env)\n",
    "    nS, nA = R.shape\n",
    "    \n",
    "    # Start with zeros for the value function\n",
    "    V = np.zeros(nS)\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        # For each state, perform a \"full backup\"\n",
    "        for s in range(nS):\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                # For each action, look at the possible next states\n",
    "                for s_next in range(nS):\n",
    "                    # If transition is possible\n",
    "                    if P[s, a, s_next] > 0:\n",
    "                        # Calculate the expected value\n",
    "                        v += action_prob * P[s, a, s_next] * (R[s, a] + discount_factor * V[s_next])\n",
    "            # How much did the value change?\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        # Stop evaluating once our value function change is below threshold\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "            \n",
    "    return V\n",
    "\n",
    "# Create the MiniGrid environment\n",
    "env = gym.make(\"MiniGrid-Empty-5x5-v0\")\n",
    "env = FullyObsWrapper(env)\n",
    "\n",
    "# Get model dimensions\n",
    "P, R, state_dicts = build_minigrid_model(env)\n",
    "nS, nA = R.shape\n",
    "\n",
    "# Create a uniform random policy\n",
    "random_policy = np.ones([nS, nA]) / nA\n",
    "\n",
    "# Evaluate the policy\n",
    "v = policy_eval_iterative(env, random_policy, discount_factor=0.99)\n",
    "\n",
    "# Print results\n",
    "print(\"State-value function using iterative policy evaluation:\\n\")\n",
    "for idx, sdict in enumerate(state_dicts):\n",
    "    pos = sdict['agent_pos']\n",
    "    d   = sdict['agent_dir']\n",
    "    print(f\"  s={idx:2d}, pos={pos}, dir={d}:  V = {v[idx]:6.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e87f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ray\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from minigrid.wrappers import FullyObsWrapper\n",
    "\n",
    "ray.init()  # start Ray (will auto-detect cores)\n",
    "\n",
    "@ray.remote\n",
    "def eval_state(s, V_old, policy_s, P, R, gamma, epsilon):\n",
    "    \"\"\"\n",
    "    Compute the new V[s] for a single state s under `policy_s`\n",
    "    V_old: torch tensor (nS,)\n",
    "    policy_s: torch tensor (nA,)\n",
    "    P: Transition probability array of shape (nS, nA, nS)\n",
    "    R: Reward array of shape (nS, nA)\n",
    "    \"\"\"\n",
    "    v = 0.0\n",
    "    for a, \u03c0 in enumerate(policy_s):\n",
    "        for s_next in range(len(V_old)):\n",
    "            # If transition is possible\n",
    "            if P[s, a, s_next] > 0:\n",
    "                # Calculate expected value\n",
    "                v += \u03c0 * P[s, a, s_next] * (R[s, a] + gamma * V_old[s_next])\n",
    "    return float(v)\n",
    "\n",
    "def policy_eval_ray_minigrid(env, policy, gamma=0.99, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Policy evaluation using Ray for parallel computation in MiniGrid\n",
    "    \"\"\"\n",
    "    # Build model to get transitions and rewards\n",
    "    P, R, state_dicts = build_minigrid_model(env)\n",
    "    nS, nA = R.shape\n",
    "    \n",
    "    # torch tensors for GPU/CPU flexibility\n",
    "    V_old = torch.zeros(nS, dtype=torch.float32)\n",
    "    policy_t = torch.tensor(policy, dtype=torch.float32)\n",
    "\n",
    "    while True:\n",
    "        # launch one task per state\n",
    "        futures = [\n",
    "            eval_state.remote(\n",
    "                s,\n",
    "                V_old,\n",
    "                policy_t[s],\n",
    "                P,\n",
    "                R,\n",
    "                gamma,\n",
    "                epsilon\n",
    "            )\n",
    "            for s in range(nS)\n",
    "        ]\n",
    "        # gather all new V's\n",
    "        V_new_list = ray.get(futures)\n",
    "        V_new = torch.tensor(V_new_list)\n",
    "\n",
    "        # check convergence\n",
    "        if torch.max(torch.abs(V_new - V_old)) < epsilon:\n",
    "            break\n",
    "        V_old = V_new\n",
    "\n",
    "    return V_new\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the MiniGrid environment\n",
    "    env = gym.make(\"MiniGrid-Empty-5x5-v0\")\n",
    "    env = FullyObsWrapper(env)\n",
    "\n",
    "    # Get model dimensions\n",
    "    P, R, state_dicts = build_minigrid_model(env)\n",
    "    nS, nA = R.shape\n",
    "    \n",
    "    # uniform random policy\n",
    "    random_policy = np.ones((nS, nA)) / nA\n",
    "    \n",
    "    # evaluate policy using Ray parallel computation\n",
    "    v = policy_eval_ray_minigrid(env, random_policy, gamma=0.99)\n",
    "    \n",
    "    # Print results in the same format as cell 1\n",
    "    print(\"State-value function using Ray parallel evaluation:\\n\")\n",
    "    for idx, sdict in enumerate(state_dicts):\n",
    "        pos = sdict['agent_pos']\n",
    "        d   = sdict['agent_dir']\n",
    "        print(f\"  s={idx:2d}, pos={pos}, dir={d}:  V = {v[idx]:6.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}