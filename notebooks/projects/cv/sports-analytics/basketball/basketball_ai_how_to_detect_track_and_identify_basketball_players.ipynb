{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (injected by papermill)\n",
    "output_dir = \".\"\n",
    "images_dir = \"./images\"\n",
    "videos_dir = \"./videos\"\n",
    "audio_dir = \"./audio\"\n",
    "text_dir = \"./text\"\n",
    "\n",
    "# Import cv2 for saving supervision images\n",
    "try:\n",
    "    import cv2\n",
    "except ImportError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aS25QDv1a8_W"
   },
   "source": [
    "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
    "\n",
    "# Basketball AI: How to Detect, Track, and Identify Basketball Players\n",
    "\n",
    "---\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb)\n",
    "[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/identify-basketball-players)\n",
    "\n",
    "![identify-basketball-players-2](https://storage.googleapis.com/com-roboflow-marketing/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players-2.png)\n",
    "\n",
    "![identify-basketball-players-1](https://storage.googleapis.com/com-roboflow-marketing/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2O5hUmxdbp0e"
   },
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F8KpVPsbxz1"
   },
   "source": [
    "### Configure your API keys\n",
    "\n",
    "To run this notebook, you need to provide your HuggingFace Token and Roboflow API key.  \n",
    "\n",
    "- The `ROBOFLOW_API_KEY` is required to pull the fine-tuned RF-DETR player detector and the SmolVLM2 number recognizer from Roboflow Universe.  \n",
    "- The `HF_TOKEN` is required to pull the pretrained SigLIP model from HuggingFace.  \n",
    "\n",
    "Follow these steps:  \n",
    "\n",
    "- Open your [`HuggingFace Settings`](https://huggingface.co/settings) page. Click `Access Tokens` then `New Token` to generate a new token.  \n",
    "- Go to your [`Roboflow Settings`](https://app.roboflow.com/settings/api) page. Click `Copy`. This will place your private key in the clipboard.  \n",
    "- In Colab, go to the left pane and click on `Secrets` (\ud83d\udd11).  \n",
    "    - Store the HuggingFace Access Token under the name `HF_TOKEN`.  \n",
    "    - Store the Roboflow API Key under the name `ROBOFLOW_API_KEY`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "id": "xw1ZVBKxbvYM",
    "outputId": "61e2af62-a737-4ddc-b84b-cc089fcdc732"
   },
   "outputs": [
    {
     "ename": "SecretNotFoundError",
     "evalue": "Secret HF_TOKEN does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2837420953.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HF_TOKEN\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HF_TOKEN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ROBOFLOW_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ROBOFLOW_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret HF_TOKEN does not exist."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
    "os.environ[\"ROBOFLOW_API_KEY\"] = userdata.get(\"ROBOFLOW_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfA2MikrcFWL"
   },
   "source": [
    "### Check GPU availability\n",
    "\n",
    "Let's make sure we have access to a GPU. Run the `nvidia-smi` command to verify. If you run into issues, go to `Runtime` -> `Change runtime type`, select `T4 GPU` or `L4 GPU`, and then click `Save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KKyA4lpocCLc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 19 15:29:41 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A4500 Laptop GPU    On  |   00000000:01:00.0  On |                  Off |\n",
      "| N/A   60C    P0             37W /  115W |     309MiB /  16384MiB |     19%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkeEI66XcVgr"
   },
   "source": [
    "**NOTE:** To make it easier for us to manage datasets, images and models we create a `HOME` constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpW8sBtBcHfG",
    "outputId": "828d0e18-0e17-4616-d030-9debafb23578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOME: /workspaces/engineering-ai-agents/aiml-common/assignments/topics/sports-analytics/basketball\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "HOME = Path.cwd()\n",
    "print(\"HOME:\", HOME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJT-DpbFt6Ns"
   },
   "source": [
    "## Install SAM2 real-time\n",
    "\n",
    "We will use `segment-anything-2-real-time`, an open-source fork of Meta\u2019s Segment Anything Model 2 optimized for real-time inference. After installing the repository, we will also download the required checkpoint files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hfPi_Fi2uAu2",
    "outputId": "8c6a2911-fe5a-4926-8783-9e3f35782016"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'segment-anything-2-real-time' already exists and is not an empty directory.\n",
      "/content/segment-anything-2-real-time\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m\u00d7\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
      "  \u001b[31m\u2502\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m\u2570\u2500>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m\u00d7\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
      "\u001b[31m\u2502\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m\u2570\u2500>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "running build_ext\n",
      "W1119 15:22:00.352000 1194 torch/utils/cpp_extension.py:615] Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "W1119 15:22:00.432000 1194 torch/utils/cpp_extension.py:507] The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.6). Most likely this shouldn't be a problem.\n",
      "W1119 15:22:00.432000 1194 torch/utils/cpp_extension.py:517] There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.5\n",
      "building 'sam2._C' extension\n",
      "creating build/temp.linux-x86_64-cpython-312/sam2/csrc\n",
      "W1119 15:22:00.570000 1194 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "W1119 15:22:00.570000 1194 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
      "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.12/dist-packages/torch/include -I/usr/local/lib/python3.12/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.12 -c sam2/csrc/connected_components.cu -o build/temp.linux-x86_64-cpython-312/sam2/csrc/connected_components.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1018\\\" -DTORCH_EXTENSION_NAME=_C -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
      "creating build/lib.linux-x86_64-cpython-312/sam2\n",
      "x86_64-linux-gnu-g++ -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-312/sam2/csrc/connected_components.o -L/usr/local/lib/python3.12/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-312/sam2/_C.so\n",
      "copying build/lib.linux-x86_64-cpython-312/sam2/_C.so -> sam2\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Gy920/segment-anything-2-real-time.git\n",
    "%cd {HOME}/segment-anything-2-real-time\n",
    "!pip install -e . -q\n",
    "!python setup.py build_ext --inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8Uwlni-uzkf",
    "outputId": "59f2bf47-ac6c-4a9c-8de4-152d45badbbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading sam2.1_hiera_tiny.pt checkpoint...\n",
      "--2025-11-19 15:23:21--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_tiny.pt\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 99.84.41.80, 99.84.41.33, 99.84.41.79, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|99.84.41.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 156008466 (149M) [application/vnd.snesdev-page-table]\n",
      "Saving to: \u2018sam2.1_hiera_tiny.pt\u2019\n",
      "\n",
      "sam2.1_hiera_tiny.p 100%[===================>] 148.78M   286MB/s    in 0.5s    \n",
      "\n",
      "2025-11-19 15:23:21 (286 MB/s) - \u2018sam2.1_hiera_tiny.pt\u2019 saved [156008466/156008466]\n",
      "\n",
      "Downloading sam2.1_hiera_small.pt checkpoint...\n",
      "--2025-11-19 15:23:21--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_small.pt\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 99.84.41.80, 99.84.41.33, 99.84.41.79, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|99.84.41.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 184416285 (176M) [application/vnd.snesdev-page-table]\n",
      "Saving to: \u2018sam2.1_hiera_small.pt\u2019\n",
      "\n",
      "sam2.1_hiera_small. 100%[===================>] 175.87M   326MB/s    in 0.5s    \n",
      "\n",
      "2025-11-19 15:23:22 (326 MB/s) - \u2018sam2.1_hiera_small.pt\u2019 saved [184416285/184416285]\n",
      "\n",
      "Downloading sam2.1_hiera_base_plus.pt checkpoint...\n",
      "--2025-11-19 15:23:22--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_base_plus.pt\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 99.84.41.80, 99.84.41.33, 99.84.41.79, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|99.84.41.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 323606802 (309M) [application/vnd.snesdev-page-table]\n",
      "Saving to: \u2018sam2.1_hiera_base_plus.pt\u2019\n",
      "\n",
      "sam2.1_hiera_base_p 100%[===================>] 308.62M   266MB/s    in 1.2s    \n",
      "\n",
      "2025-11-19 15:23:23 (266 MB/s) - \u2018sam2.1_hiera_base_plus.pt\u2019 saved [323606802/323606802]\n",
      "\n",
      "Downloading sam2.1_hiera_large.pt checkpoint...\n",
      "--2025-11-19 15:23:23--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 99.84.41.80, 99.84.41.33, 99.84.41.79, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|99.84.41.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 898083611 (856M) [application/vnd.snesdev-page-table]\n",
      "Saving to: \u2018sam2.1_hiera_large.pt\u2019\n",
      "\n",
      "sam2.1_hiera_large. 100%[===================>] 856.48M   109MB/s    in 5.3s    \n",
      "\n",
      "2025-11-19 15:23:28 (161 MB/s) - \u2018sam2.1_hiera_large.pt\u2019 saved [898083611/898083611]\n",
      "\n",
      "All checkpoints are downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "!(cd checkpoints && bash download_ckpts.sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiTmIfgBcc1e"
   },
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j20VpRlOcXa2",
    "outputId": "53f7ca5a-afdb-4047-c54a-39338f33ca02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/105.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m99.4/99.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m190.1/190.1 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m95.5/95.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m304.3/304.3 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m47.5/47.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m280.8/280.8 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m68.7/68.7 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m135.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m263.3/263.3 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m160.7/160.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m255.6/255.6 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m292.3/292.3 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m212.4/212.4 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m939.7/939.7 kB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m134.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m120.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m304.1/304.1 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m963.8/963.8 kB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for paho-mqtt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pybase64 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pyvips (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
      "bigframes 2.28.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\n",
      "typeguard 4.4.4 requires typing_extensions>=4.14.0, but you have typing-extensions 4.12.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting supervision==0.27.0rc4\n",
      "  Downloading supervision-0.27.0rc4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.12/dist-packages (from supervision==0.27.0rc4) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from supervision==0.27.0rc4) (1.16.3)\n",
      "Requirement already satisfied: matplotlib>=3.6.0 in /usr/local/lib/python3.12/dist-packages (from supervision==0.27.0rc4) (3.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.12/dist-packages (from supervision==0.27.0rc4) (6.0.3)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from supervision==0.27.0rc4) (0.7.1)\n",
      "Requirement already satisfied: pillow>=9.4 in /usr/local/lib/python3.12/dist-packages (from supervision==0.27.0rc4) (11.3.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from supervision==0.27.0rc4) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.3 in /usr/local/lib/python3.12/dist-packages (from supervision==0.27.0rc4) (4.67.1)\n",
      "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.12/dist-packages (from supervision==0.27.0rc4) (4.10.0.84)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision==0.27.0rc4) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision==0.27.0rc4) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision==0.27.0rc4) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision==0.27.0rc4) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision==0.27.0rc4) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision==0.27.0rc4) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision==0.27.0rc4) (2.9.0.post0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->supervision==0.27.0rc4) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->supervision==0.27.0rc4) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->supervision==0.27.0rc4) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->supervision==0.27.0rc4) (2025.10.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision==0.27.0rc4) (1.17.0)\n",
      "Downloading supervision-0.27.0rc4-py3-none-any.whl (209 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m209.8/209.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: supervision\n",
      "  Attempting uninstall: supervision\n",
      "    Found existing installation: supervision 0.27.0\n",
      "    Uninstalling supervision-0.27.0:\n",
      "      Successfully uninstalled supervision-0.27.0\n",
      "Successfully installed supervision-0.27.0rc4\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for sports (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q gdown\n",
    "!pip install -q inference-gpu\n",
    "\n",
    "!pip install supervision==0.27.0rc4\n",
    "!pip install -q git+https://github.com/roboflow/sports.git@feat/basketball\n",
    "\n",
    "!pip install -q transformers num2words\n",
    "!pip install -q flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "102FdCijuQfT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6lzxw4HfRxJ"
   },
   "source": [
    "Set the ONNX Runtime execution provider to CUDA to ensure model inference runs on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rU19OlpfPsG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ONNXRUNTIME_EXECUTION_PROVIDERS\"] = \"[CUDAExecutionProvider]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWkLGOhnVpBG"
   },
   "source": [
    "### Source videos\n",
    "\n",
    "As an example, we will use sample videos from Game 1 of the 2025 NBA Playoffs between the Boston Celtics and the New York Knicks. We prepared 10 sample videos from this game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALpFXOu4cwL3"
   },
   "outputs": [],
   "source": [
    "SOURCE_VIDEO_DIRECTORY = HOME / \"source\"\n",
    "\n",
    "!gdown -q https://drive.google.com/drive/folders/1eDJYqQ77Fytz15tKGdJCMeYSgmoQ-2-H -O {SOURCE_VIDEO_DIRECTORY} --folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50Uq-moPBUBY"
   },
   "outputs": [],
   "source": [
    "!ls -la {SOURCE_VIDEO_DIRECTORY}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5LLKx8dE2se"
   },
   "outputs": [],
   "source": [
    "\n",
    "SOURCE_VIDEO_PATH = SOURCE_VIDEO_DIRECTORY / \"boston-celtics-new-york-knicks-game-1-q1-04.28-04.20.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsHR16EVFBoo"
   },
   "source": [
    "### Team rosters\n",
    "\n",
    "We are preparing player rosters for both teams. We load the official lists that link jersey numbers to player names. These mappings will let us replace detected numbers with real names, making the final analytics clear and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZg88_zS7fIi"
   },
   "outputs": [],
   "source": [
    "TEAM_ROSTERS = {\n",
    "  \"New York Knicks\": {\n",
    "    \"55\": \"Hukporti\",\n",
    "    \"1\": \"Payne\",\n",
    "    \"0\": \"Wright\",\n",
    "    \"11\": \"Brunson\",\n",
    "    \"3\": \"Hart\",\n",
    "    \"32\": \"Towns\",\n",
    "    \"44\": \"Shamet\",\n",
    "    \"25\": \"Bridges\",\n",
    "    \"2\": \"McBride\",\n",
    "    \"23\": \"Robinson\",\n",
    "    \"8\": \"Anunoby\",\n",
    "    \"4\": \"Dadiet\",\n",
    "    \"5\": \"Achiuwa\",\n",
    "    \"13\": \"Kolek\"\n",
    "  },\n",
    "  \"Boston Celtics\": {\n",
    "    \"42\": \"Horford\",\n",
    "    \"55\": \"Scheierman\",\n",
    "    \"9\": \"White\",\n",
    "    \"20\": \"Davison\",\n",
    "    \"7\": \"Brown\",\n",
    "    \"0\": \"Tatum\",\n",
    "    \"27\": \"Walsh\",\n",
    "    \"4\": \"Holiday\",\n",
    "    \"8\": \"Porzingis\",\n",
    "    \"40\": \"Kornet\",\n",
    "    \"88\": \"Queta\",\n",
    "    \"11\": \"Pritchard\",\n",
    "    \"30\": \"Hauser\",\n",
    "    \"12\": \"Craig\",\n",
    "    \"26\": \"Tillman\"\n",
    "  }\n",
    "}\n",
    "\n",
    "TEAM_COLORS = {\n",
    "    \"New York Knicks\": \"#006BB6\",\n",
    "    \"Boston Celtics\": \"#007A33\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-kTisAQYQHM"
   },
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wd1Yv077YSkj"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "from typing import Dict, List, Optional, Union, Iterable, Tuple\n",
    "from operator import itemgetter\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import supervision as sv\n",
    "from inference import get_model\n",
    "from sports import (\n",
    "    clean_paths,\n",
    "    ConsecutiveValueTracker,\n",
    "    TeamClassifier,\n",
    "    MeasurementUnit,\n",
    "    ViewTransformer\n",
    ")\n",
    "from sports.basketball import (\n",
    "    CourtConfiguration,\n",
    "    League,\n",
    "    draw_court,\n",
    "    draw_points_on_court,\n",
    "    draw_paths_on_court\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PupZT1xHhmoG"
   },
   "source": [
    "## Object detection\n",
    "\n",
    "The model used in this notebook detects the following classes: `ball`, `ball-in-basket`, `number`, `player`, `player-in-possession`, `player-jump-shot`, `player-layup-dunk`, `player-shot-block`, `referee`, and `rim`. These classes enable tracking of game events, player actions, and ball location for basketball analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5UNfe7fLvKf"
   },
   "source": [
    "### Load RF-DETR object detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAD4DURmgz-s"
   },
   "outputs": [],
   "source": [
    "PLAYER_DETECTION_MODEL_ID = \"basketball-player-detection-3-ycjdo/4\"\n",
    "PLAYER_DETECTION_MODEL_CONFIDENCE = 0.4\n",
    "PLAYER_DETECTION_MODEL_IOU_THRESHOLD = 0.9\n",
    "PLAYER_DETECTION_MODEL = get_model(model_id=PLAYER_DETECTION_MODEL_ID)\n",
    "\n",
    "COLOR = sv.ColorPalette.from_hex([\n",
    "    \"#ffff00\", \"#ff9b00\", \"#ff66ff\", \"#3399ff\", \"#ff66b2\", \"#ff8080\",\n",
    "    \"#b266ff\", \"#9999ff\", \"#66ffff\", \"#33ff99\", \"#66ff66\", \"#99ff00\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMXt_1FxL2D-"
   },
   "source": [
    "### Single frame object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdasOAlhGVCY"
   },
   "outputs": [],
   "source": [
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = box_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "annotated_frame = label_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "\n",
    "cv2.imwrite(f\"{images_dir}/plot_1.png\", cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "sv.plot_image(annotated_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VzpR_LGEGad"
   },
   "source": [
    "### Keep only \"number\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vftat_boERYg"
   },
   "outputs": [],
   "source": [
    "NUMBER_CLASS_ID = 2\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[detections.class_id == NUMBER_CLASS_ID]\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = box_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "annotated_frame = label_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "\n",
    "cv2.imwrite(f\"{images_dir}/plot_2.png\", cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "sv.plot_image(annotated_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttZNW8qgPLBm"
   },
   "source": [
    "### Keep only player-related classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjM1OuY5POCA"
   },
   "outputs": [],
   "source": [
    "PLAYER_CLASS_IDS = [3, 4, 5, 6, 7] # player, player-in-possession, player-jump-shot, player-layup-dunk, player-shot-block\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = box_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "annotated_frame = label_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "\n",
    "cv2.imwrite(f\"{images_dir}/plot_3.png\", cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "sv.plot_image(annotated_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUZHIvfdNRi1"
   },
   "source": [
    "### Full video object detection\n",
    "\n",
    "We are running RF-DETR across all frames to produce a per-frame sequence of detections. These sequences seed tracking and provide number crops over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4O6W49jNz90"
   },
   "outputs": [],
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-detection{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-detection{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "    detections = sv.Detections.from_inference(result)\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = box_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=detections)\n",
    "    annotated_frame = label_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=detections)\n",
    "    return annotated_frame\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path=TARGET_VIDEO_PATH,\n",
    "    callback=callback,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQ2PGjhHp8I7"
   },
   "outputs": [],
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzfEx2GbQNuO"
   },
   "source": [
    "## Player tracking\n",
    "\n",
    "We are switching from frame-wise boxes to temporal tracks. SAM2 yields per-player masks and stable track IDs that persist through occlusions and re-entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6uqFLygS39e"
   },
   "source": [
    "### Load SAM2 tracking model\n",
    "\n",
    "We are loading a SAM2 checkpoint and config into the camera predictor. The large variant yields the highest quality masks; swap to smaller for speed if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2xcYSWjQVMu"
   },
   "outputs": [],
   "source": [
    "%cd $HOME/segment-anything-2-real-time\n",
    "\n",
    "from sam2.build_sam import build_sam2_camera_predictor\n",
    "\n",
    "SAM2_CHECKPOINT = \"checkpoints/sam2.1_hiera_large.pt\"\n",
    "SAM2_CONFIG = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "predictor = build_sam2_camera_predictor(SAM2_CONFIG, SAM2_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYfdZW-nTg5N"
   },
   "source": [
    "### Full video player tacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csTvkOMoXZU4"
   },
   "source": [
    "We are prompting SAM2 with RF-DETR boxes and tracking across the clip. The callback saves masks, IDs, and visualizations for downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tz_lvB_tQOA4"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "class SAM2Tracker:\n",
    "    def __init__(self, predictor) -> None:\n",
    "        self.predictor = predictor\n",
    "        self._prompted = False\n",
    "\n",
    "    def prompt_first_frame(self, frame: np.ndarray, detections: sv.Detections) -> None:\n",
    "        if len(detections) == 0:\n",
    "            raise ValueError(\"detections must contain at least one box\")\n",
    "\n",
    "        if detections.tracker_id is None:\n",
    "            detections.tracker_id = list(range(1, len(detections) + 1))\n",
    "\n",
    "        with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            self.predictor.load_first_frame(frame)\n",
    "            for xyxy, obj_id in zip(detections.xyxy, detections.tracker_id):\n",
    "                bbox = np.asarray([xyxy], dtype=np.float32)\n",
    "                self.predictor.add_new_prompt(\n",
    "                    frame_idx=0,\n",
    "                    obj_id=int(obj_id),\n",
    "                    bbox=bbox,\n",
    "                )\n",
    "\n",
    "        self._prompted = True\n",
    "\n",
    "    def propagate(self, frame: np.ndarray) -> sv.Detections:\n",
    "        if not self._prompted:\n",
    "            raise RuntimeError(\"Call prompt_first_frame before propagate\")\n",
    "\n",
    "        with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            tracker_ids, mask_logits = self.predictor.track(frame)\n",
    "\n",
    "        tracker_ids = np.asarray(tracker_ids, dtype=np.int32)\n",
    "        masks = (mask_logits > 0.0).cpu().numpy()\n",
    "        masks = np.squeeze(masks).astype(bool)\n",
    "\n",
    "        if masks.ndim == 2:\n",
    "            masks = masks[None, ...]\n",
    "\n",
    "        masks = np.array([\n",
    "            sv.filter_segments_by_distance(mask, relative_distance=0.03, mode=\"edge\")\n",
    "            for mask in masks\n",
    "        ])\n",
    "\n",
    "        xyxy = sv.mask_to_xyxy(masks=masks)\n",
    "        detections = sv.Detections(xyxy=xyxy, mask=masks, tracker_id=tracker_ids)\n",
    "        return detections\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self._prompted = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YcY0uJKOQnr-"
   },
   "outputs": [],
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-mask{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "# define team annotators\n",
    "\n",
    "mask_annotator = sv.MaskAnnotator(\n",
    "    color=COLOR,\n",
    "    color_lookup=sv.ColorLookup.TRACK,\n",
    "    opacity=0.5)\n",
    "box_annotator = sv.BoxAnnotator(\n",
    "    color=COLOR,\n",
    "    color_lookup=sv.ColorLookup.TRACK,\n",
    "    thickness=2\n",
    ")\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = box_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "cv2.imwrite(f\"{images_dir}/plot_4.png\", cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "sv.plot_image(annotated_frame)\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    detections = tracker.propagate(frame)\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = mask_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "    annotated_frame = box_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "    return annotated_frame\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path=TARGET_VIDEO_PATH,\n",
    "    callback=callback,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fmNMnpBb4PX"
   },
   "outputs": [],
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNTHNNatcC_I"
   },
   "source": [
    "## Cluster players into teams\n",
    "\n",
    "EWe are assigning each track to a team without labels. The pipeline uses SigLIP embeddings, UMAP to 3D, then K-means with k=2 for final team IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tx-Z3m9QhPIO"
   },
   "source": [
    "### Collecting training set\n",
    "\n",
    "We are sampling frames at 1 FPS, detecting players, and extracting central crops. Central regions emphasize jersey color and texture while reducing background artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouVXww3JlHSM"
   },
   "outputs": [],
   "source": [
    "STRIDE = 30\n",
    "\n",
    "crops = []\n",
    "\n",
    "for video_path in sv.list_files_with_extensions(SOURCE_VIDEO_DIRECTORY, extensions=[\"mp4\", \"avi\", \"mov\"]):\n",
    "    frame_generator = sv.get_video_frames_generator(source_path=video_path, stride=STRIDE)\n",
    "\n",
    "    for frame in tqdm(frame_generator):\n",
    "\n",
    "        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD, class_agnostic_nms=True)[0]\n",
    "        detections = sv.Detections.from_inference(result)\n",
    "        detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "\n",
    "        boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "        for box in boxes:\n",
    "            crops.append(sv.crop_image(frame, box))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5WyZ4tmlwgQ"
   },
   "outputs": [],
   "source": [
    "sv.plot_images_grid(\n",
    "    images=crops[:100],\n",
    "    grid_size=(10, 10),\n",
    "    size=(10, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDIWnI9NmKI0"
   },
   "source": [
    "### Train and test clustering model\n",
    "\n",
    "We are computing SigLIP embeddings for crops, reducing with UMAP, and fitting K-means. A quick validation confirms separation by uniform appearance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1GEHMzUmO4z"
   },
   "outputs": [],
   "source": [
    "team_classifier = TeamClassifier(device=\"cuda\")\n",
    "team_classifier.fit(crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8rYeP5HmkUj"
   },
   "outputs": [],
   "source": [
    "teams = team_classifier.predict(crops)\n",
    "\n",
    "team_0 = [crop for crop, team in zip(crops, teams) if team == 0]\n",
    "team_1 = [crop for crop, team in zip(crops, teams) if team == 1]\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=team_0[:50],\n",
    "    grid_size=(5, 10),\n",
    "    size=(10, 5)\n",
    ")\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=team_1[:50],\n",
    "    grid_size=(5, 10),\n",
    "    size=(10, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CdLxBkXKOca"
   },
   "source": [
    "### Test clustering model on single video frame\n",
    "\n",
    "We are applying the trained clustering to one frame\u2019s player crops. The output assigns provisional team IDs to confirm the mapping before full-video use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSdtN_9fmw_M"
   },
   "outputs": [],
   "source": [
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD, class_agnostic_nms=True)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "\n",
    "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "crops = [sv.crop_image(frame, box) for box in boxes]\n",
    "teams = np.array(team_classifier.predict(crops))\n",
    "\n",
    "team_0 = [crop for crop, team in zip(crops, teams) if team == 0]\n",
    "team_1 = [crop for crop, team in zip(crops, teams) if team == 1]\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=team_0[:10],\n",
    "    grid_size=(1, 10),\n",
    "    size=(10, 1)\n",
    ")\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=team_1[:10],\n",
    "    grid_size=(1, 10),\n",
    "    size=(10, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMVitdaknPqa"
   },
   "source": [
    "Since we do not control which IDs the clustering algorithm assigns to the teams, after training and testing we must select one of the dictionaries below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cALjgKE-nFwJ"
   },
   "outputs": [],
   "source": [
    "TEAM_NAMES = {\n",
    "    0: \"New York Knicks\",\n",
    "    1: \"Boston Celtics\",\n",
    "}\n",
    "\n",
    "# TEAM_NAMES = {\n",
    "#     0: \"Boston Celtics\",\n",
    "#     1: \"New York Knicks\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBKKa8MKo5j7"
   },
   "source": [
    "### Full video team clustering\n",
    "\n",
    "We are assigning team IDs to tracks once, then reusing them across frames via track IDs. This keeps colors and labels consistent throughout the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bF0IaqNTccu"
   },
   "outputs": [],
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-teams{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# define team annotators\n",
    "\n",
    "team_colors = sv.ColorPalette.from_hex([\n",
    "    TEAM_COLORS[TEAM_NAMES[0]],\n",
    "    TEAM_COLORS[TEAM_NAMES[1]]\n",
    "])\n",
    "\n",
    "team_mask_annotator = sv.MaskAnnotator(\n",
    "    color=team_colors,\n",
    "    opacity=0.5,\n",
    "    color_lookup=sv.ColorLookup.INDEX\n",
    ")\n",
    "\n",
    "team_box_annotator = sv.BoxAnnotator(\n",
    "    color=team_colors,\n",
    "    thickness=2,\n",
    "    color_lookup=sv.ColorLookup.INDEX\n",
    ")\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we determine the team for each player and assign a team ID to every detection\n",
    "\n",
    "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "crops = [sv.crop_image(frame, box) for box in boxes]\n",
    "TEAMS = np.array(team_classifier.predict(crops))\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    detections = tracker.propagate(frame)\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = team_mask_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=detections,\n",
    "        custom_color_lookup=TEAMS[detections.tracker_id - 1]\n",
    "    )\n",
    "    annotated_frame = team_box_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=detections,\n",
    "        custom_color_lookup=TEAMS[detections.tracker_id - 1]\n",
    "    )\n",
    "    return annotated_frame\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path=TARGET_VIDEO_PATH,\n",
    "    callback=callback,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcYjgSQrVQP0"
   },
   "outputs": [],
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0Q-gOy2su5v"
   },
   "source": [
    "## Player numbers OCR\n",
    "\n",
    "We are moving to jersey OCR to identify individuals within each team. Number reads pair with tracks and teams to resolve names later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXiZXnqntShC"
   },
   "source": [
    "### Load number recognition model\n",
    "\n",
    "We are loading the fine-tuned SmolVLM2 OCR model by ID. It was trained on jersey crops and outputs digit strings suitable for downstream validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfgolXlwtep1"
   },
   "outputs": [],
   "source": [
    "NUMBER_RECOGNITION_MODEL_ID = \"basketball-jersey-numbers-ocr/3\"\n",
    "NUMBER_RECOGNITION_MODEL = get_model(model_id=NUMBER_RECOGNITION_MODEL_ID)\n",
    "NUMBER_RECOGNITION_MODEL_PROMPT = \"Read the number.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkM_JQ61s3Oc"
   },
   "source": [
    "### Single frame player number detection and recognition\n",
    "\n",
    "We are detecting number boxes, padding, and cropping. We then run SmolVLM2 on each crop and preview predictions next to the regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ily4KtIdswTB"
   },
   "outputs": [],
   "source": [
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "frame_h, frame_w, *_ = frame.shape\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[detections.class_id == NUMBER_CLASS_ID]\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = box_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "annotated_frame = label_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "\n",
    "cv2.imwrite(f\"{images_dir}/plot_8.png\", cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "sv.plot_image(annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXUi2CR-t4mt"
   },
   "outputs": [],
   "source": [
    "crops = [\n",
    "    sv.resize_image(sv.crop_image(frame, xyxy), resolution_wh=(224, 224))\n",
    "    for xyxy\n",
    "    in sv.clip_boxes(sv.pad_boxes(xyxy=detections.xyxy, px=10, py=10), (frame_w, frame_h))\n",
    "]\n",
    "numbers = [\n",
    "    NUMBER_RECOGNITION_MODEL.predict(crop, NUMBER_RECOGNITION_MODEL_PROMPT)[0]\n",
    "    for crop\n",
    "    in crops\n",
    "]\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=crops[:10],\n",
    "    titles=numbers[:10],\n",
    "    grid_size=(1, 10),\n",
    "    size=(10, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3fw2eXcuyVA"
   },
   "source": [
    "### Single frame player detection with number detection matching\n",
    "\n",
    "We are matching numbers to players using Intersection over Smaller Area. IoS equals 1.0 implies the number lies fully inside the player mask, so we link them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFB3xRaPzVyr"
   },
   "outputs": [],
   "source": [
    "def coords_above_threshold(\n",
    "    matrix: np.ndarray, threshold: float, sort_desc: bool = True\n",
    ") -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Return all (row_index, col_index) where value > threshold.\n",
    "    Rows and columns may repeat.\n",
    "    Optionally sort by value descending.\n",
    "    \"\"\"\n",
    "    A = np.asarray(matrix)\n",
    "    rows, cols = np.where(A > threshold)\n",
    "    pairs = list(zip(rows.tolist(), cols.tolist()))\n",
    "    if sort_desc:\n",
    "        pairs.sort(key=lambda rc: A[rc[0], rc[1]], reverse=True)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wJljIgOVq8D"
   },
   "outputs": [],
   "source": [
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# define team annotators\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=4, color_lookup=sv.ColorLookup.TRACK)\n",
    "\n",
    "player_mask_annotator = sv.MaskAnnotator(color=COLOR.by_idx(3), opacity=0.8, color_lookup=sv.ColorLookup.INDEX)\n",
    "number_mask_annotator = sv.MaskAnnotator(color=COLOR.by_idx(0), opacity=0.8, color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "for index, frame in tqdm(enumerate(frame_generator)):\n",
    "\n",
    "    # we only process the first video frame\n",
    "\n",
    "    if index > 0:\n",
    "        break\n",
    "\n",
    "    frame_h, frame_w, *_ = frame.shape\n",
    "\n",
    "    player_detections = tracker.propagate(frame)\n",
    "\n",
    "    # we use RF-DETR model to detect numbers\n",
    "\n",
    "    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "    number_detections = sv.Detections.from_inference(result)\n",
    "    number_detections = number_detections[number_detections.class_id == NUMBER_CLASS_ID]\n",
    "    number_detections.mask = sv.xyxy_to_mask(boxes=number_detections.xyxy, resolution_wh=(frame_w, frame_h))\n",
    "\n",
    "    # we use mask IoS to match numbers with players\n",
    "\n",
    "    iou = sv.mask_iou_batch(\n",
    "        masks_true=player_detections.mask,\n",
    "        masks_detection=number_detections.mask,\n",
    "        overlap_metric=sv.OverlapMetric.IOS\n",
    "    )\n",
    "\n",
    "    pairs = coords_above_threshold(iou, 0.9)\n",
    "    player_idx, number_idx = zip(*pairs)\n",
    "\n",
    "    # we visualize all the masks\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = player_mask_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=player_detections)\n",
    "    annotated_frame = number_mask_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=number_detections)\n",
    "    cv2.imwrite(f\"{images_dir}/plot_10.png\", cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "    sv.plot_image(annotated_frame)\n",
    "\n",
    "    # we visualize only matched pairs\n",
    "\n",
    "    player_detections = player_detections[np.array(player_idx)]\n",
    "    number_detections = number_detections[np.array(number_idx)]\n",
    "    number_detections.tracker_id = player_detections.tracker_id\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = box_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=player_detections)\n",
    "    annotated_frame = box_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=number_detections)\n",
    "    cv2.imwrite(f\"{images_dir}/plot_10.png\", cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "    sv.plot_image(annotated_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqaXSQBVB5kV"
   },
   "source": [
    "### Validating recognized numbers\n",
    "\n",
    "We are confirming numbers across time using a consecutive-agreement threshold. The validator locks a number to a track only after repeated consistent reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-zjlroYWm-q"
   },
   "outputs": [],
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-validated-numbers{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "number_validator = ConsecutiveValueTracker(n_consecutive=3)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# define team annotators\n",
    "\n",
    "mask_annotator = sv.MaskAnnotator(\n",
    "    color=COLOR,\n",
    "    color_lookup=sv.ColorLookup.TRACK,\n",
    "    opacity=0.7)\n",
    "box_annotator = sv.BoxAnnotator(\n",
    "    color=COLOR,\n",
    "    color_lookup=sv.ColorLookup.TRACK,\n",
    "    thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    color=COLOR,\n",
    "    color_lookup=sv.ColorLookup.TRACK,\n",
    "    text_color=sv.Color.BLACK,\n",
    "    text_scale=0.8)\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    player_detections = tracker.propagate(frame)\n",
    "\n",
    "    # we perform number recognition at specific frame intervals\n",
    "\n",
    "    if index % 5 == 0:\n",
    "        frame_h, frame_w, *_ = frame.shape\n",
    "\n",
    "        # we use RF-DETR model to detect numbers\n",
    "\n",
    "        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "        number_detections = sv.Detections.from_inference(result)\n",
    "        number_detections = number_detections[number_detections.class_id == NUMBER_CLASS_ID]\n",
    "        number_detections.mask = sv.xyxy_to_mask(boxes=number_detections.xyxy, resolution_wh=(frame_w, frame_h))\n",
    "\n",
    "        # we crop out numbers detection and run recognition model\n",
    "\n",
    "        number_crops = [\n",
    "            sv.crop_image(frame, xyxy)\n",
    "            for xyxy\n",
    "            in sv.clip_boxes(sv.pad_boxes(xyxy=number_detections.xyxy, px=10, py=10), (frame_w, frame_h))\n",
    "        ]\n",
    "        numbers = [\n",
    "            NUMBER_RECOGNITION_MODEL.predict(number_crop, NUMBER_RECOGNITION_MODEL_PROMPT)[0]\n",
    "            for number_crop\n",
    "            in number_crops\n",
    "        ]\n",
    "\n",
    "        # we use mask IoS to match numbers with players\n",
    "\n",
    "        iou = sv.mask_iou_batch(\n",
    "            masks_true=player_detections.mask,\n",
    "            masks_detection=number_detections.mask,\n",
    "            overlap_metric=sv.OverlapMetric.IOS\n",
    "        )\n",
    "\n",
    "        pairs = coords_above_threshold(iou, 0.9)\n",
    "\n",
    "        if pairs:\n",
    "\n",
    "            player_idx, number_idx = zip(*pairs)\n",
    "            player_idx = [i + 1 for i in player_idx]\n",
    "            number_idx = list(number_idx)\n",
    "\n",
    "            # we update number_validator state\n",
    "\n",
    "            numbers = [numbers[int(i)] for i in number_idx]\n",
    "            number_validator.update(tracker_ids=player_idx, values=numbers)\n",
    "\n",
    "    # we visualize boxes and masks\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = mask_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=player_detections)\n",
    "    annotated_frame = box_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=player_detections)\n",
    "\n",
    "    # we extract validated numbers\n",
    "\n",
    "    numbers = number_validator.get_validated(tracker_ids=player_detections.tracker_id)\n",
    "\n",
    "    # we visualize numbers\n",
    "\n",
    "    annotated_frame = label_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=player_detections,\n",
    "        labels=numbers)\n",
    "\n",
    "    return annotated_frame\n",
    "\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path=TARGET_VIDEO_PATH,\n",
    "    callback=callback,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyYzxDolX6dR"
   },
   "outputs": [],
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cp9TzWCZOmr9"
   },
   "source": [
    "## Player recognition\n",
    "\n",
    "We are overlaying names, numbers, team colors, and masks for each tracked player. The final render shows stable identities aligned with roster data across the full clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OnWZ2ZKgZJv0"
   },
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/drive/folders/1RBjpI5Xleb58lujeusxH0W5zYMMA4ytO -O {HOME / \"fonts\"} --folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnoBYFhcYfpV"
   },
   "outputs": [],
   "source": [
    "frames_history = []\n",
    "detections_history = []\n",
    "\n",
    "number_validator = ConsecutiveValueTracker(n_consecutive=3)\n",
    "team_validator = ConsecutiveValueTracker(n_consecutive=1)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we determine the team for each player and assign a team ID to every detection\n",
    "\n",
    "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "crops = [sv.crop_image(frame, box) for box in boxes]\n",
    "TEAMS = np.array(team_classifier.predict(crops))\n",
    "\n",
    "team_validator.update(tracker_ids=detections.tracker_id, values=TEAMS)\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "\n",
    "for index, frame in tqdm(enumerate(frame_generator)):\n",
    "    player_detections = tracker.propagate(frame)\n",
    "    frames_history.append(frame)\n",
    "    detections_history.append(player_detections)\n",
    "\n",
    "    # we perform number recognition at specific frame intervals\n",
    "\n",
    "    if index % 5 == 0:\n",
    "        frame_h, frame_w, *_ = frame.shape\n",
    "\n",
    "        # we use RF-DETR model to detect numbers\n",
    "\n",
    "        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "        number_detections = sv.Detections.from_inference(result)\n",
    "        number_detections = number_detections[number_detections.class_id == NUMBER_CLASS_ID]\n",
    "        number_detections.mask = sv.xyxy_to_mask(boxes=number_detections.xyxy, resolution_wh=(frame_w, frame_h))\n",
    "\n",
    "        # we crop out numbers detection and run recognition model\n",
    "\n",
    "        number_crops = [\n",
    "            sv.crop_image(frame, xyxy)\n",
    "            for xyxy\n",
    "            in sv.clip_boxes(sv.pad_boxes(xyxy=number_detections.xyxy, px=10, py=10), (frame_w, frame_h))\n",
    "        ]\n",
    "        numbers = [\n",
    "            NUMBER_RECOGNITION_MODEL.predict(number_crop, NUMBER_RECOGNITION_MODEL_PROMPT)[0]\n",
    "            for number_crop\n",
    "            in number_crops\n",
    "        ]\n",
    "\n",
    "        # we use mask IoS to match numbers with players\n",
    "\n",
    "        iou = sv.mask_iou_batch(\n",
    "            masks_true=player_detections.mask,\n",
    "            masks_detection=number_detections.mask,\n",
    "            overlap_metric=sv.OverlapMetric.IOS\n",
    "        )\n",
    "\n",
    "        pairs = coords_above_threshold(iou, 0.9)\n",
    "\n",
    "        if pairs:\n",
    "\n",
    "            player_idx, number_idx = zip(*pairs)\n",
    "            player_idx = [i + 1 for i in player_idx]\n",
    "            number_idx = list(number_idx)\n",
    "\n",
    "            # we update number_validator state\n",
    "\n",
    "            numbers = [numbers[int(i)] for i in number_idx]\n",
    "            number_validator.update(tracker_ids=player_idx, values=numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1scdRpWZUgd"
   },
   "outputs": [],
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-result{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
    "\n",
    "team_colors = sv.ColorPalette.from_hex([\n",
    "    TEAM_COLORS[TEAM_NAMES[0]],\n",
    "    TEAM_COLORS[TEAM_NAMES[1]]\n",
    "])\n",
    "\n",
    "team_mask_annotator = sv.MaskAnnotator(\n",
    "    color=team_colors,\n",
    "    opacity=0.5,\n",
    "    color_lookup=sv.ColorLookup.INDEX)\n",
    "team_label_annotator = sv.RichLabelAnnotator(\n",
    "    font_path=f\"{HOME}/fonts/Staatliches-Regular.ttf\",\n",
    "    font_size=40,\n",
    "    color=team_colors,\n",
    "    text_color=sv.Color.WHITE,\n",
    "    text_position=sv.Position.BOTTOM_CENTER,\n",
    "    text_offset=(0, 10),\n",
    "    color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
    "    for frame, detections in tqdm(zip(frames_history, detections_history)):\n",
    "        detections = detections[detections.area > 100]\n",
    "\n",
    "        teams = team_validator.get_validated(tracker_ids=detections.tracker_id)\n",
    "        teams = np.array(teams).astype(int)\n",
    "        numbers = number_validator.get_validated(tracker_ids=detections.tracker_id)\n",
    "        numbers = np.array(numbers)\n",
    "\n",
    "        labels = [\n",
    "            f\"#{number} {TEAM_ROSTERS[TEAM_NAMES[team]].get(number)}\"\n",
    "            for number, team\n",
    "            in zip(numbers, teams)\n",
    "        ]\n",
    "\n",
    "        annotated_frame = frame.copy()\n",
    "        annotated_frame = team_mask_annotator.annotate(\n",
    "            scene=annotated_frame,\n",
    "            detections=detections,\n",
    "            custom_color_lookup=teams)\n",
    "        annotated_frame = team_label_annotator.annotate(\n",
    "            scene=annotated_frame,\n",
    "            detections=detections,\n",
    "            labels=labels,\n",
    "            custom_color_lookup=teams)\n",
    "\n",
    "        sink.write_frame(annotated_frame)\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ul99bONuZp-I"
   },
   "outputs": [],
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-YW3b5Qi-U6"
   },
   "source": [
    "## Court keypoints detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsKMDgdqjmnI"
   },
   "source": [
    "### Load keypoint detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eyHvSFpSjiDV"
   },
   "outputs": [],
   "source": [
    "KEYPOINT_DETECTION_MODEL_ID = \"basketball-court-detection-2/14\"\n",
    "KEYPOINT_DETECTION_MODEL_CONFIDENCE = 0.3\n",
    "KEYPOINT_DETECTION_MODEL_ANCHOR_CONFIDENCE = 0.5\n",
    "KEYPOINT_DETECTION_MODEL = get_model(model_id=KEYPOINT_DETECTION_MODEL_ID)\n",
    "KEYPOINT_COLOR = sv.Color.from_hex('#FF1493')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqchilTtkWz6"
   },
   "source": [
    "### Single frame keypoint detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJRCgmIBkRXx"
   },
   "outputs": [],
   "source": [
    "vertex_annotator = sv.VertexAnnotator(color=KEYPOINT_COLOR, radius=8)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = KEYPOINT_DETECTION_MODEL.infer(frame, confidence=KEYPOINT_DETECTION_MODEL_CONFIDENCE)[0]\n",
    "key_points = sv.KeyPoints.from_inference(result)\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = vertex_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    key_points=key_points)\n",
    "\n",
    "cv2.imwrite(f\"{images_dir}/plot_11.png\", cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "sv.plot_image(annotated_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCcYa07Dk-Lf"
   },
   "source": [
    "### Detecting keypoints with high confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4nHvCAGlDvF"
   },
   "outputs": [],
   "source": [
    "vertex_annotator = sv.VertexAnnotator(color=KEYPOINT_COLOR, radius=8)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = KEYPOINT_DETECTION_MODEL.infer(frame, confidence=KEYPOINT_DETECTION_MODEL_CONFIDENCE)[0]\n",
    "key_points = sv.KeyPoints.from_inference(result)\n",
    "key_points = key_points[:, key_points.confidence[0] > KEYPOINT_DETECTION_MODEL_ANCHOR_CONFIDENCE]\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = vertex_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    key_points=key_points)\n",
    "\n",
    "cv2.imwrite(f\"{images_dir}/plot_12.png\", cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "sv.plot_image(annotated_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3B_KM5MNlYBm"
   },
   "source": [
    "## Map player positions to court coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdbDdYc10Pb-"
   },
   "source": [
    "## Single frame player position mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2r-NClenOfQ"
   },
   "outputs": [],
   "source": [
    "config = CourtConfiguration(league=League.NBA, measurement_unit=MeasurementUnit.FEET)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# define team annotators\n",
    "\n",
    "team_colors = sv.ColorPalette.from_hex([\n",
    "    TEAM_COLORS[TEAM_NAMES[0]],\n",
    "    TEAM_COLORS[TEAM_NAMES[1]]\n",
    "])\n",
    "\n",
    "team_box_annotator = sv.BoxAnnotator(\n",
    "    color=team_colors,\n",
    "    thickness=2,\n",
    "    color_lookup=sv.ColorLookup.INDEX\n",
    ")\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we determine the team for each player and assign a team ID to every detection\n",
    "\n",
    "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "crops = [sv.crop_image(frame, box) for box in boxes]\n",
    "TEAMS = np.array(team_classifier.predict(crops))\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = team_box_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections,\n",
    "    custom_color_lookup=TEAMS[detections.tracker_id - 1]\n",
    ")\n",
    "cv2.imwrite(f\"{images_dir}/plot_13.png\", cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "sv.plot_image(annotated_frame)\n",
    "\n",
    "# we use a keypoint model to detect court landmarks\n",
    "\n",
    "result = KEYPOINT_DETECTION_MODEL.infer(frame, confidence=KEYPOINT_DETECTION_MODEL_CONFIDENCE)[0]\n",
    "key_points = sv.KeyPoints.from_inference(result)\n",
    "landmarks_mask = key_points.confidence[0] > KEYPOINT_DETECTION_MODEL_ANCHOR_CONFIDENCE\n",
    "\n",
    "if np.count_nonzero(landmarks_mask) >= 4:\n",
    "\n",
    "    # we calculate homography matrix\n",
    "\n",
    "    court_landmarks = np.array(config.vertices)[landmarks_mask]\n",
    "    frame_landmarks = key_points[:, landmarks_mask].xy[0]\n",
    "\n",
    "    frame_to_court_transformer = ViewTransformer(\n",
    "        source=frame_landmarks,\n",
    "        target=court_landmarks,\n",
    "    )\n",
    "\n",
    "    frame_xy = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n",
    "\n",
    "    if len(frame_xy) > 0:\n",
    "\n",
    "        # we transform points\n",
    "\n",
    "        court_xy = frame_to_court_transformer.transform_points(points=frame_xy)\n",
    "\n",
    "        # we visualize the results\n",
    "\n",
    "        court = draw_court(config=config)\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=court_xy[TEAMS == 0],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[0]]),\n",
    "            court=court\n",
    "        )\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=court_xy[TEAMS == 1],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[1]]),\n",
    "            court=court\n",
    "        )\n",
    "\n",
    "        cv2.imwrite(f\"{images_dir}/plot_13.png\", cv2.cvtColor(court, cv2.COLOR_RGB2BGR))\n",
    "        sv.plot_image(court)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJiEzaE85jmW"
   },
   "source": [
    "### Full video player position mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzEhOEjQ3epJ"
   },
   "outputs": [],
   "source": [
    "video_xy = []\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we determine the team for each player and assign a team ID to every detection\n",
    "\n",
    "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "crops = [sv.crop_image(frame, box) for box in boxes]\n",
    "TEAMS = np.array(team_classifier.predict(crops))\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "for frame_idx, frame in tqdm(enumerate(frame_generator)):\n",
    "    detections = tracker.propagate(frame)\n",
    "\n",
    "    # we use a keypoint model to detect court landmarks\n",
    "\n",
    "    result = KEYPOINT_DETECTION_MODEL.infer(frame, confidence=KEYPOINT_DETECTION_MODEL_CONFIDENCE)[0]\n",
    "    key_points = sv.KeyPoints.from_inference(result)\n",
    "    landmarks_mask = key_points.confidence[0] > KEYPOINT_DETECTION_MODEL_ANCHOR_CONFIDENCE\n",
    "\n",
    "    if np.count_nonzero(landmarks_mask) >= 4:\n",
    "\n",
    "        # we calculate homography matrix\n",
    "\n",
    "        court_landmarks = np.array(config.vertices)[landmarks_mask]\n",
    "        frame_landmarks = key_points[:, landmarks_mask].xy[0]\n",
    "\n",
    "        frame_to_court_transformer = ViewTransformer(\n",
    "            source=frame_landmarks,\n",
    "            target=court_landmarks,\n",
    "        )\n",
    "\n",
    "        frame_xy = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n",
    "        court_xy = frame_to_court_transformer.transform_points(points=frame_xy)\n",
    "        video_xy.append(court_xy)\n",
    "\n",
    "video_xy = np.array(video_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAeep8b23zXs"
   },
   "outputs": [],
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-map{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "config = CourtConfiguration(league=League.NBA, measurement_unit=MeasurementUnit.FEET)\n",
    "court = draw_court(config=config)\n",
    "court_h, court_w, _ = court.shape\n",
    "\n",
    "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
    "video_info.width = court_w\n",
    "video_info.height = court_h\n",
    "\n",
    "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
    "    for frame_xy in tqdm(video_xy):\n",
    "        court = draw_court(config=config)\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=frame_xy[TEAMS == 0],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[0]]),\n",
    "            court=court\n",
    "        )\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=frame_xy[TEAMS == 1],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[1]]),\n",
    "            court=court\n",
    "        )\n",
    "\n",
    "        sink.write_frame(court)\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeSmIUzW4SHN"
   },
   "outputs": [],
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUhD1VKNM41D"
   },
   "source": [
    "### Clean player movement paths\n",
    "\n",
    "We are detecting sudden jumps in position using robust speed analysis. We are removing short abnormal runs and nearby frames to eliminate teleport-like artifacts. We are filling missing segments with linear interpolation to ensure continuous motion. We are smoothing all paths with a Savitzky\u2013Golay filter to achieve stable and natural movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gYSJSEm7KG-"
   },
   "outputs": [],
   "source": [
    "court = draw_paths_on_court(\n",
    "    config=config,\n",
    "    paths=[video_xy[:, 0, :]],\n",
    ")\n",
    "\n",
    "cv2.imwrite(f\"{images_dir}/plot_14.png\", cv2.cvtColor(court, cv2.COLOR_RGB2BGR))\n",
    "sv.plot_image(court)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7T6ImF7A-9JF"
   },
   "outputs": [],
   "source": [
    "cleaned_xy, edited_mask = clean_paths(\n",
    "    video_xy,\n",
    "    jump_sigma=3.5,\n",
    "    min_jump_dist=0.6,\n",
    "    max_jump_run=18,\n",
    "    pad_around_runs=2,\n",
    "    smooth_window=9,\n",
    "    smooth_poly=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWjBXVMZNxAa"
   },
   "outputs": [],
   "source": [
    "def split_true_runs(mask: np.ndarray, coords: np.ndarray) -> list[np.ndarray]:\n",
    "    mask = mask.squeeze()\n",
    "    idx = np.flatnonzero(mask)\n",
    "    if idx.size == 0:\n",
    "        return []\n",
    "    splits = np.where(np.diff(idx) > 1)[0] + 1\n",
    "    groups = np.split(idx, splits)\n",
    "    return [coords[g, 0, :] for g in groups]\n",
    "\n",
    "\n",
    "court = draw_paths_on_court(\n",
    "    config=config,\n",
    "    paths=[video_xy[:, 0, :]],\n",
    "    color=sv.Color.GREEN,\n",
    ")\n",
    "\n",
    "court = draw_paths_on_court(\n",
    "    config=config,\n",
    "    paths=split_true_runs(edited_mask[:, 0], video_xy),\n",
    "    color=sv.Color.RED,\n",
    "    court=court\n",
    ")\n",
    "\n",
    "cv2.imwrite(f\"{images_dir}/plot_15.png\", cv2.cvtColor(court, cv2.COLOR_RGB2BGR))\n",
    "sv.plot_image(court)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "waPGh4My_Bs6"
   },
   "outputs": [],
   "source": [
    "test = draw_paths_on_court(\n",
    "    config=config,\n",
    "    paths=[cleaned_xy[:, 0, :]],\n",
    ")\n",
    "\n",
    "cv2.imwrite(f\"{images_dir}/plot_16.png\", cv2.cvtColor(test, cv2.COLOR_RGB2BGR))\n",
    "sv.plot_image(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxCoYbu5_HSG"
   },
   "outputs": [],
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-map{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "config = CourtConfiguration(league=League.NBA, measurement_unit=MeasurementUnit.FEET)\n",
    "court = draw_court(config=config)\n",
    "court_h, court_w, _ = court.shape\n",
    "\n",
    "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
    "video_info.width = court_w\n",
    "video_info.height = court_h\n",
    "\n",
    "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
    "    for frame_xy in tqdm(cleaned_xy):\n",
    "        court = draw_court(config=config)\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=frame_xy[TEAMS == 0],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[0]]),\n",
    "            court=court\n",
    "        )\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=frame_xy[TEAMS == 1],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[1]]),\n",
    "            court=court\n",
    "        )\n",
    "\n",
    "        sink.write_frame(court)\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_jZI2P8_SSo"
   },
   "outputs": [],
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARvSRBTLaZXD"
   },
   "source": [
    "<div align=\"center\">\n",
    "  <p>\n",
    "    Looking for more tutorials or have questions?\n",
    "    Check out our <a href=\"https://github.com/roboflow/notebooks\">GitHub repo</a> for more notebooks,\n",
    "    or visit our <a href=\"https://discord.gg/GbfgXGJ8Bk\">discord</a>.\n",
    "  </p>\n",
    "  \n",
    "  <p>\n",
    "    <strong>If you found this helpful, please consider giving us a \u2b50\n",
    "    <a href=\"https://github.com/roboflow/notebooks\">on GitHub</a>!</strong>\n",
    "  </p>\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}