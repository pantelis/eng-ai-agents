{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "058d3a813f3249d6a39a95349a50cd16": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ace695827b8e4a18833c896c00dc843a",
       "IPY_MODEL_1f4df631559344c297d2b914d5ff820c",
       "IPY_MODEL_8ed64f56e4614a00ada52060d0a5ca75"
      ],
      "layout": "IPY_MODEL_d2c03546b5874c5481fbbf78194348b6"
     }
    },
    "ace695827b8e4a18833c896c00dc843a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4889e3fec4a04e16aa4d1795f4d34450",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5c4216188504468085e1d81d326d3785",
      "value": "Processingâ€‡video:â€‡100%"
     }
    },
    "1f4df631559344c297d2b914d5ff820c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2846a58f7a93404d8eb2f87127679e02",
      "max": 238,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5acd69210ae64107976ef41ef5d6ed0c",
      "value": 238
     }
    },
    "8ed64f56e4614a00ada52060d0a5ca75": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_50a6141ba6c64f119a6111333f9c772b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4332db8eab0d4b4a9c13acfd4e1b1d91",
      "value": "â€‡238/238â€‡[00:09&lt;00:00,â€‡26.43it/s]"
     }
    },
    "d2c03546b5874c5481fbbf78194348b6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4889e3fec4a04e16aa4d1795f4d34450": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c4216188504468085e1d81d326d3785": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2846a58f7a93404d8eb2f87127679e02": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5acd69210ae64107976ef41ef5d6ed0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "50a6141ba6c64f119a6111333f9c772b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4332db8eab0d4b4a9c13acfd4e1b1d91": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d67967b3e344f5e90322e5dd377b67d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0ca1824c60574445b308ad71955c4df8",
       "IPY_MODEL_5a88b1e7f0f543329e633d49e10772e1",
       "IPY_MODEL_6ace82065adb409485a5842935e02eef"
      ],
      "layout": "IPY_MODEL_b2a0e516ebc84005bdc2a3835b9071b9"
     }
    },
    "0ca1824c60574445b308ad71955c4df8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f571cd217334fc2a7a4541d8ff29ae4",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6d376c71fd054b8ebd3e1499e394339a",
      "value": "Processingâ€‡video:â€‡100%"
     }
    },
    "5a88b1e7f0f543329e633d49e10772e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df12d7c1b6ee42d3a22d13bbcca34d29",
      "max": 238,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2a56228e5ddf4fa09b38df76e289002f",
      "value": 238
     }
    },
    "6ace82065adb409485a5842935e02eef": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8f89baaa75046c0a90bc22fb9b07f16",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b227d73448f9417abdede7489dd2727b",
      "value": "â€‡238/238â€‡[01:50&lt;00:00,â€‡â€‡2.22it/s]"
     }
    },
    "b2a0e516ebc84005bdc2a3835b9071b9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f571cd217334fc2a7a4541d8ff29ae4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d376c71fd054b8ebd3e1499e394339a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df12d7c1b6ee42d3a22d13bbcca34d29": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a56228e5ddf4fa09b38df76e289002f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d8f89baaa75046c0a90bc22fb9b07f16": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b227d73448f9417abdede7489dd2727b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
    "\n",
    "# Basketball AI: How to Detect, Track, and Identify Basketball Players\n",
    "\n",
    "---\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb)\n",
    "[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/identify-basketball-players)\n",
    "\n",
    "![identify-basketball-players-2](https://storage.googleapis.com/com-roboflow-marketing/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players-2.png)\n",
    "\n",
    "![identify-basketball-players-1](https://storage.googleapis.com/com-roboflow-marketing/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players-1.png)"
   ],
   "metadata": {
    "id": "aS25QDv1a8_W"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment setup"
   ],
   "metadata": {
    "id": "2O5hUmxdbp0e"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configure your API keys\n",
    "\n",
    "To run this notebook, you need to provide your HuggingFace Token and Roboflow API key.  \n",
    "\n",
    "- The `ROBOFLOW_API_KEY` is required to pull the fine-tuned RF-DETR player detector and the SmolVLM2 number recognizer from Roboflow Universe.  \n",
    "- The `HF_TOKEN` is required to pull the pretrained SigLIP model from HuggingFace.  \n",
    "\n",
    "Follow these steps:  \n",
    "\n",
    "- Open your [`HuggingFace Settings`](https://huggingface.co/settings) page. Click `Access Tokens` then `New Token` to generate a new token.  \n",
    "- Go to your [`Roboflow Settings`](https://app.roboflow.com/settings/api) page. Click `Copy`. This will place your private key in the clipboard.  \n",
    "- In Colab, go to the left pane and click on `Secrets` (ðŸ”‘).  \n",
    "    - Store the HuggingFace Access Token under the name `HF_TOKEN`.  \n",
    "    - Store the Roboflow API Key under the name `ROBOFLOW_API_KEY`.  "
   ],
   "metadata": {
    "id": "4F8KpVPsbxz1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Parameters (injected by papermill)\n",
    "output_dir = \".\"\n",
    "images_dir = \"./images\"\n",
    "videos_dir = \"./videos\"\n",
    "audio_dir = \"./audio\"\n",
    "text_dir = \"./text\"\n",
    "\n",
    "# Import cv2 for saving supervision images\n",
    "try:\n",
    "    import cv2\n",
    "except ImportError:\n",
    "    pass\n"
   ],
   "metadata": {
    "id": "xw1ZVBKxbvYM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check GPU availability\n",
    "\n",
    "Let's make sure we have access to a GPU. Run the `nvidia-smi` command to verify. If you run into issues, go to `Runtime` -> `Change runtime type`, select `T4 GPU` or `L4 GPU`, and then click `Save`."
   ],
   "metadata": {
    "id": "UfA2MikrcFWL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
    "os.environ[\"ROBOFLOW_API_KEY\"] = userdata.get(\"ROBOFLOW_API_KEY\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KKyA4lpocCLc",
    "outputId": "303e5f8d-4567-48b7-da43-b941522274c4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**NOTE:** To make it easier for us to manage datasets, images and models we create a `HOME` constant."
   ],
   "metadata": {
    "id": "TkeEI66XcVgr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpW8sBtBcHfG",
    "outputId": "0a29aa0b-5210-4655-d052-f45bc460df56"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install SAM2 real-time\n",
    "\n",
    "We will use `segment-anything-2-real-time`, an open-source fork of Metaâ€™s Segment Anything Model 2 optimized for real-time inference. After installing the repository, we will also download the required checkpoint files.\n"
   ],
   "metadata": {
    "id": "QJT-DpbFt6Ns"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "HOME = Path.cwd()\n",
    "print(\"HOME:\", HOME)"
   ],
   "metadata": {
    "id": "hfPi_Fi2uAu2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/Gy920/segment-anything-2-real-time.git\n",
    "%cd {HOME}/segment-anything-2-real-time\n",
    "!pip install -e . -q\n",
    "!python setup.py build_ext --inplace"
   ],
   "metadata": {
    "id": "W8Uwlni-uzkf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install dependencies"
   ],
   "metadata": {
    "id": "xiTmIfgBcc1e"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!(cd checkpoints && bash download_ckpts.sh)"
   ],
   "metadata": {
    "id": "j20VpRlOcXa2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set the ONNX Runtime execution provider to CUDA to ensure model inference runs on the GPU."
   ],
   "metadata": {
    "id": "p6lzxw4HfRxJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q gdown\n",
    "!pip install -q inference-gpu\n",
    "\n",
    "!pip install supervision==0.27.0rc4\n",
    "!pip install -q git+https://github.com/roboflow/sports.git@feat/basketball\n",
    "\n",
    "!pip install -q transformers num2words\n",
    "!pip install -q flash-attn --no-build-isolation"
   ],
   "metadata": {
    "id": "5rU19OlpfPsG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Source videos\n",
    "\n",
    "As an example, we will use sample videos from Game 1 of the 2025 NBA Playoffs between the Boston Celtics and the New York Knicks. We prepared 10 sample videos from this game."
   ],
   "metadata": {
    "id": "MWkLGOhnVpBG"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ALpFXOu4cwL3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ONNXRUNTIME_EXECUTION_PROVIDERS\"] = \"[CUDAExecutionProvider]\""
   ],
   "metadata": {
    "id": "50Uq-moPBUBY",
    "outputId": "8edf9bd9-f05e-4f27-fe52-4364fc271eb0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "SOURCE_VIDEO_DIRECTORY = HOME / \"source\"\n",
    "\n",
    "!gdown -q https://drive.google.com/drive/folders/1eDJYqQ77Fytz15tKGdJCMeYSgmoQ-2-H -O {SOURCE_VIDEO_DIRECTORY} --folder"
   ],
   "metadata": {
    "id": "D5LLKx8dE2se"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Team rosters\n",
    "\n",
    "We are preparing player rosters for both teams. We load the official lists that link jersey numbers to player names. These mappings will let us replace detected numbers with real names, making the final analytics clear and readable."
   ],
   "metadata": {
    "id": "fsHR16EVFBoo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!ls -la {SOURCE_VIDEO_DIRECTORY}"
   ],
   "metadata": {
    "id": "IZg88_zS7fIi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import dependencies"
   ],
   "metadata": {
    "id": "a-kTisAQYQHM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "SOURCE_VIDEO_PATH = SOURCE_VIDEO_DIRECTORY / \"boston-celtics-new-york-knicks-game-1-q1-04.28-04.20.mp4\""
   ],
   "metadata": {
    "id": "Wd1Yv077YSkj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Object detection\n",
    "\n",
    "The model used in this notebook detects the following classes: `ball`, `ball-in-basket`, `number`, `player`, `player-in-possession`, `player-jump-shot`, `player-layup-dunk`, `player-shot-block`, `referee`, and `rim`. These classes enable tracking of game events, player actions, and ball location for basketball analytics."
   ],
   "metadata": {
    "id": "PupZT1xHhmoG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load RF-DETR object detection model"
   ],
   "metadata": {
    "id": "T5UNfe7fLvKf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "TEAM_ROSTERS = {\n",
    "  \"New York Knicks\": {\n",
    "    \"55\": \"Hukporti\",\n",
    "    \"1\": \"Payne\",\n",
    "    \"0\": \"Wright\",\n",
    "    \"11\": \"Brunson\",\n",
    "    \"3\": \"Hart\",\n",
    "    \"32\": \"Towns\",\n",
    "    \"44\": \"Shamet\",\n",
    "    \"25\": \"Bridges\",\n",
    "    \"2\": \"McBride\",\n",
    "    \"23\": \"Robinson\",\n",
    "    \"8\": \"Anunoby\",\n",
    "    \"4\": \"Dadiet\",\n",
    "    \"5\": \"Achiuwa\",\n",
    "    \"13\": \"Kolek\"\n",
    "  },\n",
    "  \"Boston Celtics\": {\n",
    "    \"42\": \"Horford\",\n",
    "    \"55\": \"Scheierman\",\n",
    "    \"9\": \"White\",\n",
    "    \"20\": \"Davison\",\n",
    "    \"7\": \"Brown\",\n",
    "    \"0\": \"Tatum\",\n",
    "    \"27\": \"Walsh\",\n",
    "    \"4\": \"Holiday\",\n",
    "    \"8\": \"Porzingis\",\n",
    "    \"40\": \"Kornet\",\n",
    "    \"88\": \"Queta\",\n",
    "    \"11\": \"Pritchard\",\n",
    "    \"30\": \"Hauser\",\n",
    "    \"12\": \"Craig\",\n",
    "    \"26\": \"Tillman\"\n",
    "  }\n",
    "}\n",
    "\n",
    "TEAM_COLORS = {\n",
    "    \"New York Knicks\": \"#006BB6\",\n",
    "    \"Boston Celtics\": \"#007A33\"\n",
    "}"
   ],
   "metadata": {
    "id": "PAD4DURmgz-s"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single frame object detection"
   ],
   "metadata": {
    "id": "tMXt_1FxL2D-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython.display import Video\n",
    "from typing import Dict, List, Optional, Union, Iterable, Tuple\n",
    "from operator import itemgetter\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import supervision as sv\n",
    "from inference import get_model\n",
    "from sports import (\n",
    "    clean_paths,\n",
    "    ConsecutiveValueTracker,\n",
    "    TeamClassifier,\n",
    "    MeasurementUnit,\n",
    "    ViewTransformer\n",
    ")\n",
    "from sports.basketball import (\n",
    "    CourtConfiguration,\n",
    "    League,\n",
    "    draw_court,\n",
    "    draw_points_on_court,\n",
    "    draw_paths_on_court\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "LdasOAlhGVCY",
    "outputId": "370512fb-358f-46db-85cb-cab3660611a6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Keep only \"number\" class"
   ],
   "metadata": {
    "id": "1VzpR_LGEGad"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "PLAYER_DETECTION_MODEL_ID = \"basketball-player-detection-3-ycjdo/4\"\n",
    "PLAYER_DETECTION_MODEL_CONFIDENCE = 0.4\n",
    "PLAYER_DETECTION_MODEL_IOU_THRESHOLD = 0.9\n",
    "PLAYER_DETECTION_MODEL = get_model(model_id=PLAYER_DETECTION_MODEL_ID)\n",
    "\n",
    "COLOR = sv.ColorPalette.from_hex([\n",
    "    \"#ffff00\", \"#ff9b00\", \"#ff66ff\", \"#3399ff\", \"#ff66b2\", \"#ff8080\",\n",
    "    \"#b266ff\", \"#9999ff\", \"#66ffff\", \"#33ff99\", \"#66ff66\", \"#99ff00\"\n",
    "])"
   ],
   "metadata": {
    "id": "Vftat_boERYg",
    "outputId": "4db514ee-436b-45c3-e848-f6e642e3189b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Keep only player-related classes"
   ],
   "metadata": {
    "id": "ttZNW8qgPLBm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = box_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "annotated_frame = label_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "\n",
    "cv2.imwrite(f\"{images_dir}/plot_1.png\", cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "sv.plot_image(annotated_frame)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "EjM1OuY5POCA",
    "outputId": "ffabe662-1e55-4120-ecac-72da366d11f4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Full video object detection\n",
    "\n",
    "We are running RF-DETR across all frames to produce a per-frame sequence of detections. These sequences seed tracking and provide number crops over time."
   ],
   "metadata": {
    "id": "PUZHIvfdNRi1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "NUMBER_CLASS_ID = 2\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[detections.class_id == NUMBER_CLASS_ID]\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = box_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "annotated_frame = label_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "\n",
    "cv2.imwrite(f\"{images_dir}/plot_2.png\", cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "sv.plot_image(annotated_frame)"
   ],
   "metadata": {
    "id": "F4O6W49jNz90",
    "outputId": "5269768c-d2bf-4833-da48-9901bf995b3a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "058d3a813f3249d6a39a95349a50cd16",
      "ace695827b8e4a18833c896c00dc843a",
      "1f4df631559344c297d2b914d5ff820c",
      "8ed64f56e4614a00ada52060d0a5ca75",
      "d2c03546b5874c5481fbbf78194348b6",
      "4889e3fec4a04e16aa4d1795f4d34450",
      "5c4216188504468085e1d81d326d3785",
      "2846a58f7a93404d8eb2f87127679e02",
      "5acd69210ae64107976ef41ef5d6ed0c",
      "50a6141ba6c64f119a6111333f9c772b",
      "4332db8eab0d4b4a9c13acfd4e1b1d91"
     ]
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "PLAYER_CLASS_IDS = [3, 4, 5, 6, 7] # player, player-in-possession, player-jump-shot, player-layup-dunk, player-shot-block\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = box_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "annotated_frame = label_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "\n",
    "cv2.imwrite(f\"{images_dir}/plot_3.png\", cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "sv.plot_image(annotated_frame)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "rQ2PGjhHp8I7",
    "outputId": "58a9ff49-45ce-4bc5-e6eb-a9a00a283e0f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Player tracking\n",
    "\n",
    "We are switching from frame-wise boxes to temporal tracks. SAM2 yields per-player masks and stable track IDs that persist through occlusions and re-entries."
   ],
   "metadata": {
    "id": "WzfEx2GbQNuO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load SAM2 tracking model\n",
    "\n",
    "We are loading a SAM2 checkpoint and config into the camera predictor. The large variant yields the highest quality masks; swap to smaller for speed if needed."
   ],
   "metadata": {
    "id": "b6uqFLygS39e"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-detection{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-detection{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "    detections = sv.Detections.from_inference(result)\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = box_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=detections)\n",
    "    annotated_frame = label_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=detections)\n",
    "    return annotated_frame\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path=TARGET_VIDEO_PATH,\n",
    "    callback=callback,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B2xcYSWjQVMu",
    "outputId": "36dc25dd-5c0b-434a-febd-96e8144253e4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Full video player tacking"
   ],
   "metadata": {
    "id": "mYfdZW-nTg5N"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are prompting SAM2 with RF-DETR boxes and tracking across the clip. The callback saves masks, IDs, and visualizations for downstream use."
   ],
   "metadata": {
    "id": "csTvkOMoXZU4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ],
   "metadata": {
    "id": "Tz_lvB_tQOA4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%cd $HOME/segment-anything-2-real-time\n",
    "\n",
    "from sam2.build_sam import build_sam2_camera_predictor\n",
    "\n",
    "SAM2_CHECKPOINT = \"checkpoints/sam2.1_hiera_large.pt\"\n",
    "SAM2_CONFIG = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "predictor = build_sam2_camera_predictor(SAM2_CONFIG, SAM2_CHECKPOINT)"
   ],
   "metadata": {
    "id": "YcY0uJKOQnr-",
    "outputId": "bc051598-afc0-4ef1-fa63-7ae22c002190",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592,
     "referenced_widgets": [
      "7d67967b3e344f5e90322e5dd377b67d",
      "0ca1824c60574445b308ad71955c4df8",
      "5a88b1e7f0f543329e633d49e10772e1",
      "6ace82065adb409485a5842935e02eef",
      "b2a0e516ebc84005bdc2a3835b9071b9",
      "5f571cd217334fc2a7a4541d8ff29ae4",
      "6d376c71fd054b8ebd3e1499e394339a",
      "df12d7c1b6ee42d3a22d13bbcca34d29",
      "2a56228e5ddf4fa09b38df76e289002f",
      "d8f89baaa75046c0a90bc22fb9b07f16",
      "b227d73448f9417abdede7489dd2727b"
     ]
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "class SAM2Tracker:\n",
    "    def __init__(self, predictor) -> None:\n",
    "        self.predictor = predictor\n",
    "        self._prompted = False\n",
    "\n",
    "    def prompt_first_frame(self, frame: np.ndarray, detections: sv.Detections) -> None:\n",
    "        if len(detections) == 0:\n",
    "            raise ValueError(\"detections must contain at least one box\")\n",
    "\n",
    "        if detections.tracker_id is None:\n",
    "            detections.tracker_id = list(range(1, len(detections) + 1))\n",
    "\n",
    "        with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            self.predictor.load_first_frame(frame)\n",
    "            for xyxy, obj_id in zip(detections.xyxy, detections.tracker_id):\n",
    "                bbox = np.asarray([xyxy], dtype=np.float32)\n",
    "                self.predictor.add_new_prompt(\n",
    "                    frame_idx=0,\n",
    "                    obj_id=int(obj_id),\n",
    "                    bbox=bbox,\n",
    "                )\n",
    "\n",
    "        self._prompted = True\n",
    "\n",
    "    def propagate(self, frame: np.ndarray) -> sv.Detections:\n",
    "        if not self._prompted:\n",
    "            raise RuntimeError(\"Call prompt_first_frame before propagate\")\n",
    "\n",
    "        with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            tracker_ids, mask_logits = self.predictor.track(frame)\n",
    "\n",
    "        tracker_ids = np.asarray(tracker_ids, dtype=np.int32)\n",
    "        masks = (mask_logits > 0.0).cpu().numpy()\n",
    "        masks = np.squeeze(masks).astype(bool)\n",
    "\n",
    "        if masks.ndim == 2:\n",
    "            masks = masks[None, ...]\n",
    "\n",
    "        masks = np.array([\n",
    "            sv.filter_segments_by_distance(mask, relative_distance=0.03, mode=\"edge\")\n",
    "            for mask in masks\n",
    "        ])\n",
    "\n",
    "        xyxy = sv.mask_to_xyxy(masks=masks)\n",
    "        detections = sv.Detections(xyxy=xyxy, mask=masks, tracker_id=tracker_ids)\n",
    "        return detections\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self._prompted = False"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "8fmNMnpBb4PX",
    "outputId": "d619a3e5-35c3-4d4e-aca3-6c837cd5000c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cluster players into teams\n",
    "\n",
    "EWe are assigning each track to a team without labels. The pipeline uses SigLIP embeddings, UMAP to 3D, then K-means with k=2 for final team IDs."
   ],
   "metadata": {
    "id": "qNTHNNatcC_I"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Collecting training set\n",
    "\n",
    "We are sampling frames at 1 FPS, detecting players, and extracting central crops. Central regions emphasize jersey color and texture while reducing background artifacts."
   ],
   "metadata": {
    "id": "Tx-Z3m9QhPIO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-mask{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "# define team annotators\n",
    "\n",
    "mask_annotator = sv.MaskAnnotator(\n",
    "    color=COLOR,\n",
    "    color_lookup=sv.ColorLookup.TRACK,\n",
    "    opacity=0.5)\n",
    "box_annotator = sv.BoxAnnotator(\n",
    "    color=COLOR,\n",
    "    color_lookup=sv.ColorLookup.TRACK,\n",
    "    thickness=2\n",
    ")\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = box_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "cv2.imwrite(f\"{images_dir}/plot_4.png\", cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "sv.plot_image(annotated_frame)\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    detections = tracker.propagate(frame)\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = mask_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "    annotated_frame = box_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "    return annotated_frame\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path=TARGET_VIDEO_PATH,\n",
    "    callback=callback,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ],
   "metadata": {
    "id": "ouVXww3JlHSM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "p5WyZ4tmlwgQ",
    "outputId": "081f46bd-3ff7-4706-fc8e-978a88b28cad"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train and test clustering model\n",
    "\n",
    "We are computing SigLIP embeddings for crops, reducing with UMAP, and fitting K-means. A quick validation confirms separation by uniform appearance."
   ],
   "metadata": {
    "id": "gDIWnI9NmKI0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "STRIDE = 30\n",
    "\n",
    "crops = []\n",
    "\n",
    "for video_path in sv.list_files_with_extensions(SOURCE_VIDEO_DIRECTORY, extensions=[\"mp4\", \"avi\", \"mov\"]):\n",
    "    frame_generator = sv.get_video_frames_generator(source_path=video_path, stride=STRIDE)\n",
    "\n",
    "    for frame in tqdm(frame_generator):\n",
    "\n",
    "        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD, class_agnostic_nms=True)[0]\n",
    "        detections = sv.Detections.from_inference(result)\n",
    "        detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "\n",
    "        boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "        for box in boxes:\n",
    "            crops.append(sv.crop_image(frame, box))"
   ],
   "metadata": {
    "id": "X1GEHMzUmO4z"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sv.plot_images_grid(\n",
    "    images=crops[:100],\n",
    "    grid_size=(10, 10),\n",
    "    size=(10, 10)\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 844
    },
    "id": "U8rYeP5HmkUj",
    "outputId": "75e4043d-4866-47b7-dc71-5c67c3e02c95"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test clustering model on single video frame\n",
    "\n",
    "We are applying the trained clustering to one frameâ€™s player crops. The output assigns provisional team IDs to confirm the mapping before full-video use."
   ],
   "metadata": {
    "id": "5CdLxBkXKOca"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "team_classifier = TeamClassifier(device=\"cuda\")\n",
    "team_classifier.fit(crops)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "XSdtN_9fmw_M",
    "outputId": "99a99486-cb2c-40d8-974d-0d752f01f78d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we do not control which IDs the clustering algorithm assigns to the teams, after training and testing we must select one of the dictionaries below."
   ],
   "metadata": {
    "id": "TMVitdaknPqa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "teams = team_classifier.predict(crops)\n",
    "\n",
    "team_0 = [crop for crop, team in zip(crops, teams) if team == 0]\n",
    "team_1 = [crop for crop, team in zip(crops, teams) if team == 1]\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=team_0[:50],\n",
    "    grid_size=(5, 10),\n",
    "    size=(10, 5)\n",
    ")\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=team_1[:50],\n",
    "    grid_size=(5, 10),\n",
    "    size=(10, 5)\n",
    ")"
   ],
   "metadata": {
    "id": "cALjgKE-nFwJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Full video team clustering\n",
    "\n",
    "We are assigning team IDs to tracks once, then reusing them across frames via track IDs. This keeps colors and labels consistent throughout the video."
   ],
   "metadata": {
    "id": "iBKKa8MKo5j7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD, class_agnostic_nms=True)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "\n",
    "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "crops = [sv.crop_image(frame, box) for box in boxes]\n",
    "teams = np.array(team_classifier.predict(crops))\n",
    "\n",
    "team_0 = [crop for crop, team in zip(crops, teams) if team == 0]\n",
    "team_1 = [crop for crop, team in zip(crops, teams) if team == 1]\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=team_0[:10],\n",
    "    grid_size=(1, 10),\n",
    "    size=(10, 1)\n",
    ")\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=team_1[:10],\n",
    "    grid_size=(1, 10),\n",
    "    size=(10, 1)\n",
    ")"
   ],
   "metadata": {
    "id": "_bF0IaqNTccu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "TEAM_NAMES = {\n",
    "    0: \"New York Knicks\",\n",
    "    1: \"Boston Celtics\",\n",
    "}\n",
    "\n",
    "# TEAM_NAMES = {\n",
    "#     0: \"Boston Celtics\",\n",
    "#     1: \"New York Knicks\",\n",
    "# }"
   ],
   "metadata": {
    "id": "pcYjgSQrVQP0",
    "outputId": "9763fefe-6066-4471-9014-2b5512585c17",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Player numbers OCR\n",
    "\n",
    "We are moving to jersey OCR to identify individuals within each team. Number reads pair with tracks and teams to resolve names later."
   ],
   "metadata": {
    "id": "-0Q-gOy2su5v"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load number recognition model\n",
    "\n",
    "We are loading the fine-tuned SmolVLM2 OCR model by ID. It was trained on jersey crops and outputs digit strings suitable for downstream validation."
   ],
   "metadata": {
    "id": "tXiZXnqntShC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-teams{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# define team annotators\n",
    "\n",
    "team_colors = sv.ColorPalette.from_hex([\n",
    "    TEAM_COLORS[TEAM_NAMES[0]],\n",
    "    TEAM_COLORS[TEAM_NAMES[1]]\n",
    "])\n",
    "\n",
    "team_mask_annotator = sv.MaskAnnotator(\n",
    "    color=team_colors,\n",
    "    opacity=0.5,\n",
    "    color_lookup=sv.ColorLookup.INDEX\n",
    ")\n",
    "\n",
    "team_box_annotator = sv.BoxAnnotator(\n",
    "    color=team_colors,\n",
    "    thickness=2,\n",
    "    color_lookup=sv.ColorLookup.INDEX\n",
    ")\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we determine the team for each player and assign a team ID to every detection\n",
    "\n",
    "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "crops = [sv.crop_image(frame, box) for box in boxes]\n",
    "TEAMS = np.array(team_classifier.predict(crops))\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    detections = tracker.propagate(frame)\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = team_mask_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=detections,\n",
    "        custom_color_lookup=TEAMS[detections.tracker_id - 1]\n",
    "    )\n",
    "    annotated_frame = team_box_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=detections,\n",
    "        custom_color_lookup=TEAMS[detections.tracker_id - 1]\n",
    "    )\n",
    "    return annotated_frame\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path=TARGET_VIDEO_PATH,\n",
    "    callback=callback,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ],
   "metadata": {
    "id": "MfgolXlwtep1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single frame player number detection and recognition\n",
    "\n",
    "We are detecting number boxes, padding, and cropping. We then run SmolVLM2 on each crop and preview predictions next to the regions."
   ],
   "metadata": {
    "id": "AkM_JQ61s3Oc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "Ily4KtIdswTB",
    "outputId": "671d3db8-ea24-4bb0-882e-6fbd84894a9c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "NUMBER_RECOGNITION_MODEL_ID = \"basketball-jersey-numbers-ocr/3\"\n",
    "NUMBER_RECOGNITION_MODEL = get_model(model_id=NUMBER_RECOGNITION_MODEL_ID)\n",
    "NUMBER_RECOGNITION_MODEL_PROMPT = \"Read the number.\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "id": "yXUi2CR-t4mt",
    "outputId": "471f74ce-24cf-43b9-b06e-f2a6e2f7cf85"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single frame player detection with number detection matching\n",
    "\n",
    "We are matching numbers to players using Intersection over Smaller Area. IoS equals 1.0 implies the number lies fully inside the player mask, so we link them."
   ],
   "metadata": {
    "id": "i3fw2eXcuyVA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "frame_h, frame_w, *_ = frame.shape\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[detections.class_id == NUMBER_CLASS_ID]\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = box_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "annotated_frame = label_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "\n",
    "cv2.imwrite(f\"{images_dir}/plot_8.png\", cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "sv.plot_image(annotated_frame)"
   ],
   "metadata": {
    "id": "HFB3xRaPzVyr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "crops = [\n",
    "    sv.resize_image(sv.crop_image(frame, xyxy), resolution_wh=(224, 224))\n",
    "    for xyxy\n",
    "    in sv.clip_boxes(sv.pad_boxes(xyxy=detections.xyxy, px=10, py=10), (frame_w, frame_h))\n",
    "]\n",
    "numbers = [\n",
    "    NUMBER_RECOGNITION_MODEL.predict(crop, NUMBER_RECOGNITION_MODEL_PROMPT)[0]\n",
    "    for crop\n",
    "    in crops\n",
    "]\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=crops[:10],\n",
    "    titles=numbers[:10],\n",
    "    grid_size=(1, 10),\n",
    "    size=(10, 1)\n",
    ")"
   ],
   "metadata": {
    "id": "7wJljIgOVq8D",
    "outputId": "c4c5d59d-a3b1-4b5c-bfe0-cd614a70cf65",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Validating recognized numbers\n",
    "\n",
    "We are confirming numbers across time using a consecutive-agreement threshold. The validator locks a number to a track only after repeated consistent reads."
   ],
   "metadata": {
    "id": "BqaXSQBVB5kV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def coords_above_threshold(\n",
    "    matrix: np.ndarray, threshold: float, sort_desc: bool = True\n",
    ") -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Return all (row_index, col_index) where value > threshold.\n",
    "    Rows and columns may repeat.\n",
    "    Optionally sort by value descending.\n",
    "    \"\"\"\n",
    "    A = np.asarray(matrix)\n",
    "    rows, cols = np.where(A > threshold)\n",
    "    pairs = list(zip(rows.tolist(), cols.tolist()))\n",
    "    if sort_desc:\n",
    "        pairs.sort(key=lambda rc: A[rc[0], rc[1]], reverse=True)\n",
    "    return pairs"
   ],
   "metadata": {
    "id": "j-zjlroYWm-q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# define team annotators\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=4, color_lookup=sv.ColorLookup.TRACK)\n",
    "\n",
    "player_mask_annotator = sv.MaskAnnotator(color=COLOR.by_idx(3), opacity=0.8, color_lookup=sv.ColorLookup.INDEX)\n",
    "number_mask_annotator = sv.MaskAnnotator(color=COLOR.by_idx(0), opacity=0.8, color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "for index, frame in tqdm(enumerate(frame_generator)):\n",
    "\n",
    "    # we only process the first video frame\n",
    "\n",
    "    if index > 0:\n",
    "        break\n",
    "\n",
    "    frame_h, frame_w, *_ = frame.shape\n",
    "\n",
    "    player_detections = tracker.propagate(frame)\n",
    "\n",
    "    # we use RF-DETR model to detect numbers\n",
    "\n",
    "    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "    number_detections = sv.Detections.from_inference(result)\n",
    "    number_detections = number_detections[number_detections.class_id == NUMBER_CLASS_ID]\n",
    "    number_detections.mask = sv.xyxy_to_mask(boxes=number_detections.xyxy, resolution_wh=(frame_w, frame_h))\n",
    "\n",
    "    # we use mask IoS to match numbers with players\n",
    "\n",
    "    iou = sv.mask_iou_batch(\n",
    "        masks_true=player_detections.mask,\n",
    "        masks_detection=number_detections.mask,\n",
    "        overlap_metric=sv.OverlapMetric.IOS\n",
    "    )\n",
    "\n",
    "    pairs = coords_above_threshold(iou, 0.9)\n",
    "    player_idx, number_idx = zip(*pairs)\n",
    "\n",
    "    # we visualize all the masks\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = player_mask_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=player_detections)\n",
    "    annotated_frame = number_mask_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=number_detections)\n",
    "    cv2.imwrite(f\"{images_dir}/plot_10.png\", cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "    sv.plot_image(annotated_frame)\n",
    "\n",
    "    # we visualize only matched pairs\n",
    "\n",
    "    player_detections = player_detections[np.array(player_idx)]\n",
    "    number_detections = number_detections[np.array(number_idx)]\n",
    "    number_detections.tracker_id = player_detections.tracker_id\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = box_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=player_detections)\n",
    "    annotated_frame = box_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=number_detections)\n",
    "    cv2.imwrite(f\"{images_dir}/plot_10.png\", cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "    sv.plot_image(annotated_frame)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "YyYzxDolX6dR",
    "outputId": "a320ee25-fbe4-490d-e126-27917d75afe2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Player recognition\n",
    "\n",
    "We are overlaying names, numbers, team colors, and masks for each tracked player. The final render shows stable identities aligned with roster data across the full clip."
   ],
   "metadata": {
    "id": "cp9TzWCZOmr9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-validated-numbers{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "number_validator = ConsecutiveValueTracker(n_consecutive=3)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# define team annotators\n",
    "\n",
    "mask_annotator = sv.MaskAnnotator(\n",
    "    color=COLOR,\n",
    "    color_lookup=sv.ColorLookup.TRACK,\n",
    "    opacity=0.7)\n",
    "box_annotator = sv.BoxAnnotator(\n",
    "    color=COLOR,\n",
    "    color_lookup=sv.ColorLookup.TRACK,\n",
    "    thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    color=COLOR,\n",
    "    color_lookup=sv.ColorLookup.TRACK,\n",
    "    text_color=sv.Color.BLACK,\n",
    "    text_scale=0.8)\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    player_detections = tracker.propagate(frame)\n",
    "\n",
    "    # we perform number recognition at specific frame intervals\n",
    "\n",
    "    if index % 5 == 0:\n",
    "        frame_h, frame_w, *_ = frame.shape\n",
    "\n",
    "        # we use RF-DETR model to detect numbers\n",
    "\n",
    "        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "        number_detections = sv.Detections.from_inference(result)\n",
    "        number_detections = number_detections[number_detections.class_id == NUMBER_CLASS_ID]\n",
    "        number_detections.mask = sv.xyxy_to_mask(boxes=number_detections.xyxy, resolution_wh=(frame_w, frame_h))\n",
    "\n",
    "        # we crop out numbers detection and run recognition model\n",
    "\n",
    "        number_crops = [\n",
    "            sv.crop_image(frame, xyxy)\n",
    "            for xyxy\n",
    "            in sv.clip_boxes(sv.pad_boxes(xyxy=number_detections.xyxy, px=10, py=10), (frame_w, frame_h))\n",
    "        ]\n",
    "        numbers = [\n",
    "            NUMBER_RECOGNITION_MODEL.predict(number_crop, NUMBER_RECOGNITION_MODEL_PROMPT)[0]\n",
    "            for number_crop\n",
    "            in number_crops\n",
    "        ]\n",
    "\n",
    "        # we use mask IoS to match numbers with players\n",
    "\n",
    "        iou = sv.mask_iou_batch(\n",
    "            masks_true=player_detections.mask,\n",
    "            masks_detection=number_detections.mask,\n",
    "            overlap_metric=sv.OverlapMetric.IOS\n",
    "        )\n",
    "\n",
    "        pairs = coords_above_threshold(iou, 0.9)\n",
    "\n",
    "        if pairs:\n",
    "\n",
    "            player_idx, number_idx = zip(*pairs)\n",
    "            player_idx = [i + 1 for i in player_idx]\n",
    "            number_idx = list(number_idx)\n",
    "\n",
    "            # we update number_validator state\n",
    "\n",
    "            numbers = [numbers[int(i)] for i in number_idx]\n",
    "            number_validator.update(tracker_ids=player_idx, values=numbers)\n",
    "\n",
    "    # we visualize boxes and masks\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = mask_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=player_detections)\n",
    "    annotated_frame = box_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=player_detections)\n",
    "\n",
    "    # we extract validated numbers\n",
    "\n",
    "    numbers = number_validator.get_validated(tracker_ids=player_detections.tracker_id)\n",
    "\n",
    "    # we visualize numbers\n",
    "\n",
    "    annotated_frame = label_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=player_detections,\n",
    "        labels=numbers)\n",
    "\n",
    "    return annotated_frame\n",
    "\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path=TARGET_VIDEO_PATH,\n",
    "    callback=callback,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ],
   "metadata": {
    "id": "OnWZ2ZKgZJv0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ],
   "metadata": {
    "id": "fnoBYFhcYfpV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!gdown https://drive.google.com/drive/folders/1RBjpI5Xleb58lujeusxH0W5zYMMA4ytO -O {HOME / \"fonts\"} --folder"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1scdRpWZUgd",
    "outputId": "4393c507-f974-4c98-a3f4-8eb8e6c93fc0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "frames_history = []\n",
    "detections_history = []\n",
    "\n",
    "number_validator = ConsecutiveValueTracker(n_consecutive=3)\n",
    "team_validator = ConsecutiveValueTracker(n_consecutive=1)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we determine the team for each player and assign a team ID to every detection\n",
    "\n",
    "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "crops = [sv.crop_image(frame, box) for box in boxes]\n",
    "TEAMS = np.array(team_classifier.predict(crops))\n",
    "\n",
    "team_validator.update(tracker_ids=detections.tracker_id, values=TEAMS)\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "\n",
    "for index, frame in tqdm(enumerate(frame_generator)):\n",
    "    player_detections = tracker.propagate(frame)\n",
    "    frames_history.append(frame)\n",
    "    detections_history.append(player_detections)\n",
    "\n",
    "    # we perform number recognition at specific frame intervals\n",
    "\n",
    "    if index % 5 == 0:\n",
    "        frame_h, frame_w, *_ = frame.shape\n",
    "\n",
    "        # we use RF-DETR model to detect numbers\n",
    "\n",
    "        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "        number_detections = sv.Detections.from_inference(result)\n",
    "        number_detections = number_detections[number_detections.class_id == NUMBER_CLASS_ID]\n",
    "        number_detections.mask = sv.xyxy_to_mask(boxes=number_detections.xyxy, resolution_wh=(frame_w, frame_h))\n",
    "\n",
    "        # we crop out numbers detection and run recognition model\n",
    "\n",
    "        number_crops = [\n",
    "            sv.crop_image(frame, xyxy)\n",
    "            for xyxy\n",
    "            in sv.clip_boxes(sv.pad_boxes(xyxy=number_detections.xyxy, px=10, py=10), (frame_w, frame_h))\n",
    "        ]\n",
    "        numbers = [\n",
    "            NUMBER_RECOGNITION_MODEL.predict(number_crop, NUMBER_RECOGNITION_MODEL_PROMPT)[0]\n",
    "            for number_crop\n",
    "            in number_crops\n",
    "        ]\n",
    "\n",
    "        # we use mask IoS to match numbers with players\n",
    "\n",
    "        iou = sv.mask_iou_batch(\n",
    "            masks_true=player_detections.mask,\n",
    "            masks_detection=number_detections.mask,\n",
    "            overlap_metric=sv.OverlapMetric.IOS\n",
    "        )\n",
    "\n",
    "        pairs = coords_above_threshold(iou, 0.9)\n",
    "\n",
    "        if pairs:\n",
    "\n",
    "            player_idx, number_idx = zip(*pairs)\n",
    "            player_idx = [i + 1 for i in player_idx]\n",
    "            number_idx = list(number_idx)\n",
    "\n",
    "            # we update number_validator state\n",
    "\n",
    "            numbers = [numbers[int(i)] for i in number_idx]\n",
    "            number_validator.update(tracker_ids=player_idx, values=numbers)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "Ul99bONuZp-I",
    "outputId": "bd4aa22a-9300-453a-d62e-4db3ae955e1d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Court keypoints detection"
   ],
   "metadata": {
    "id": "r-YW3b5Qi-U6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load keypoint detection model"
   ],
   "metadata": {
    "id": "OsKMDgdqjmnI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-result{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
    "\n",
    "team_colors = sv.ColorPalette.from_hex([\n",
    "    TEAM_COLORS[TEAM_NAMES[0]],\n",
    "    TEAM_COLORS[TEAM_NAMES[1]]\n",
    "])\n",
    "\n",
    "team_mask_annotator = sv.MaskAnnotator(\n",
    "    color=team_colors,\n",
    "    opacity=0.5,\n",
    "    color_lookup=sv.ColorLookup.INDEX)\n",
    "team_label_annotator = sv.RichLabelAnnotator(\n",
    "    font_path=f\"{HOME}/fonts/Staatliches-Regular.ttf\",\n",
    "    font_size=40,\n",
    "    color=team_colors,\n",
    "    text_color=sv.Color.WHITE,\n",
    "    text_position=sv.Position.BOTTOM_CENTER,\n",
    "    text_offset=(0, 10),\n",
    "    color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
    "    for frame, detections in tqdm(zip(frames_history, detections_history)):\n",
    "        detections = detections[detections.area > 100]\n",
    "\n",
    "        teams = team_validator.get_validated(tracker_ids=detections.tracker_id)\n",
    "        teams = np.array(teams).astype(int)\n",
    "        numbers = number_validator.get_validated(tracker_ids=detections.tracker_id)\n",
    "        numbers = np.array(numbers)\n",
    "\n",
    "        labels = [\n",
    "            f\"#{number} {TEAM_ROSTERS[TEAM_NAMES[team]].get(number)}\"\n",
    "            for number, team\n",
    "            in zip(numbers, teams)\n",
    "        ]\n",
    "\n",
    "        annotated_frame = frame.copy()\n",
    "        annotated_frame = team_mask_annotator.annotate(\n",
    "            scene=annotated_frame,\n",
    "            detections=detections,\n",
    "            custom_color_lookup=teams)\n",
    "        annotated_frame = team_label_annotator.annotate(\n",
    "            scene=annotated_frame,\n",
    "            detections=detections,\n",
    "            labels=labels,\n",
    "            custom_color_lookup=teams)\n",
    "\n",
    "        sink.write_frame(annotated_frame)\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ],
   "metadata": {
    "id": "eyHvSFpSjiDV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single frame keypoint detection"
   ],
   "metadata": {
    "id": "WqchilTtkWz6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "WJRCgmIBkRXx",
    "outputId": "84890fea-7d9f-4f0e-a1f5-dc143e986327"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Detecting keypoints with high confidence"
   ],
   "metadata": {
    "id": "qCcYa07Dk-Lf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "KEYPOINT_DETECTION_MODEL_ID = \"basketball-court-detection-2/14\"\n",
    "KEYPOINT_DETECTION_MODEL_CONFIDENCE = 0.3\n",
    "KEYPOINT_DETECTION_MODEL_ANCHOR_CONFIDENCE = 0.5\n",
    "KEYPOINT_DETECTION_MODEL = get_model(model_id=KEYPOINT_DETECTION_MODEL_ID)\n",
    "KEYPOINT_COLOR = sv.Color.from_hex('#FF1493')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "m4nHvCAGlDvF",
    "outputId": "2e4ba75c-f53f-4d09-fbe2-3727c3d5c8eb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Map player positions to court coordinates"
   ],
   "metadata": {
    "id": "3B_KM5MNlYBm"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single frame player position mapping"
   ],
   "metadata": {
    "id": "sdbDdYc10Pb-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "vertex_annotator = sv.VertexAnnotator(color=KEYPOINT_COLOR, radius=8)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = KEYPOINT_DETECTION_MODEL.infer(frame, confidence=KEYPOINT_DETECTION_MODEL_CONFIDENCE)[0]\n",
    "key_points = sv.KeyPoints.from_inference(result)\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = vertex_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    key_points=key_points)\n",
    "\n",
    "cv2.imwrite(f\"{images_dir}/plot_11.png\", cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "sv.plot_image(annotated_frame)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "z2r-NClenOfQ",
    "outputId": "53fcdca3-416d-4c2e-b545-d8ef2e0ba7b5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Full video player position mapping"
   ],
   "metadata": {
    "id": "TJiEzaE85jmW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "vertex_annotator = sv.VertexAnnotator(color=KEYPOINT_COLOR, radius=8)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = KEYPOINT_DETECTION_MODEL.infer(frame, confidence=KEYPOINT_DETECTION_MODEL_CONFIDENCE)[0]\n",
    "key_points = sv.KeyPoints.from_inference(result)\n",
    "key_points = key_points[:, key_points.confidence[0] > KEYPOINT_DETECTION_MODEL_ANCHOR_CONFIDENCE]\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = vertex_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    key_points=key_points)\n",
    "\n",
    "cv2.imwrite(f\"{images_dir}/plot_12.png\", cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "sv.plot_image(annotated_frame)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nzEhOEjQ3epJ",
    "outputId": "ec2b6ad5-b3bf-47e3-d77a-937fedb12d30"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "config = CourtConfiguration(league=League.NBA, measurement_unit=MeasurementUnit.FEET)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# define team annotators\n",
    "\n",
    "team_colors = sv.ColorPalette.from_hex([\n",
    "    TEAM_COLORS[TEAM_NAMES[0]],\n",
    "    TEAM_COLORS[TEAM_NAMES[1]]\n",
    "])\n",
    "\n",
    "team_box_annotator = sv.BoxAnnotator(\n",
    "    color=team_colors,\n",
    "    thickness=2,\n",
    "    color_lookup=sv.ColorLookup.INDEX\n",
    ")\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we determine the team for each player and assign a team ID to every detection\n",
    "\n",
    "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "crops = [sv.crop_image(frame, box) for box in boxes]\n",
    "TEAMS = np.array(team_classifier.predict(crops))\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = team_box_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections,\n",
    "    custom_color_lookup=TEAMS[detections.tracker_id - 1]\n",
    ")\n",
    "cv2.imwrite(f\"{images_dir}/plot_13.png\", cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "sv.plot_image(annotated_frame)\n",
    "\n",
    "# we use a keypoint model to detect court landmarks\n",
    "\n",
    "result = KEYPOINT_DETECTION_MODEL.infer(frame, confidence=KEYPOINT_DETECTION_MODEL_CONFIDENCE)[0]\n",
    "key_points = sv.KeyPoints.from_inference(result)\n",
    "landmarks_mask = key_points.confidence[0] > KEYPOINT_DETECTION_MODEL_ANCHOR_CONFIDENCE\n",
    "\n",
    "if np.count_nonzero(landmarks_mask) >= 4:\n",
    "\n",
    "    # we calculate homography matrix\n",
    "\n",
    "    court_landmarks = np.array(config.vertices)[landmarks_mask]\n",
    "    frame_landmarks = key_points[:, landmarks_mask].xy[0]\n",
    "\n",
    "    frame_to_court_transformer = ViewTransformer(\n",
    "        source=frame_landmarks,\n",
    "        target=court_landmarks,\n",
    "    )\n",
    "\n",
    "    frame_xy = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n",
    "\n",
    "    if len(frame_xy) > 0:\n",
    "\n",
    "        # we transform points\n",
    "\n",
    "        court_xy = frame_to_court_transformer.transform_points(points=frame_xy)\n",
    "\n",
    "        # we visualize the results\n",
    "\n",
    "        court = draw_court(config=config)\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=court_xy[TEAMS == 0],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[0]]),\n",
    "            court=court\n",
    "        )\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=court_xy[TEAMS == 1],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[1]]),\n",
    "            court=court\n",
    "        )\n",
    "\n",
    "        cv2.imwrite(f\"{images_dir}/plot_13.png\", cv2.cvtColor(court, cv2.COLOR_RGB2BGR))\n",
    "        sv.plot_image(court)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MAeep8b23zXs",
    "outputId": "e6521013-e344-4980-ff2c-9021bd84e33a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "video_xy = []\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we determine the team for each player and assign a team ID to every detection\n",
    "\n",
    "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "crops = [sv.crop_image(frame, box) for box in boxes]\n",
    "TEAMS = np.array(team_classifier.predict(crops))\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "for frame_idx, frame in tqdm(enumerate(frame_generator)):\n",
    "    detections = tracker.propagate(frame)\n",
    "\n",
    "    # we use a keypoint model to detect court landmarks\n",
    "\n",
    "    result = KEYPOINT_DETECTION_MODEL.infer(frame, confidence=KEYPOINT_DETECTION_MODEL_CONFIDENCE)[0]\n",
    "    key_points = sv.KeyPoints.from_inference(result)\n",
    "    landmarks_mask = key_points.confidence[0] > KEYPOINT_DETECTION_MODEL_ANCHOR_CONFIDENCE\n",
    "\n",
    "    if np.count_nonzero(landmarks_mask) >= 4:\n",
    "\n",
    "        # we calculate homography matrix\n",
    "\n",
    "        court_landmarks = np.array(config.vertices)[landmarks_mask]\n",
    "        frame_landmarks = key_points[:, landmarks_mask].xy[0]\n",
    "\n",
    "        frame_to_court_transformer = ViewTransformer(\n",
    "            source=frame_landmarks,\n",
    "            target=court_landmarks,\n",
    "        )\n",
    "\n",
    "        frame_xy = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n",
    "        court_xy = frame_to_court_transformer.transform_points(points=frame_xy)\n",
    "        video_xy.append(court_xy)\n",
    "\n",
    "video_xy = np.array(video_xy)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "ZeSmIUzW4SHN",
    "outputId": "65fc71a2-5e0c-403a-e7ed-0ac86fa1c55b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean player movement paths\n",
    "\n",
    "We are detecting sudden jumps in position using robust speed analysis. We are removing short abnormal runs and nearby frames to eliminate teleport-like artifacts. We are filling missing segments with linear interpolation to ensure continuous motion. We are smoothing all paths with a Savitzkyâ€“Golay filter to achieve stable and natural movement."
   ],
   "metadata": {
    "id": "BUhD1VKNM41D"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-map{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "config = CourtConfiguration(league=League.NBA, measurement_unit=MeasurementUnit.FEET)\n",
    "court = draw_court(config=config)\n",
    "court_h, court_w, _ = court.shape\n",
    "\n",
    "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
    "video_info.width = court_w\n",
    "video_info.height = court_h\n",
    "\n",
    "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
    "    for frame_xy in tqdm(video_xy):\n",
    "        court = draw_court(config=config)\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=frame_xy[TEAMS == 0],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[0]]),\n",
    "            court=court\n",
    "        )\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=frame_xy[TEAMS == 1],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[1]]),\n",
    "            court=court\n",
    "        )\n",
    "\n",
    "        sink.write_frame(court)\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "id": "8gYSJSEm7KG-",
    "outputId": "deec75cb-3d81-49cc-f1f8-2386835e4e92"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ],
   "metadata": {
    "id": "7T6ImF7A-9JF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "court = draw_paths_on_court(\n",
    "    config=config,\n",
    "    paths=[video_xy[:, 0, :]],\n",
    ")\n",
    "\n",
    "cv2.imwrite(f\"{images_dir}/plot_14.png\", cv2.cvtColor(court, cv2.COLOR_RGB2BGR))\n",
    "sv.plot_image(court)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "id": "ZWjBXVMZNxAa",
    "outputId": "e17752a9-efe9-4e2f-b1db-98177bf2fed6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "cleaned_xy, edited_mask = clean_paths(\n",
    "    video_xy,\n",
    "    jump_sigma=3.5,\n",
    "    min_jump_dist=0.6,\n",
    "    max_jump_run=18,\n",
    "    pad_around_runs=2,\n",
    "    smooth_window=9,\n",
    "    smooth_poly=2,\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "id": "waPGh4My_Bs6",
    "outputId": "02a0bc09-a92f-4c1a-aef1-117a53d5d64f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def split_true_runs(mask: np.ndarray, coords: np.ndarray) -> list[np.ndarray]:\n",
    "    mask = mask.squeeze()\n",
    "    idx = np.flatnonzero(mask)\n",
    "    if idx.size == 0:\n",
    "        return []\n",
    "    splits = np.where(np.diff(idx) > 1)[0] + 1\n",
    "    groups = np.split(idx, splits)\n",
    "    return [coords[g, 0, :] for g in groups]\n",
    "\n",
    "\n",
    "court = draw_paths_on_court(\n",
    "    config=config,\n",
    "    paths=[video_xy[:, 0, :]],\n",
    "    color=sv.Color.GREEN,\n",
    ")\n",
    "\n",
    "court = draw_paths_on_court(\n",
    "    config=config,\n",
    "    paths=split_true_runs(edited_mask[:, 0], video_xy),\n",
    "    color=sv.Color.RED,\n",
    "    court=court\n",
    ")\n",
    "\n",
    "cv2.imwrite(f\"{images_dir}/plot_15.png\", cv2.cvtColor(court, cv2.COLOR_RGB2BGR))\n",
    "sv.plot_image(court)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JxCoYbu5_HSG",
    "outputId": "ffc4bb34-acdb-42d7-bef2-cd8dbe5adcc6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test = draw_paths_on_court(\n",
    "    config=config,\n",
    "    paths=[cleaned_xy[:, 0, :]],\n",
    ")\n",
    "\n",
    "cv2.imwrite(f\"{images_dir}/plot_16.png\", cv2.cvtColor(test, cv2.COLOR_RGB2BGR))\n",
    "sv.plot_image(test)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "S_jZI2P8_SSo",
    "outputId": "2b5990f2-7272-4b28-b1d1-9bf59285fb5a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classify shorts as made or miss"
   ],
   "metadata": {
    "id": "t6bu0UouKA1x"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Keep only \"player-jump-shot\" class"
   ],
   "metadata": {
    "id": "hie0heVdMY06"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-map{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "config = CourtConfiguration(league=League.NBA, measurement_unit=MeasurementUnit.FEET)\n",
    "court = draw_court(config=config)\n",
    "court_h, court_w, _ = court.shape\n",
    "\n",
    "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
    "video_info.width = court_w\n",
    "video_info.height = court_h\n",
    "\n",
    "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
    "    for frame_xy in tqdm(cleaned_xy):\n",
    "        court = draw_court(config=config)\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=frame_xy[TEAMS == 0],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[0]]),\n",
    "            court=court\n",
    "        )\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=frame_xy[TEAMS == 1],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[1]]),\n",
    "            court=court\n",
    "        )\n",
    "\n",
    "        sink.write_frame(court)\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ],
   "metadata": {
    "id": "n3CU5QQ2KYtO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ],
   "metadata": {
    "id": "KV9G1ZnNLf4v",
    "outputId": "79247f80-69d5-4dd5-b686-56dc37b3b021",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mark jump-shot location on the court"
   ],
   "metadata": {
    "id": "gNsjk7_PN1fW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "config = CourtConfiguration(league=League.NBA, measurement_unit=MeasurementUnit.FEET)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH, start=FRAME_IDX, iterative_seek=True)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# we use a RF-DETR model to detect players\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[detections.class_id == PLAYER_JUMP_SHOT_CLASS_ID]\n",
    "\n",
    "# we use a keypoint model to detect court landmarks\n",
    "\n",
    "result = KEYPOINT_DETECTION_MODEL.infer(frame, confidence=KEYPOINT_DETECTION_MODEL_CONFIDENCE)[0]\n",
    "key_points = sv.KeyPoints.from_inference(result)\n",
    "landmarks_mask = key_points.confidence[0] > KEYPOINT_DETECTION_MODEL_ANCHOR_CONFIDENCE\n",
    "\n",
    "if np.count_nonzero(landmarks_mask) >= 4:\n",
    "\n",
    "    # we calculate homography matrix\n",
    "\n",
    "    court_landmarks = np.array(config.vertices)[landmarks_mask]\n",
    "    frame_landmarks = key_points[:, landmarks_mask].xy[0]\n",
    "\n",
    "    frame_to_court_transformer = ViewTransformer(\n",
    "        source=frame_landmarks,\n",
    "        target=court_landmarks,\n",
    "    )\n",
    "\n",
    "    frame_xy = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n",
    "\n",
    "    # transform video frame coordinates into court coordinates\n",
    "\n",
    "    court_xy = frame_to_court_transformer.transform_points(points=frame_xy)\n",
    "\n",
    "    court = draw_made_and_miss_on_court(\n",
    "        config=config,\n",
    "        made_xy=court_xy,\n",
    "        made_size=25,\n",
    "        made_color=sv.Color.from_hex(\"#007A33\"),\n",
    "        made_thickness=6,\n",
    "        line_thickness=4\n",
    "    )\n",
    "\n",
    "    sv.plot_image(court)"
   ],
   "metadata": {
    "id": "HDS4Ug9gN4or",
    "outputId": "866940fa-500a-49ec-ffb3-bee1c10003bd",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Detect shot events"
   ],
   "metadata": {
    "id": "-R6foiAhRaAd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "BALL_IN_BASKET_CLASS_ID = 1\n",
    "JUMP_SHOT_CLASS_ID = 5\n",
    "LAYUP_DUNK_CLASS_ID = 6\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
    "\n",
    "shot_event_tracker = ShotEventTracker(\n",
    "    reset_time_frames=int(video_info.fps * 1.7),\n",
    "    minimum_frames_between_starts=int(video_info.fps * 0.5),\n",
    "    cooldown_frames_after_made=int(video_info.fps * 0.5),\n",
    ")\n",
    "\n",
    "for frame_index, frame in enumerate(frame_generator):\n",
    "\n",
    "    # we use a RF-DETR model to detect jump shot, layup, dunk and ball in basket\n",
    "\n",
    "    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "    detections = sv.Detections.from_inference(result)\n",
    "\n",
    "    has_jump_shot = len(detections[detections.class_id == JUMP_SHOT_CLASS_ID]) > 0\n",
    "    has_layup_dunk = len(detections[detections.class_id == LAYUP_DUNK_CLASS_ID]) > 0\n",
    "    has_ball_in_basket = len(detections[detections.class_id == BALL_IN_BASKET_CLASS_ID]) > 0\n",
    "\n",
    "    events = shot_event_tracker.update(\n",
    "        frame_index=frame_index,\n",
    "        has_jump_shot=has_jump_shot,\n",
    "        has_layup_dunk=has_layup_dunk,\n",
    "        has_ball_in_basket=has_ball_in_basket,\n",
    "    )\n",
    "\n",
    "    if events:\n",
    "        print(events)\n",
    "\n",
    "        annotated_frame = frame.copy()\n",
    "        annotated_frame = box_annotator.annotate(\n",
    "            scene=annotated_frame,\n",
    "            detections=detections)\n",
    "        annotated_frame = label_annotator.annotate(\n",
    "            scene=annotated_frame,\n",
    "            detections=detections)\n",
    "\n",
    "        sv.plot_image(annotated_frame)"
   ],
   "metadata": {
    "id": "4kH3iKTmRhx_",
    "outputId": "18414e96-e8b5-4484-ad78-a721a6590426",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div align=\"center\">\n",
    "  <p>\n",
    "    Looking for more tutorials or have questions?\n",
    "    Check out our <a href=\"https://github.com/roboflow/notebooks\">GitHub repo</a> for more notebooks,\n",
    "    or visit our <a href=\"https://discord.gg/GbfgXGJ8Bk\">discord</a>.\n",
    "  </p>\n",
    "  \n",
    "  <p>\n",
    "    <strong>If you found this helpful, please consider giving us a â­\n",
    "    <a href=\"https://github.com/roboflow/notebooks\">on GitHub</a>!</strong>\n",
    "  </p>\n",
    "\n",
    "</div>"
   ],
   "metadata": {
    "id": "ARvSRBTLaZXD"
   }
  }
 ]
}
