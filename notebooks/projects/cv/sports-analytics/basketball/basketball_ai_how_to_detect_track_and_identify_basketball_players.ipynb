{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
    "os.environ[\"ROBOFLOW_API_KEY\"] = userdata.get(\"ROBOFLOW_API_KEY\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!nvidia-smi"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "HOME = Path.cwd()\n",
    "print(\"HOME:\", HOME)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!git clone https://github.com/Gy920/segment-anything-2-real-time.git\n",
    "%cd {HOME}/segment-anything-2-real-time\n",
    "!pip install -e . -q\n",
    "!python setup.py build_ext --inplace"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!(cd checkpoints && bash download_ckpts.sh)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install -q gdown\n",
    "!pip install -q inference-gpu\n",
    "\n",
    "!pip install supervision==0.27.0rc4\n",
    "!pip install -q git+https://github.com/roboflow/sports.git@feat/basketball\n",
    "\n",
    "!pip install -q transformers num2words\n",
    "!pip install -q flash-attn --no-build-isolation"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ONNXRUNTIME_EXECUTION_PROVIDERS\"] = \"[CUDAExecutionProvider]\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "SOURCE_VIDEO_DIRECTORY = HOME / \"source\"\n",
    "\n",
    "!gdown -q https://drive.google.com/drive/folders/1eDJYqQ77Fytz15tKGdJCMeYSgmoQ-2-H -O {SOURCE_VIDEO_DIRECTORY} --folder"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!ls -la {SOURCE_VIDEO_DIRECTORY}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "\n",
    "SOURCE_VIDEO_PATH = SOURCE_VIDEO_DIRECTORY / \"boston-celtics-new-york-knicks-game-1-q1-04.28-04.20.mp4\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "TEAM_ROSTERS = {\n",
    "  \"New York Knicks\": {\n",
    "    \"55\": \"Hukporti\",\n",
    "    \"1\": \"Payne\",\n",
    "    \"0\": \"Wright\",\n",
    "    \"11\": \"Brunson\",\n",
    "    \"3\": \"Hart\",\n",
    "    \"32\": \"Towns\",\n",
    "    \"44\": \"Shamet\",\n",
    "    \"25\": \"Bridges\",\n",
    "    \"2\": \"McBride\",\n",
    "    \"23\": \"Robinson\",\n",
    "    \"8\": \"Anunoby\",\n",
    "    \"4\": \"Dadiet\",\n",
    "    \"5\": \"Achiuwa\",\n",
    "    \"13\": \"Kolek\"\n",
    "  },\n",
    "  \"Boston Celtics\": {\n",
    "    \"42\": \"Horford\",\n",
    "    \"55\": \"Scheierman\",\n",
    "    \"9\": \"White\",\n",
    "    \"20\": \"Davison\",\n",
    "    \"7\": \"Brown\",\n",
    "    \"0\": \"Tatum\",\n",
    "    \"27\": \"Walsh\",\n",
    "    \"4\": \"Holiday\",\n",
    "    \"8\": \"Porzingis\",\n",
    "    \"40\": \"Kornet\",\n",
    "    \"88\": \"Queta\",\n",
    "    \"11\": \"Pritchard\",\n",
    "    \"30\": \"Hauser\",\n",
    "    \"12\": \"Craig\",\n",
    "    \"26\": \"Tillman\"\n",
    "  }\n",
    "}\n",
    "\n",
    "TEAM_COLORS = {\n",
    "    \"New York Knicks\": \"#006BB6\",\n",
    "    \"Boston Celtics\": \"#007A33\"\n",
    "}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from IPython.display import Video\n",
    "from typing import Dict, List, Optional, Union, Iterable, Tuple\n",
    "from operator import itemgetter\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import supervision as sv\n",
    "from inference import get_model\n",
    "from sports import (\n",
    "    clean_paths,\n",
    "    ConsecutiveValueTracker,\n",
    "    TeamClassifier,\n",
    "    MeasurementUnit,\n",
    "    ViewTransformer\n",
    ")\n",
    "from sports.basketball import (\n",
    "    CourtConfiguration,\n",
    "    League,\n",
    "    draw_court,\n",
    "    draw_points_on_court,\n",
    "    draw_paths_on_court\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "PLAYER_DETECTION_MODEL_ID = \"basketball-player-detection-3-ycjdo/4\"\n",
    "PLAYER_DETECTION_MODEL_CONFIDENCE = 0.4\n",
    "PLAYER_DETECTION_MODEL_IOU_THRESHOLD = 0.9\n",
    "PLAYER_DETECTION_MODEL = get_model(model_id=PLAYER_DETECTION_MODEL_ID)\n",
    "\n",
    "COLOR = sv.ColorPalette.from_hex([\n",
    "    \"#ffff00\", \"#ff9b00\", \"#ff66ff\", \"#3399ff\", \"#ff66b2\", \"#ff8080\",\n",
    "    \"#b266ff\", \"#9999ff\", \"#66ffff\", \"#33ff99\", \"#66ff66\", \"#99ff00\"\n",
    "])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = box_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "annotated_frame = label_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "\n",
    "sv.plot_image(annotated_frame)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "NUMBER_CLASS_ID = 2\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[detections.class_id == NUMBER_CLASS_ID]\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = box_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "annotated_frame = label_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "\n",
    "sv.plot_image(annotated_frame)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "PLAYER_CLASS_IDS = [3, 4, 5, 6, 7] # player, player-in-possession, player-jump-shot, player-layup-dunk, player-shot-block\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = box_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "annotated_frame = label_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "\n",
    "sv.plot_image(annotated_frame)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-detection{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-detection{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "    detections = sv.Detections.from_inference(result)\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = box_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=detections)\n",
    "    annotated_frame = label_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=detections)\n",
    "    return annotated_frame\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path=TARGET_VIDEO_PATH,\n",
    "    callback=callback,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%cd $HOME/segment-anything-2-real-time\n",
    "\n",
    "from sam2.build_sam import build_sam2_camera_predictor\n",
    "\n",
    "SAM2_CHECKPOINT = \"checkpoints/sam2.1_hiera_large.pt\"\n",
    "SAM2_CONFIG = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "predictor = build_sam2_camera_predictor(SAM2_CONFIG, SAM2_CHECKPOINT)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "class SAM2Tracker:\n",
    "    def __init__(self, predictor) -> None:\n",
    "        self.predictor = predictor\n",
    "        self._prompted = False\n",
    "\n",
    "    def prompt_first_frame(self, frame: np.ndarray, detections: sv.Detections) -> None:\n",
    "        if len(detections) == 0:\n",
    "            raise ValueError(\"detections must contain at least one box\")\n",
    "\n",
    "        if detections.tracker_id is None:\n",
    "            detections.tracker_id = list(range(1, len(detections) + 1))\n",
    "\n",
    "        with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            self.predictor.load_first_frame(frame)\n",
    "            for xyxy, obj_id in zip(detections.xyxy, detections.tracker_id):\n",
    "                bbox = np.asarray([xyxy], dtype=np.float32)\n",
    "                self.predictor.add_new_prompt(\n",
    "                    frame_idx=0,\n",
    "                    obj_id=int(obj_id),\n",
    "                    bbox=bbox,\n",
    "                )\n",
    "\n",
    "        self._prompted = True\n",
    "\n",
    "    def propagate(self, frame: np.ndarray) -> sv.Detections:\n",
    "        if not self._prompted:\n",
    "            raise RuntimeError(\"Call prompt_first_frame before propagate\")\n",
    "\n",
    "        with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            tracker_ids, mask_logits = self.predictor.track(frame)\n",
    "\n",
    "        tracker_ids = np.asarray(tracker_ids, dtype=np.int32)\n",
    "        masks = (mask_logits > 0.0).cpu().numpy()\n",
    "        masks = np.squeeze(masks).astype(bool)\n",
    "\n",
    "        if masks.ndim == 2:\n",
    "            masks = masks[None, ...]\n",
    "\n",
    "        masks = np.array([\n",
    "            sv.filter_segments_by_distance(mask, relative_distance=0.03, mode=\"edge\")\n",
    "            for mask in masks\n",
    "        ])\n",
    "\n",
    "        xyxy = sv.mask_to_xyxy(masks=masks)\n",
    "        detections = sv.Detections(xyxy=xyxy, mask=masks, tracker_id=tracker_ids)\n",
    "        return detections\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self._prompted = False"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-mask{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "# define team annotators\n",
    "\n",
    "mask_annotator = sv.MaskAnnotator(\n",
    "    color=COLOR,\n",
    "    color_lookup=sv.ColorLookup.TRACK,\n",
    "    opacity=0.5)\n",
    "box_annotator = sv.BoxAnnotator(\n",
    "    color=COLOR,\n",
    "    color_lookup=sv.ColorLookup.TRACK,\n",
    "    thickness=2\n",
    ")\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = box_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "sv.plot_image(annotated_frame)\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    detections = tracker.propagate(frame)\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = mask_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "    annotated_frame = box_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "    return annotated_frame\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path=TARGET_VIDEO_PATH,\n",
    "    callback=callback,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "STRIDE = 30\n",
    "\n",
    "crops = []\n",
    "\n",
    "for video_path in sv.list_files_with_extensions(SOURCE_VIDEO_DIRECTORY, extensions=[\"mp4\", \"avi\", \"mov\"]):\n",
    "    frame_generator = sv.get_video_frames_generator(source_path=video_path, stride=STRIDE)\n",
    "\n",
    "    for frame in tqdm(frame_generator):\n",
    "\n",
    "        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD, class_agnostic_nms=True)[0]\n",
    "        detections = sv.Detections.from_inference(result)\n",
    "        detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "\n",
    "        boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "        for box in boxes:\n",
    "            crops.append(sv.crop_image(frame, box))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "sv.plot_images_grid(\n",
    "    images=crops[:100],\n",
    "    grid_size=(10, 10),\n",
    "    size=(10, 10)\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "team_classifier = TeamClassifier(device=\"cuda\")\n",
    "team_classifier.fit(crops)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "teams = team_classifier.predict(crops)\n",
    "\n",
    "team_0 = [crop for crop, team in zip(crops, teams) if team == 0]\n",
    "team_1 = [crop for crop, team in zip(crops, teams) if team == 1]\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=team_0[:50],\n",
    "    grid_size=(5, 10),\n",
    "    size=(10, 5)\n",
    ")\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=team_1[:50],\n",
    "    grid_size=(5, 10),\n",
    "    size=(10, 5)\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD, class_agnostic_nms=True)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "\n",
    "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "crops = [sv.crop_image(frame, box) for box in boxes]\n",
    "teams = np.array(team_classifier.predict(crops))\n",
    "\n",
    "team_0 = [crop for crop, team in zip(crops, teams) if team == 0]\n",
    "team_1 = [crop for crop, team in zip(crops, teams) if team == 1]\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=team_0[:10],\n",
    "    grid_size=(1, 10),\n",
    "    size=(10, 1)\n",
    ")\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=team_1[:10],\n",
    "    grid_size=(1, 10),\n",
    "    size=(10, 1)\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "TEAM_NAMES = {\n",
    "    0: \"New York Knicks\",\n",
    "    1: \"Boston Celtics\",\n",
    "}\n",
    "\n",
    "# TEAM_NAMES = {\n",
    "#     0: \"Boston Celtics\",\n",
    "#     1: \"New York Knicks\",\n",
    "# }"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-teams{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# define team annotators\n",
    "\n",
    "team_colors = sv.ColorPalette.from_hex([\n",
    "    TEAM_COLORS[TEAM_NAMES[0]],\n",
    "    TEAM_COLORS[TEAM_NAMES[1]]\n",
    "])\n",
    "\n",
    "team_mask_annotator = sv.MaskAnnotator(\n",
    "    color=team_colors,\n",
    "    opacity=0.5,\n",
    "    color_lookup=sv.ColorLookup.INDEX\n",
    ")\n",
    "\n",
    "team_box_annotator = sv.BoxAnnotator(\n",
    "    color=team_colors,\n",
    "    thickness=2,\n",
    "    color_lookup=sv.ColorLookup.INDEX\n",
    ")\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we determine the team for each player and assign a team ID to every detection\n",
    "\n",
    "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "crops = [sv.crop_image(frame, box) for box in boxes]\n",
    "TEAMS = np.array(team_classifier.predict(crops))\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    detections = tracker.propagate(frame)\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = team_mask_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=detections,\n",
    "        custom_color_lookup=TEAMS[detections.tracker_id - 1]\n",
    "    )\n",
    "    annotated_frame = team_box_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=detections,\n",
    "        custom_color_lookup=TEAMS[detections.tracker_id - 1]\n",
    "    )\n",
    "    return annotated_frame\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path=TARGET_VIDEO_PATH,\n",
    "    callback=callback,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "NUMBER_RECOGNITION_MODEL_ID = \"basketball-jersey-numbers-ocr/3\"\n",
    "NUMBER_RECOGNITION_MODEL = get_model(model_id=NUMBER_RECOGNITION_MODEL_ID)\n",
    "NUMBER_RECOGNITION_MODEL_PROMPT = \"Read the number.\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "frame_h, frame_w, *_ = frame.shape\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[detections.class_id == NUMBER_CLASS_ID]\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = box_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "annotated_frame = label_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections)\n",
    "\n",
    "sv.plot_image(annotated_frame)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "crops = [\n",
    "    sv.resize_image(sv.crop_image(frame, xyxy), resolution_wh=(224, 224))\n",
    "    for xyxy\n",
    "    in sv.clip_boxes(sv.pad_boxes(xyxy=detections.xyxy, px=10, py=10), (frame_w, frame_h))\n",
    "]\n",
    "numbers = [\n",
    "    NUMBER_RECOGNITION_MODEL.predict(crop, NUMBER_RECOGNITION_MODEL_PROMPT)[0]\n",
    "    for crop\n",
    "    in crops\n",
    "]\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=crops[:10],\n",
    "    titles=numbers[:10],\n",
    "    grid_size=(1, 10),\n",
    "    size=(10, 1)\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def coords_above_threshold(\n",
    "    matrix: np.ndarray, threshold: float, sort_desc: bool = True\n",
    ") -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Return all (row_index, col_index) where value > threshold.\n",
    "    Rows and columns may repeat.\n",
    "    Optionally sort by value descending.\n",
    "    \"\"\"\n",
    "    A = np.asarray(matrix)\n",
    "    rows, cols = np.where(A > threshold)\n",
    "    pairs = list(zip(rows.tolist(), cols.tolist()))\n",
    "    if sort_desc:\n",
    "        pairs.sort(key=lambda rc: A[rc[0], rc[1]], reverse=True)\n",
    "    return pairs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# define team annotators\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=4, color_lookup=sv.ColorLookup.TRACK)\n",
    "\n",
    "player_mask_annotator = sv.MaskAnnotator(color=COLOR.by_idx(3), opacity=0.8, color_lookup=sv.ColorLookup.INDEX)\n",
    "number_mask_annotator = sv.MaskAnnotator(color=COLOR.by_idx(0), opacity=0.8, color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "for index, frame in tqdm(enumerate(frame_generator)):\n",
    "\n",
    "    # we only process the first video frame\n",
    "\n",
    "    if index > 0:\n",
    "        break\n",
    "\n",
    "    frame_h, frame_w, *_ = frame.shape\n",
    "\n",
    "    player_detections = tracker.propagate(frame)\n",
    "\n",
    "    # we use RF-DETR model to detect numbers\n",
    "\n",
    "    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "    number_detections = sv.Detections.from_inference(result)\n",
    "    number_detections = number_detections[number_detections.class_id == NUMBER_CLASS_ID]\n",
    "    number_detections.mask = sv.xyxy_to_mask(boxes=number_detections.xyxy, resolution_wh=(frame_w, frame_h))\n",
    "\n",
    "    # we use mask IoS to match numbers with players\n",
    "\n",
    "    iou = sv.mask_iou_batch(\n",
    "        masks_true=player_detections.mask,\n",
    "        masks_detection=number_detections.mask,\n",
    "        overlap_metric=sv.OverlapMetric.IOS\n",
    "    )\n",
    "\n",
    "    pairs = coords_above_threshold(iou, 0.9)\n",
    "    player_idx, number_idx = zip(*pairs)\n",
    "\n",
    "    # we visualize all the masks\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = player_mask_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=player_detections)\n",
    "    annotated_frame = number_mask_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=number_detections)\n",
    "    sv.plot_image(annotated_frame)\n",
    "\n",
    "    # we visualize only matched pairs\n",
    "\n",
    "    player_detections = player_detections[np.array(player_idx)]\n",
    "    number_detections = number_detections[np.array(number_idx)]\n",
    "    number_detections.tracker_id = player_detections.tracker_id\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = box_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=player_detections)\n",
    "    annotated_frame = box_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=number_detections)\n",
    "    sv.plot_image(annotated_frame)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-validated-numbers{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "number_validator = ConsecutiveValueTracker(n_consecutive=3)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# define team annotators\n",
    "\n",
    "mask_annotator = sv.MaskAnnotator(\n",
    "    color=COLOR,\n",
    "    color_lookup=sv.ColorLookup.TRACK,\n",
    "    opacity=0.7)\n",
    "box_annotator = sv.BoxAnnotator(\n",
    "    color=COLOR,\n",
    "    color_lookup=sv.ColorLookup.TRACK,\n",
    "    thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    color=COLOR,\n",
    "    color_lookup=sv.ColorLookup.TRACK,\n",
    "    text_color=sv.Color.BLACK,\n",
    "    text_scale=0.8)\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    player_detections = tracker.propagate(frame)\n",
    "\n",
    "    # we perform number recognition at specific frame intervals\n",
    "\n",
    "    if index % 5 == 0:\n",
    "        frame_h, frame_w, *_ = frame.shape\n",
    "\n",
    "        # we use RF-DETR model to detect numbers\n",
    "\n",
    "        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "        number_detections = sv.Detections.from_inference(result)\n",
    "        number_detections = number_detections[number_detections.class_id == NUMBER_CLASS_ID]\n",
    "        number_detections.mask = sv.xyxy_to_mask(boxes=number_detections.xyxy, resolution_wh=(frame_w, frame_h))\n",
    "\n",
    "        # we crop out numbers detection and run recognition model\n",
    "\n",
    "        number_crops = [\n",
    "            sv.crop_image(frame, xyxy)\n",
    "            for xyxy\n",
    "            in sv.clip_boxes(sv.pad_boxes(xyxy=number_detections.xyxy, px=10, py=10), (frame_w, frame_h))\n",
    "        ]\n",
    "        numbers = [\n",
    "            NUMBER_RECOGNITION_MODEL.predict(number_crop, NUMBER_RECOGNITION_MODEL_PROMPT)[0]\n",
    "            for number_crop\n",
    "            in number_crops\n",
    "        ]\n",
    "\n",
    "        # we use mask IoS to match numbers with players\n",
    "\n",
    "        iou = sv.mask_iou_batch(\n",
    "            masks_true=player_detections.mask,\n",
    "            masks_detection=number_detections.mask,\n",
    "            overlap_metric=sv.OverlapMetric.IOS\n",
    "        )\n",
    "\n",
    "        pairs = coords_above_threshold(iou, 0.9)\n",
    "\n",
    "        if pairs:\n",
    "\n",
    "            player_idx, number_idx = zip(*pairs)\n",
    "            player_idx = [i + 1 for i in player_idx]\n",
    "            number_idx = list(number_idx)\n",
    "\n",
    "            # we update number_validator state\n",
    "\n",
    "            numbers = [numbers[int(i)] for i in number_idx]\n",
    "            number_validator.update(tracker_ids=player_idx, values=numbers)\n",
    "\n",
    "    # we visualize boxes and masks\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame = mask_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=player_detections)\n",
    "    annotated_frame = box_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=player_detections)\n",
    "\n",
    "    # we extract validated numbers\n",
    "\n",
    "    numbers = number_validator.get_validated(tracker_ids=player_detections.tracker_id)\n",
    "\n",
    "    # we visualize numbers\n",
    "\n",
    "    annotated_frame = label_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=player_detections,\n",
    "        labels=numbers)\n",
    "\n",
    "    return annotated_frame\n",
    "\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path=TARGET_VIDEO_PATH,\n",
    "    callback=callback,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!gdown https://drive.google.com/drive/folders/1RBjpI5Xleb58lujeusxH0W5zYMMA4ytO -O {HOME / \"fonts\"} --folder"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "frames_history = []\n",
    "detections_history = []\n",
    "\n",
    "number_validator = ConsecutiveValueTracker(n_consecutive=3)\n",
    "team_validator = ConsecutiveValueTracker(n_consecutive=1)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we determine the team for each player and assign a team ID to every detection\n",
    "\n",
    "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "crops = [sv.crop_image(frame, box) for box in boxes]\n",
    "TEAMS = np.array(team_classifier.predict(crops))\n",
    "\n",
    "team_validator.update(tracker_ids=detections.tracker_id, values=TEAMS)\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "\n",
    "for index, frame in tqdm(enumerate(frame_generator)):\n",
    "    player_detections = tracker.propagate(frame)\n",
    "    frames_history.append(frame)\n",
    "    detections_history.append(player_detections)\n",
    "\n",
    "    # we perform number recognition at specific frame intervals\n",
    "\n",
    "    if index % 5 == 0:\n",
    "        frame_h, frame_w, *_ = frame.shape\n",
    "\n",
    "        # we use RF-DETR model to detect numbers\n",
    "\n",
    "        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "        number_detections = sv.Detections.from_inference(result)\n",
    "        number_detections = number_detections[number_detections.class_id == NUMBER_CLASS_ID]\n",
    "        number_detections.mask = sv.xyxy_to_mask(boxes=number_detections.xyxy, resolution_wh=(frame_w, frame_h))\n",
    "\n",
    "        # we crop out numbers detection and run recognition model\n",
    "\n",
    "        number_crops = [\n",
    "            sv.crop_image(frame, xyxy)\n",
    "            for xyxy\n",
    "            in sv.clip_boxes(sv.pad_boxes(xyxy=number_detections.xyxy, px=10, py=10), (frame_w, frame_h))\n",
    "        ]\n",
    "        numbers = [\n",
    "            NUMBER_RECOGNITION_MODEL.predict(number_crop, NUMBER_RECOGNITION_MODEL_PROMPT)[0]\n",
    "            for number_crop\n",
    "            in number_crops\n",
    "        ]\n",
    "\n",
    "        # we use mask IoS to match numbers with players\n",
    "\n",
    "        iou = sv.mask_iou_batch(\n",
    "            masks_true=player_detections.mask,\n",
    "            masks_detection=number_detections.mask,\n",
    "            overlap_metric=sv.OverlapMetric.IOS\n",
    "        )\n",
    "\n",
    "        pairs = coords_above_threshold(iou, 0.9)\n",
    "\n",
    "        if pairs:\n",
    "\n",
    "            player_idx, number_idx = zip(*pairs)\n",
    "            player_idx = [i + 1 for i in player_idx]\n",
    "            number_idx = list(number_idx)\n",
    "\n",
    "            # we update number_validator state\n",
    "\n",
    "            numbers = [numbers[int(i)] for i in number_idx]\n",
    "            number_validator.update(tracker_ids=player_idx, values=numbers)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-result{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
    "\n",
    "team_colors = sv.ColorPalette.from_hex([\n",
    "    TEAM_COLORS[TEAM_NAMES[0]],\n",
    "    TEAM_COLORS[TEAM_NAMES[1]]\n",
    "])\n",
    "\n",
    "team_mask_annotator = sv.MaskAnnotator(\n",
    "    color=team_colors,\n",
    "    opacity=0.5,\n",
    "    color_lookup=sv.ColorLookup.INDEX)\n",
    "team_label_annotator = sv.RichLabelAnnotator(\n",
    "    font_path=f\"{HOME}/fonts/Staatliches-Regular.ttf\",\n",
    "    font_size=40,\n",
    "    color=team_colors,\n",
    "    text_color=sv.Color.WHITE,\n",
    "    text_position=sv.Position.BOTTOM_CENTER,\n",
    "    text_offset=(0, 10),\n",
    "    color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
    "    for frame, detections in tqdm(zip(frames_history, detections_history)):\n",
    "        detections = detections[detections.area > 100]\n",
    "\n",
    "        teams = team_validator.get_validated(tracker_ids=detections.tracker_id)\n",
    "        teams = np.array(teams).astype(int)\n",
    "        numbers = number_validator.get_validated(tracker_ids=detections.tracker_id)\n",
    "        numbers = np.array(numbers)\n",
    "\n",
    "        labels = [\n",
    "            f\"#{number} {TEAM_ROSTERS[TEAM_NAMES[team]].get(number)}\"\n",
    "            for number, team\n",
    "            in zip(numbers, teams)\n",
    "        ]\n",
    "\n",
    "        annotated_frame = frame.copy()\n",
    "        annotated_frame = team_mask_annotator.annotate(\n",
    "            scene=annotated_frame,\n",
    "            detections=detections,\n",
    "            custom_color_lookup=teams)\n",
    "        annotated_frame = team_label_annotator.annotate(\n",
    "            scene=annotated_frame,\n",
    "            detections=detections,\n",
    "            labels=labels,\n",
    "            custom_color_lookup=teams)\n",
    "\n",
    "        sink.write_frame(annotated_frame)\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "KEYPOINT_DETECTION_MODEL_ID = \"basketball-court-detection-2/14\"\n",
    "KEYPOINT_DETECTION_MODEL_CONFIDENCE = 0.3\n",
    "KEYPOINT_DETECTION_MODEL_ANCHOR_CONFIDENCE = 0.5\n",
    "KEYPOINT_DETECTION_MODEL = get_model(model_id=KEYPOINT_DETECTION_MODEL_ID)\n",
    "KEYPOINT_COLOR = sv.Color.from_hex('#FF1493')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "vertex_annotator = sv.VertexAnnotator(color=KEYPOINT_COLOR, radius=8)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = KEYPOINT_DETECTION_MODEL.infer(frame, confidence=KEYPOINT_DETECTION_MODEL_CONFIDENCE)[0]\n",
    "key_points = sv.KeyPoints.from_inference(result)\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = vertex_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    key_points=key_points)\n",
    "\n",
    "sv.plot_image(annotated_frame)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "vertex_annotator = sv.VertexAnnotator(color=KEYPOINT_COLOR, radius=8)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = KEYPOINT_DETECTION_MODEL.infer(frame, confidence=KEYPOINT_DETECTION_MODEL_CONFIDENCE)[0]\n",
    "key_points = sv.KeyPoints.from_inference(result)\n",
    "key_points = key_points[:, key_points.confidence[0] > KEYPOINT_DETECTION_MODEL_ANCHOR_CONFIDENCE]\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = vertex_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    key_points=key_points)\n",
    "\n",
    "sv.plot_image(annotated_frame)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "config = CourtConfiguration(league=League.NBA, measurement_unit=MeasurementUnit.FEET)\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# define team annotators\n",
    "\n",
    "team_colors = sv.ColorPalette.from_hex([\n",
    "    TEAM_COLORS[TEAM_NAMES[0]],\n",
    "    TEAM_COLORS[TEAM_NAMES[1]]\n",
    "])\n",
    "\n",
    "team_box_annotator = sv.BoxAnnotator(\n",
    "    color=team_colors,\n",
    "    thickness=2,\n",
    "    color_lookup=sv.ColorLookup.INDEX\n",
    ")\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we determine the team for each player and assign a team ID to every detection\n",
    "\n",
    "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "crops = [sv.crop_image(frame, box) for box in boxes]\n",
    "TEAMS = np.array(team_classifier.predict(crops))\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = team_box_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=detections,\n",
    "    custom_color_lookup=TEAMS[detections.tracker_id - 1]\n",
    ")\n",
    "sv.plot_image(annotated_frame)\n",
    "\n",
    "# we use a keypoint model to detect court landmarks\n",
    "\n",
    "result = KEYPOINT_DETECTION_MODEL.infer(frame, confidence=KEYPOINT_DETECTION_MODEL_CONFIDENCE)[0]\n",
    "key_points = sv.KeyPoints.from_inference(result)\n",
    "landmarks_mask = key_points.confidence[0] > KEYPOINT_DETECTION_MODEL_ANCHOR_CONFIDENCE\n",
    "\n",
    "if np.count_nonzero(landmarks_mask) >= 4:\n",
    "\n",
    "    # we calculate homography matrix\n",
    "\n",
    "    court_landmarks = np.array(config.vertices)[landmarks_mask]\n",
    "    frame_landmarks = key_points[:, landmarks_mask].xy[0]\n",
    "\n",
    "    frame_to_court_transformer = ViewTransformer(\n",
    "        source=frame_landmarks,\n",
    "        target=court_landmarks,\n",
    "    )\n",
    "\n",
    "    frame_xy = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n",
    "\n",
    "    if len(frame_xy) > 0:\n",
    "\n",
    "        # we transform points\n",
    "\n",
    "        court_xy = frame_to_court_transformer.transform_points(points=frame_xy)\n",
    "\n",
    "        # we visualize the results\n",
    "\n",
    "        court = draw_court(config=config)\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=court_xy[TEAMS == 0],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[0]]),\n",
    "            court=court\n",
    "        )\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=court_xy[TEAMS == 1],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[1]]),\n",
    "            court=court\n",
    "        )\n",
    "\n",
    "        sv.plot_image(court)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "video_xy = []\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "# we use RF-DETR model to aquire future SAM2 prompt\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=PLAYER_DETECTION_MODEL_CONFIDENCE, iou_threshold=PLAYER_DETECTION_MODEL_IOU_THRESHOLD)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "detections = detections[np.isin(detections.class_id, PLAYER_CLASS_IDS)]\n",
    "detections.tracker_id = np.arange(1, len(detections.class_id) + 1)\n",
    "\n",
    "# we determine the team for each player and assign a team ID to every detection\n",
    "\n",
    "boxes = sv.scale_boxes(xyxy=detections.xyxy, factor=0.4)\n",
    "crops = [sv.crop_image(frame, box) for box in boxes]\n",
    "TEAMS = np.array(team_classifier.predict(crops))\n",
    "\n",
    "# we prompt SAM2 using RF-DETR model detections\n",
    "\n",
    "tracker = SAM2Tracker(predictor)\n",
    "tracker.prompt_first_frame(frame, detections)\n",
    "\n",
    "# we propagate tacks across all video frames\n",
    "\n",
    "for frame_idx, frame in tqdm(enumerate(frame_generator)):\n",
    "    detections = tracker.propagate(frame)\n",
    "\n",
    "    # we use a keypoint model to detect court landmarks\n",
    "\n",
    "    result = KEYPOINT_DETECTION_MODEL.infer(frame, confidence=KEYPOINT_DETECTION_MODEL_CONFIDENCE)[0]\n",
    "    key_points = sv.KeyPoints.from_inference(result)\n",
    "    landmarks_mask = key_points.confidence[0] > KEYPOINT_DETECTION_MODEL_ANCHOR_CONFIDENCE\n",
    "\n",
    "    if np.count_nonzero(landmarks_mask) >= 4:\n",
    "\n",
    "        # we calculate homography matrix\n",
    "\n",
    "        court_landmarks = np.array(config.vertices)[landmarks_mask]\n",
    "        frame_landmarks = key_points[:, landmarks_mask].xy[0]\n",
    "\n",
    "        frame_to_court_transformer = ViewTransformer(\n",
    "            source=frame_landmarks,\n",
    "            target=court_landmarks,\n",
    "        )\n",
    "\n",
    "        frame_xy = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n",
    "        court_xy = frame_to_court_transformer.transform_points(points=frame_xy)\n",
    "        video_xy.append(court_xy)\n",
    "\n",
    "video_xy = np.array(video_xy)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-map{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "config = CourtConfiguration(league=League.NBA, measurement_unit=MeasurementUnit.FEET)\n",
    "court = draw_court(config=config)\n",
    "court_h, court_w, _ = court.shape\n",
    "\n",
    "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
    "video_info.width = court_w\n",
    "video_info.height = court_h\n",
    "\n",
    "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
    "    for frame_xy in tqdm(video_xy):\n",
    "        court = draw_court(config=config)\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=frame_xy[TEAMS == 0],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[0]]),\n",
    "            court=court\n",
    "        )\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=frame_xy[TEAMS == 1],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[1]]),\n",
    "            court=court\n",
    "        )\n",
    "\n",
    "        sink.write_frame(court)\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "court = draw_paths_on_court(\n",
    "    config=config,\n",
    "    paths=[video_xy[:, 0, :]],\n",
    ")\n",
    "\n",
    "sv.plot_image(court)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "cleaned_xy, edited_mask = clean_paths(\n",
    "    video_xy,\n",
    "    jump_sigma=3.5,\n",
    "    min_jump_dist=0.6,\n",
    "    max_jump_run=18,\n",
    "    pad_around_runs=2,\n",
    "    smooth_window=9,\n",
    "    smooth_poly=2,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def split_true_runs(mask: np.ndarray, coords: np.ndarray) -> list[np.ndarray]:\n",
    "    mask = mask.squeeze()\n",
    "    idx = np.flatnonzero(mask)\n",
    "    if idx.size == 0:\n",
    "        return []\n",
    "    splits = np.where(np.diff(idx) > 1)[0] + 1\n",
    "    groups = np.split(idx, splits)\n",
    "    return [coords[g, 0, :] for g in groups]\n",
    "\n",
    "\n",
    "court = draw_paths_on_court(\n",
    "    config=config,\n",
    "    paths=[video_xy[:, 0, :]],\n",
    "    color=sv.Color.GREEN,\n",
    ")\n",
    "\n",
    "court = draw_paths_on_court(\n",
    "    config=config,\n",
    "    paths=split_true_runs(edited_mask[:, 0], video_xy),\n",
    "    color=sv.Color.RED,\n",
    "    court=court\n",
    ")\n",
    "\n",
    "sv.plot_image(court)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "test = draw_paths_on_court(\n",
    "    config=config,\n",
    "    paths=[cleaned_xy[:, 0, :]],\n",
    ")\n",
    "\n",
    "sv.plot_image(test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "TARGET_VIDEO_PATH = HOME / f\"{SOURCE_VIDEO_PATH.stem}-map{SOURCE_VIDEO_PATH.suffix}\"\n",
    "TARGET_VIDEO_COMPRESSED_PATH = HOME / f\"{TARGET_VIDEO_PATH.stem}-compressed{TARGET_VIDEO_PATH.suffix}\"\n",
    "\n",
    "config = CourtConfiguration(league=League.NBA, measurement_unit=MeasurementUnit.FEET)\n",
    "court = draw_court(config=config)\n",
    "court_h, court_w, _ = court.shape\n",
    "\n",
    "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
    "video_info.width = court_w\n",
    "video_info.height = court_h\n",
    "\n",
    "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
    "    for frame_xy in tqdm(cleaned_xy):\n",
    "        court = draw_court(config=config)\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=frame_xy[TEAMS == 0],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[0]]),\n",
    "            court=court\n",
    "        )\n",
    "        court = draw_points_on_court(\n",
    "            config=config,\n",
    "            xy=frame_xy[TEAMS == 1],\n",
    "            fill_color=sv.Color.from_hex(TEAM_COLORS[TEAM_NAMES[1]]),\n",
    "            court=court\n",
    "        )\n",
    "\n",
    "        sink.write_frame(court)\n",
    "\n",
    "!ffmpeg -y -loglevel error -i {TARGET_VIDEO_PATH} -vcodec libx264 -crf 28 {TARGET_VIDEO_COMPRESSED_PATH}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Video(TARGET_VIDEO_COMPRESSED_PATH, embed=True, width=720)"
   ],
   "outputs": []
  }
 ]
}