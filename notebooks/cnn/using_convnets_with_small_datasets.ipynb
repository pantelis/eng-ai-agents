{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\ntitle: Using ConvNets with Small Datasets\n---\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pantelis/aiml-common/blob/master/lectures/cnn/cnn-example-architectures/using_convnets_with_small_datasets.ipynb)\n\nThis notebook is a PyTorch adaptation of the canonical small-dataset convnet example from\n[Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python) (F. Chollet, Chapter 5).\n\nWe use the `pantelism/cats-vs-dogs` dataset hosted on Hugging Face (the same 4,000-image Kaggle\nsubset used in the original) and demonstrate:\n\n1. **Baseline**: training a small convnet from scratch → clear overfitting with only 2,000 training samples\n2. **Regularisation**: data augmentation + dropout → substantially lower validation loss and higher accuracy\n\nThe trained model is saved as `cats_and_dogs_small.pth` for use by the companion\nvisualisation notebook.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install huggingface_hub scikit-learn seaborn --quiet\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom sklearn.metrics import confusion_matrix, roc_curve\nimport seaborn as sns\n\n# ── Config ──────────────────────────────────────────────────────────────────\nIMG_SIZE        = 150\nBATCH_SIZE      = 32\nEPOCHS_BASELINE = 20   # enough to show overfitting clearly\nEPOCHS_AUG      = 30   # enough to show regularisation benefit\nLR              = 1e-4\nSEED            = 42\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {DEVICE}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Dataset\n\n`pantelism/cats-vs-dogs` hosts the exact 4,000-image Kaggle subset used in the original\nChollet notebook as a single ZIP file (`dogs-vs-cats-subset.zip`).\nThe archive already contains pre-split `train/` (2,000), `validation/` (1,000), and\n`test/` (1,000) folders, each with `cats/` and `dogs/` sub-directories.\n\nWe download it via `hf_hub_download` and load directly with\n`torchvision.datasets.ImageFolder` — no manual splitting required.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, zipfile\nfrom huggingface_hub import hf_hub_download\nfrom torchvision.datasets import ImageFolder\n\n# ── Download ZIP from Hugging Face Hub ───────────────────────────────────────\nzip_path = hf_hub_download(\n    repo_id=\"pantelism/cats-vs-dogs\",\n    filename=\"dogs-vs-cats-subset.zip\",\n    repo_type=\"dataset\",\n)\nprint(f\"ZIP path: {zip_path}\")\n\n# ── Extract once (ZIP root is subset/) ───────────────────────────────────────\nextract_dir = \"/tmp/cats-vs-dogs\"\nif not os.path.exists(extract_dir):\n    with zipfile.ZipFile(zip_path, \"r\") as zf:\n        zf.extractall(extract_dir)\n    print(\"Extracted dataset\")\nelse:\n    print(\"Dataset already extracted\")\n\n# ZIP extracts to: extract_dir/subset/train|validation|test/cats|dogs/\nbase_dir  = os.path.join(extract_dir, \"subset\")\ntrain_dir = os.path.join(base_dir, \"train\")\nval_dir   = os.path.join(base_dir, \"validation\")\ntest_dir  = os.path.join(base_dir, \"test\")\nprint(f\"Train cats: {len(os.listdir(os.path.join(train_dir,'cats')))}, \"\n      f\"dogs: {len(os.listdir(os.path.join(train_dir,'dogs')))}\")\n\n# ── Transforms ───────────────────────────────────────────────────────────────\nbasic_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n])\n\naug_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(40),\n    transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), shear=20),\n    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n])\n\n# ── DataLoaders ──────────────────────────────────────────────────────────────\ntrain_ds_basic = ImageFolder(train_dir, transform=basic_tf)\ntrain_ds_aug   = ImageFolder(train_dir, transform=aug_tf)\nval_ds         = ImageFolder(val_dir,   transform=basic_tf)\ntest_ds        = ImageFolder(test_dir,  transform=basic_tf)\n\nlabel_names = train_ds_basic.classes   # ['cats', 'dogs']\n\ndef make_loader(ds, shuffle=False):\n    return DataLoader(ds, batch_size=BATCH_SIZE, shuffle=shuffle,\n                      num_workers=2, pin_memory=True)\n\ntrain_loader_basic = make_loader(train_ds_basic, shuffle=True)\ntrain_loader_aug   = make_loader(train_ds_aug,   shuffle=True)\nval_loader         = make_loader(val_ds)\ntest_loader        = make_loader(test_ds)\n\nprint(f\"Train {len(train_ds_basic)} | Val {len(val_ds)} | Test {len(test_ds)}\")\nprint(\"Classes:\", label_names)\n\nimgs, labels = next(iter(train_loader_basic))\nprint(f\"Batch shape: {imgs.shape}, Labels: {labels[:8].tolist()}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Model architecture\n\nWe replicate the Chollet convnet — four `Conv2d → ReLU → MaxPool2d` blocks that\nprogressively increase depth (32 → 64 → 128 → 128) while halving spatial dimensions\n(150 → 74 → 36 → 17 → 7), followed by a fully-connected head.\n\nAn optional `Dropout(0.5)` layer is inserted before the first dense layer for the\nregularised variant.\n\n```\nInput 3×150×150\n  Conv2d(3→32, k=3)  → ReLU → MaxPool2d(2)   →  32×74×74\n  Conv2d(32→64, k=3) → ReLU → MaxPool2d(2)   →  64×36×36\n  Conv2d(64→128,k=3) → ReLU → MaxPool2d(2)   → 128×17×17\n  Conv2d(128→128,k=3)→ ReLU → MaxPool2d(2)   → 128×7×7\n  Flatten → [Dropout(0.5)] → Linear(6272→512) → ReLU → Linear(512→1)\n```\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SmallConvNet(nn.Module):\n    def __init__(self, dropout: bool = False):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3,   32,  3), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(32,  64,  3), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(64,  128, 3), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(128, 128, 3), nn.ReLU(), nn.MaxPool2d(2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.5) if dropout else nn.Identity(),\n            nn.Linear(128 * 7 * 7, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        return self.classifier(x).squeeze(1)   # shape (B,)\n\n# Verify output shape\n_dummy = torch.zeros(2, 3, IMG_SIZE, IMG_SIZE)\nassert SmallConvNet()(_dummy).shape == (2,), \"unexpected output shape\"\nprint(\"Architecture verified — output shape (B,) ✓\")\nprint(SmallConvNet())\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Baseline: training from scratch with no regularisation\n\nWe train for 20 epochs with RMSprop and binary cross-entropy loss.\nWith only 2,000 training samples the network overfits quickly:\ntraining accuracy climbs to ~95% while validation accuracy plateaus\naround 70–72%, a textbook overfitting signature.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_model(model, train_loader, val_loader, epochs, lr=LR):\n    model = model.to(DEVICE)\n    criterion = nn.BCEWithLogitsLoss()\n    optimiser = torch.optim.RMSprop(model.parameters(), lr=lr)\n\n    history = dict(train_loss=[], val_loss=[], train_acc=[], val_acc=[])\n\n    for epoch in range(epochs):\n        # ── Training pass ────────────────────────────────────────────────────\n        model.train()\n        t_loss = t_correct = t_n = 0\n        for imgs, labels in train_loader:\n            imgs   = imgs.to(DEVICE)\n            labels = labels.float().to(DEVICE)   # ImageFolder returns Long; BCE needs Float\n            optimiser.zero_grad()\n            logits = model(imgs)\n            loss   = criterion(logits, labels)\n            loss.backward()\n            optimiser.step()\n            t_loss    += loss.item() * len(imgs)\n            t_correct += ((logits > 0) == labels.bool()).sum().item()\n            t_n       += len(imgs)\n\n        # ── Validation pass ──────────────────────────────────────────────────\n        model.eval()\n        v_loss = v_correct = v_n = 0\n        with torch.no_grad():\n            for imgs, labels in val_loader:\n                imgs   = imgs.to(DEVICE)\n                labels = labels.float().to(DEVICE)\n                logits = model(imgs)\n                v_loss    += criterion(logits, labels).item() * len(imgs)\n                v_correct += ((logits > 0) == labels.bool()).sum().item()\n                v_n       += len(imgs)\n\n        history[\"train_loss\"].append(t_loss / t_n)\n        history[\"train_acc\"].append(t_correct / t_n)\n        history[\"val_loss\"].append(v_loss / v_n)\n        history[\"val_acc\"].append(v_correct / v_n)\n\n        if (epoch + 1) % 5 == 0 or epoch == 0:\n            print(\n                f\"Epoch {epoch+1:3d}/{epochs}  \"\n                f\"loss {history['train_loss'][-1]:.4f}  acc {history['train_acc'][-1]:.3f}  |  \"\n                f\"val_loss {history['val_loss'][-1]:.4f}  val_acc {history['val_acc'][-1]:.3f}\"\n            )\n    return history\n\n\ndef plot_history(history, title, save_path=None):\n    epochs = range(1, len(history[\"train_acc\"]) + 1)\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n    ax1.plot(epochs, history[\"train_acc\"], \"bo-\", label=\"Training\")\n    ax1.plot(epochs, history[\"val_acc\"],   \"b-\",  label=\"Validation\")\n    ax1.set_title(f\"{title} — Accuracy\"); ax1.set_xlabel(\"Epoch\"); ax1.legend()\n    ax2.plot(epochs, history[\"train_loss\"], \"ro-\", label=\"Training\")\n    ax2.plot(epochs, history[\"val_loss\"],   \"r-\",  label=\"Validation\")\n    ax2.set_title(f\"{title} — Loss\"); ax2.set_xlabel(\"Epoch\"); ax2.legend()\n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path, dpi=120, bbox_inches=\"tight\")\n    plt.show()\n\n\ntorch.manual_seed(SEED)\nmodel_baseline = SmallConvNet(dropout=False)\nprint(\"Training baseline …\")\nhist_baseline = train_model(model_baseline, train_loader_basic, val_loader, EPOCHS_BASELINE)\nplot_history(hist_baseline, \"Baseline (no augmentation)\", \"baseline_curves.png\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data augmentation + dropout\n\nData augmentation generates new views of each training image on-the-fly — random\nhorizontal flips, rotations, translations, shears, and crop-resizes — so the model\nnever sees the exact same pixel pattern twice.  Combined with `Dropout(0.5)`, this\nsubstantially reduces the train-validation gap characteristic of overfitting.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "torch.manual_seed(SEED)\nmodel_aug = SmallConvNet(dropout=True)\nprint(\"Training augmented model (data augmentation + dropout) …\")\nhist_aug = train_model(model_aug, train_loader_aug, val_loader, EPOCHS_AUG)\nplot_history(hist_aug, \"Augmentation + Dropout\", \"augmented_curves.png\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Evaluation on the held-out test set\n\nWe evaluate the regularised model on the 1,000-image test split and report:\n\n- **Confusion matrix** — to see which mistakes are made\n- **ROC curve** — to characterise the trade-off across thresholds\n- **Test accuracy** — headline metric\n\nThe model is saved as `cats_and_dogs_small.pth` for the companion\nvisualisation notebook.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Save model ───────────────────────────────────────────────────────────────\ntorch.save(model_aug.state_dict(), \"cats_and_dogs_small.pth\")\nprint(\"Saved cats_and_dogs_small.pth\")\n\n# ── Collect predictions ──────────────────────────────────────────────────────\nmodel_aug.eval()\nall_labels, all_probs = [], []\nwith torch.no_grad():\n    for imgs, labels in test_loader:\n        logits = model_aug(imgs.to(DEVICE))\n        probs  = torch.sigmoid(logits).cpu().numpy()\n        all_probs.extend(probs)\n        all_labels.extend(labels.numpy())\n\nall_labels = np.array(all_labels, dtype=int)\nall_probs  = np.array(all_probs)\npreds      = (all_probs > 0.5).astype(int)\n\n# ── Confusion matrix + ROC ───────────────────────────────────────────────────\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\ncm = confusion_matrix(all_labels, preds)\nsns.heatmap(cm, annot=True, fmt=\"d\", ax=axes[0], cmap=\"Blues\",\n            xticklabels=label_names, yticklabels=label_names)\naxes[0].set_title(\"Confusion matrix (test set)\")\naxes[0].set_ylabel(\"True label\"); axes[0].set_xlabel(\"Predicted label\")\n\nfp, tp, _ = roc_curve(all_labels, all_probs)\naxes[1].plot(100 * fp, 100 * tp, linewidth=2)\naxes[1].set_xlabel(\"False positive rate [%]\"); axes[1].set_ylabel(\"True positive rate [%]\")\naxes[1].set_title(\"ROC curve\"); axes[1].grid(True)\n\nplt.tight_layout()\nplt.savefig(\"evaluation.png\", dpi=120, bbox_inches=\"tight\")\nplt.show()\n\nacc = (preds == all_labels).mean()\nprint(f\"Test accuracy: {acc:.3f}\")\n"
  }
 ]
}
