{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04cb5986",
   "metadata": {
    "papermill": {
     "duration": 0.003022,
     "end_time": "2026-02-24T13:50:49.517489",
     "exception": false,
     "start_time": "2026-02-24T13:50:49.514467",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "title: Visualizing What ConvNets Learn\n",
    "description: Four techniques for understanding what convolutional neural networks learn — intermediate activations, filter visualization, Grad-CAM, and occlusion sensitivity using PyTorch and ResNet-50.\n",
    "---\n",
    "\n",
    "Convolutional neural networks are often called \"black boxes\", but there are principled techniques to inspect what they have learned. This notebook demonstrates four complementary interpretability methods using a pretrained ResNet-50 on ImageNet.\n",
    "\n",
    "| Technique | Question answered | Tool |\n",
    "|---|---|---|\n",
    "| Intermediate activations | What does each layer \"see\"? | Forward hooks |\n",
    "| Filter visualization | What pattern maximally excites each filter? | Gradient ascent |\n",
    "| Grad-CAM | Which image regions drive the prediction? | Gradient-weighted class activation map |\n",
    "| Occlusion sensitivity | Which pixels matter most? | Systematic patch occlusion |\n",
    "\n",
    "All four methods use **PyTorch hooks** — no model modification required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060060d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T13:50:49.524382Z",
     "iopub.status.busy": "2026-02-24T13:50:49.524061Z",
     "iopub.status.idle": "2026-02-24T13:51:00.359677Z",
     "shell.execute_reply": "2026-02-24T13:51:00.358683Z"
    },
    "papermill": {
     "duration": 10.840286,
     "end_time": "2026-02-24T13:51:00.360608",
     "exception": false,
     "start_time": "2026-02-24T13:50:49.520322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')\n",
    "\n",
    "# ImageNet normalisation constants\n",
    "MEAN = torch.tensor([0.485, 0.456, 0.406], device=DEVICE).view(1, 3, 1, 1)\n",
    "STD  = torch.tensor([0.229, 0.224, 0.225], device=DEVICE).view(1, 3, 1, 1)\n",
    "\n",
    "def preprocess(img: Image.Image, size=224) -> torch.Tensor:\n",
    "    \"\"\"PIL image -> normalised BCHW tensor on DEVICE.\"\"\"\n",
    "    tf = T.Compose([T.Resize((size, size)), T.ToTensor()])\n",
    "    return (tf(img).unsqueeze(0).to(DEVICE) - MEAN) / STD\n",
    "\n",
    "def tensor_to_img(t: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"BCHW normalised tensor -> HWC uint8 numpy array.\"\"\"\n",
    "    t = (t * STD + MEAN).clamp(0, 1)\n",
    "    return (t.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "\n",
    "# ResNet-50 pretrained on ImageNet\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2).to(DEVICE)\n",
    "model.eval()\n",
    "print('ResNet-50 loaded.')\n",
    "\n",
    "# ImageNet class labels\n",
    "labels_url = ('https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels'\n",
    "              '/master/imagenet-simple-labels.json')\n",
    "with urllib.request.urlopen(labels_url) as r:\n",
    "    LABELS = json.load(r)\n",
    "print(f'Loaded {len(LABELS)} ImageNet labels.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7f21bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T13:51:00.372680Z",
     "iopub.status.busy": "2026-02-24T13:51:00.372302Z",
     "iopub.status.idle": "2026-02-24T13:51:01.338995Z",
     "shell.execute_reply": "2026-02-24T13:51:01.338174Z"
    },
    "papermill": {
     "duration": 0.975994,
     "end_time": "2026-02-24T13:51:01.341696",
     "exception": false,
     "start_time": "2026-02-24T13:51:00.365702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download a CC-licensed elephant image from Wikimedia Commons\n",
    "IMG_URL = 'https://img-datasets.s3.amazonaws.com/elephant.jpg'\n",
    "IMG_PATH = Path('elephant.jpg')\n",
    "if not IMG_PATH.exists():\n",
    "    req = urllib.request.Request(\n",
    "        IMG_URL,\n",
    "        headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'}\n",
    "    )\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        IMG_PATH.write_bytes(response.read())\n",
    "\n",
    "pil_img = Image.open(IMG_PATH).convert('RGB')\n",
    "img_tensor = preprocess(pil_img)   # (1, 3, 224, 224)\n",
    "\n",
    "# Top-5 predictions\n",
    "with torch.no_grad():\n",
    "    logits = model(img_tensor)\n",
    "probs  = F.softmax(logits, dim=1)[0]\n",
    "top5   = probs.topk(5)\n",
    "\n",
    "print('Top-5 predictions:')\n",
    "for prob, idx in zip(top5.values, top5.indices):\n",
    "    print(f'  {LABELS[idx]:30s}  {prob.item()*100:.1f}%')\n",
    "\n",
    "TARGET_CLASS = top5.indices[0].item()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.imshow(pil_img)\n",
    "ax.set_title(f'Input: {LABELS[TARGET_CLASS]}')\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('input_image.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350a75b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T13:51:01.360819Z",
     "iopub.status.busy": "2026-02-24T13:51:01.360596Z",
     "iopub.status.idle": "2026-02-24T13:51:02.638575Z",
     "shell.execute_reply": "2026-02-24T13:51:02.637713Z"
    },
    "papermill": {
     "duration": 1.289521,
     "end_time": "2026-02-24T13:51:02.640866",
     "exception": false,
     "start_time": "2026-02-24T13:51:01.351345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Technique 1: Intermediate activations ---\n",
    "#\n",
    "# Register forward hooks on each residual stage of ResNet-50.\n",
    "# ResNet-50 structure: conv1 -> bn1 -> relu -> maxpool -> layer1 -> layer2 -> layer3 -> layer4\n",
    "\n",
    "HOOK_LAYERS = {\n",
    "    'conv1':  model.relu,      # after first conv + BN + ReLU  (64ch, 112x112)\n",
    "    'layer1': model.layer1,    # after residual stage 1        (256ch, 56x56)\n",
    "    'layer2': model.layer2,    # after residual stage 2        (512ch, 28x28)\n",
    "    'layer3': model.layer3,    # after residual stage 3        (1024ch, 14x14)\n",
    "    'layer4': model.layer4,    # after residual stage 4        (2048ch, 7x7)\n",
    "}\n",
    "\n",
    "activations: dict = {}\n",
    "hooks = []\n",
    "\n",
    "def make_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        activations[name] = output.detach().cpu()\n",
    "    return hook\n",
    "\n",
    "for name, layer in HOOK_LAYERS.items():\n",
    "    hooks.append(layer.register_forward_hook(make_hook(name)))\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(img_tensor)\n",
    "\n",
    "for h in hooks:\n",
    "    h.remove()\n",
    "\n",
    "# Plot 8 channels from each stage\n",
    "N_CHANNELS = 8\n",
    "fig, axes = plt.subplots(len(HOOK_LAYERS), N_CHANNELS,\n",
    "                         figsize=(N_CHANNELS * 1.5, len(HOOK_LAYERS) * 1.5))\n",
    "\n",
    "for row, (name, act) in enumerate(activations.items()):\n",
    "    for col in range(N_CHANNELS):\n",
    "        ch = act[0, col].numpy()\n",
    "        axes[row, col].imshow(ch, cmap='viridis')\n",
    "        axes[row, col].axis('off')\n",
    "        if col == 0:\n",
    "            axes[row, col].set_title(name, fontsize=7, loc='left')\n",
    "\n",
    "fig.suptitle('Intermediate activations — first 8 channels per residual stage', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('activations.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "for name, act in activations.items():\n",
    "    print(f'{name}: shape={tuple(act.shape)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc7c88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T13:51:02.669953Z",
     "iopub.status.busy": "2026-02-24T13:51:02.669699Z",
     "iopub.status.idle": "2026-02-24T13:51:05.698145Z",
     "shell.execute_reply": "2026-02-24T13:51:05.697453Z"
    },
    "papermill": {
     "duration": 3.051327,
     "end_time": "2026-02-24T13:51:05.705728",
     "exception": false,
     "start_time": "2026-02-24T13:51:02.654401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Technique 2: Filter visualization via gradient ascent ---\n",
    "#\n",
    "# Start from random noise and update the input so that one specific\n",
    "# convolutional filter's mean activation is maximised.\n",
    "#\n",
    "# ResNet-50 does not have a flat Sequential backbone like VGG.\n",
    "# We hook into specific layers and run a full forward pass.\n",
    "\n",
    "def visualize_filter(model, target_layer, filter_idx: int,\n",
    "                     n_steps=60, lr=0.05, size=128) -> np.ndarray:\n",
    "    \"\"\"Return a (size, size, 3) uint8 image that maximally excites filter_idx.\"\"\"\n",
    "    x = torch.randn(1, 3, size, size, device=DEVICE) * 0.1\n",
    "    x.requires_grad_(True)\n",
    "\n",
    "    captured = {}\n",
    "    def fwd_hook(module, inp, out):\n",
    "        captured['act'] = out\n",
    "\n",
    "    hook = target_layer.register_forward_hook(fwd_hook)\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        if x.grad is not None:\n",
    "            x.grad.zero_()\n",
    "        model(x)  # full forward pass\n",
    "        loss = -captured['act'][0, filter_idx].mean()\n",
    "        loss.backward()\n",
    "        x.data += lr * x.grad / (x.grad.std() + 1e-8)\n",
    "\n",
    "    hook.remove()\n",
    "\n",
    "    img = x.detach().squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    img -= img.min()\n",
    "    mx = img.max()\n",
    "    if mx > 0:\n",
    "        img /= mx\n",
    "    return (img * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "# 4 filters from conv1 (early edges) and 4 from layer3 (high-level textures)\n",
    "configs = [\n",
    "    (model.conv1,              0, 'conv1'),\n",
    "    (model.conv1,              8, 'conv1'),\n",
    "    (model.conv1,             16, 'conv1'),\n",
    "    (model.conv1,             32, 'conv1'),\n",
    "    (model.layer3[-1].conv3,   0, 'layer3'),\n",
    "    (model.layer3[-1].conv3,  64, 'layer3'),\n",
    "    (model.layer3[-1].conv3, 128, 'layer3'),\n",
    "    (model.layer3[-1].conv3, 256, 'layer3'),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(10, 5))\n",
    "for ax, (layer, filt_idx, label) in zip(axes.flat, configs):\n",
    "    vis = visualize_filter(model, layer, filt_idx)\n",
    "    ax.imshow(vis)\n",
    "    ax.set_title(f'{label} f{filt_idx}', fontsize=8)\n",
    "    ax.axis('off')\n",
    "\n",
    "fig.suptitle('Filter visualization — gradient ascent (top: conv1, bottom: layer3)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('filter_visualization.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9513fcce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T13:51:05.756041Z",
     "iopub.status.busy": "2026-02-24T13:51:05.755733Z",
     "iopub.status.idle": "2026-02-24T13:51:06.538056Z",
     "shell.execute_reply": "2026-02-24T13:51:06.537185Z"
    },
    "papermill": {
     "duration": 0.812472,
     "end_time": "2026-02-24T13:51:06.542270",
     "exception": false,
     "start_time": "2026-02-24T13:51:05.729798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Technique 3: Grad-CAM ---\n",
    "#\n",
    "# Gradient-weighted Class Activation Mapping (Selvaraju et al., 2017).\n",
    "# Weights each feature-map channel by the global average of its gradient\n",
    "# w.r.t. the target class score, then applies ReLU and upsamples.\n",
    "\n",
    "def grad_cam(model, img_tensor: torch.Tensor,\n",
    "             target_class: int, target_layer) -> np.ndarray:\n",
    "    \"\"\"Return a (224, 224) heat map in [0, 1].\"\"\"\n",
    "    fmaps, grads = {}, {}\n",
    "\n",
    "    def fwd_hook(m, inp, out):\n",
    "        fmaps['A'] = out\n",
    "\n",
    "    def bwd_hook(m, grad_in, grad_out):\n",
    "        grads['dA'] = grad_out[0]\n",
    "\n",
    "    h1 = target_layer.register_forward_hook(fwd_hook)\n",
    "    h2 = target_layer.register_full_backward_hook(bwd_hook)\n",
    "\n",
    "    out = model(img_tensor)\n",
    "    model.zero_grad()\n",
    "    out[0, target_class].backward()\n",
    "\n",
    "    h1.remove()\n",
    "    h2.remove()\n",
    "\n",
    "    # alpha_k = global-average gradient per channel\n",
    "    alpha = grads['dA'][0].mean(dim=(1, 2), keepdim=True)  # (C, 1, 1)\n",
    "    cam = torch.relu((alpha * fmaps['A'][0]).sum(0))        # (H, W)\n",
    "\n",
    "    cam = F.interpolate(\n",
    "        cam.unsqueeze(0).unsqueeze(0),\n",
    "        size=(224, 224), mode='bilinear', align_corners=False\n",
    "    ).squeeze().detach().cpu().numpy()\n",
    "\n",
    "    cam -= cam.min()\n",
    "    if cam.max() > 0:\n",
    "        cam /= cam.max()\n",
    "    return cam\n",
    "\n",
    "\n",
    "# Disable inplace ReLU — required for register_full_backward_hook to work\n",
    "for m in model.modules():\n",
    "    if isinstance(m, torch.nn.ReLU):\n",
    "        m.inplace = False\n",
    "\n",
    "# Last residual block of ResNet-50: layer4[-1]\n",
    "heatmap = grad_cam(model, img_tensor, TARGET_CLASS, model.layer4[-1])\n",
    "\n",
    "rgb = np.array(pil_img.resize((224, 224))).astype(np.float32) / 255.0\n",
    "colormap = plt.get_cmap('jet')(heatmap)[..., :3]\n",
    "overlay = (0.55 * rgb + 0.45 * colormap).clip(0, 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow(rgb);              axes[0].set_title('Input');    axes[0].axis('off')\n",
    "axes[1].imshow(heatmap, cmap='jet'); axes[1].set_title('Grad-CAM'); axes[1].axis('off')\n",
    "axes[2].imshow(overlay);          axes[2].set_title('Overlay');  axes[2].axis('off')\n",
    "\n",
    "fig.suptitle(f'Grad-CAM — target class: \"{LABELS[TARGET_CLASS]}\"')\n",
    "plt.tight_layout()\n",
    "plt.savefig('gradcam.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba0598f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T13:51:06.608734Z",
     "iopub.status.busy": "2026-02-24T13:51:06.608304Z",
     "iopub.status.idle": "2026-02-24T13:51:07.674085Z",
     "shell.execute_reply": "2026-02-24T13:51:07.673354Z"
    },
    "papermill": {
     "duration": 1.101808,
     "end_time": "2026-02-24T13:51:07.676681",
     "exception": false,
     "start_time": "2026-02-24T13:51:06.574873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Technique 4: Occlusion sensitivity ---\n",
    "#\n",
    "# Slide a grey patch across the image and record how much the\n",
    "# target-class confidence drops at each position.\n",
    "# Large drops indicate regions that were important to the prediction.\n",
    "\n",
    "def occlusion_sensitivity(model, img_tensor: torch.Tensor,\n",
    "                           target_class: int,\n",
    "                           patch: int = 40, stride: int = 20) -> np.ndarray:\n",
    "    \"\"\"Return a (H, W) map of confidence drop when each patch is occluded.\"\"\"\n",
    "    _, _, H, W = img_tensor.shape\n",
    "\n",
    "    with torch.no_grad():\n",
    "        base_prob = F.softmax(model(img_tensor), dim=1)[0, target_class].item()\n",
    "\n",
    "    sensitivity = np.zeros((H, W), dtype=np.float32)\n",
    "    counts      = np.zeros((H, W), dtype=np.float32)\n",
    "\n",
    "    for y in range(0, H - patch + 1, stride):\n",
    "        for x in range(0, W - patch + 1, stride):\n",
    "            occluded = img_tensor.clone()\n",
    "            occluded[:, :, y:y+patch, x:x+patch] = 0.0  # mid-grey in normalised space\n",
    "            with torch.no_grad():\n",
    "                prob = F.softmax(model(occluded), dim=1)[0, target_class].item()\n",
    "            drop = base_prob - prob\n",
    "            sensitivity[y:y+patch, x:x+patch] += drop\n",
    "            counts[y:y+patch, x:x+patch]      += 1.0\n",
    "\n",
    "    counts = np.where(counts == 0, 1, counts)\n",
    "    return sensitivity / counts\n",
    "\n",
    "\n",
    "print('Running occlusion sensitivity (patch=40, stride=20) ...')\n",
    "sens_map = occlusion_sensitivity(model, img_tensor, TARGET_CLASS, patch=40, stride=20)\n",
    "\n",
    "rgb = np.array(pil_img.resize((224, 224)))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9, 4))\n",
    "axes[0].imshow(rgb)\n",
    "axes[0].set_title('Input')\n",
    "axes[0].axis('off')\n",
    "\n",
    "vmax = np.abs(sens_map).max()\n",
    "im = axes[1].imshow(sens_map, cmap='RdBu_r', vmin=-vmax, vmax=vmax)\n",
    "axes[1].set_title('Occlusion sensitivity\\n(red = high confidence drop)')\n",
    "axes[1].axis('off')\n",
    "plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "fig.suptitle(f'Occlusion sensitivity — target: \"{LABELS[TARGET_CLASS]}\"')\n",
    "plt.tight_layout()\n",
    "plt.savefig('occlusion.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20.414429,
   "end_time": "2026-02-24T13:51:08.836312",
   "environment_variables": {},
   "exception": null,
   "input_path": "notebooks/cnn/visualizing-what-convnets-learn/visualizing-what-convnets-learn.ipynb",
   "output_path": "notebooks/cnn/visualizing-what-convnets-learn/visualizing-what-convnets-learn-executed.ipynb",
   "parameters": {},
   "start_time": "2026-02-24T13:50:48.421883",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
