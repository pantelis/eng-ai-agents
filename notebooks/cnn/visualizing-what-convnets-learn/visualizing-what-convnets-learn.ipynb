{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Visualizing What ConvNets Learn\n",
    "description: Four techniques for understanding what convolutional neural networks learn — intermediate activations, filter visualization, Grad-CAM, and occlusion sensitivity using PyTorch and VGG16.\n",
    "---\n",
    "\n",
    "# Visualizing What ConvNets Learn\n",
    "\n",
    "Convolutional neural networks are often called \"black boxes\", but there are principled techniques to inspect what they have learned. This notebook demonstrates four complementary interpretability methods using a pretrained VGG16 on ImageNet.\n",
    "\n",
    "| Technique | Question answered | Tool |\n",
    "|---|---|---|\n",
    "| Intermediate activations | What does each layer \"see\"? | Forward hooks |\n",
    "| Filter visualization | What pattern maximally excites each filter? | Gradient ascent |\n",
    "| Grad-CAM | Which image regions drive the prediction? | Gradient-weighted class activation map |\n",
    "| Occlusion sensitivity | Which pixels matter most? | Systematic patch occlusion |\n",
    "\n",
    "All four methods use **PyTorch hooks** — no model modification required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')\n",
    "\n",
    "# ImageNet normalisation constants\n",
    "MEAN = torch.tensor([0.485, 0.456, 0.406], device=DEVICE).view(1, 3, 1, 1)\n",
    "STD  = torch.tensor([0.229, 0.224, 0.225], device=DEVICE).view(1, 3, 1, 1)\n",
    "\n",
    "def preprocess(img: Image.Image, size=224) -> torch.Tensor:\n",
    "    \"\"\"PIL image -> normalised BCHW tensor on DEVICE.\"\"\"\n",
    "    tf = T.Compose([T.Resize((size, size)), T.ToTensor()])\n",
    "    return (tf(img).unsqueeze(0).to(DEVICE) - MEAN) / STD\n",
    "\n",
    "def tensor_to_img(t: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"BCHW normalised tensor -> HWC uint8 numpy array.\"\"\"\n",
    "    t = (t * STD + MEAN).clamp(0, 1)\n",
    "    return (t.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "\n",
    "# VGG16 pretrained on ImageNet\n",
    "model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).to(DEVICE)\n",
    "model.eval()\n",
    "print('VGG16 loaded.')\n",
    "\n",
    "# ImageNet class labels\n",
    "labels_url = ('https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels'\n",
    "              '/master/imagenet-simple-labels.json')\n",
    "with urllib.request.urlopen(labels_url) as r:\n",
    "    LABELS = json.load(r)\n",
    "print(f'Loaded {len(LABELS)} ImageNet labels.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a CC-licensed elephant image from Wikimedia Commons\nIMG_URL = 'https://img-datasets.s3.amazonaws.com/elephant.jpg'\nIMG_PATH = Path('elephant.jpg')\nif not IMG_PATH.exists():\n    req = urllib.request.Request(\n        IMG_URL,\n        headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'}\n    )\n    with urllib.request.urlopen(req) as response:\n        IMG_PATH.write_bytes(response.read())\n\npil_img = Image.open(IMG_PATH).convert('RGB')\nimg_tensor = preprocess(pil_img)   # (1, 3, 224, 224)\n\n# Top-5 predictions\nwith torch.no_grad():\n    logits = model(img_tensor)\nprobs  = F.softmax(logits, dim=1)[0]\ntop5   = probs.topk(5)\n\nprint('Top-5 predictions:')\nfor prob, idx in zip(top5.values, top5.indices):\n    print(f'  {LABELS[idx]:30s}  {prob.item()*100:.1f}%')\n\nTARGET_CLASS = top5.indices[0].item()\n\nfig, ax = plt.subplots(figsize=(4, 4))\nax.imshow(pil_img)\nax.set_title(f'Input: {LABELS[TARGET_CLASS]}')\nax.axis('off')\nplt.tight_layout()\nplt.savefig('input_image.png', dpi=120, bbox_inches='tight')\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Technique 1: Intermediate activations ---\n",
    "#\n",
    "# Register forward hooks on every max-pool layer in VGG16.\n",
    "# Pool indices: block1=4, block2=9, block3=18, block4=27, block5=30\n",
    "\n",
    "HOOK_LAYERS = {\n",
    "    'block1_pool': model.features[4],\n",
    "    'block2_pool': model.features[9],\n",
    "    'block3_pool': model.features[18],\n",
    "    'block4_pool': model.features[27],\n",
    "    'block5_pool': model.features[30],\n",
    "}\n",
    "\n",
    "activations: dict = {}\n",
    "hooks = []\n",
    "\n",
    "def make_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        activations[name] = output.detach().cpu()\n",
    "    return hook\n",
    "\n",
    "for name, layer in HOOK_LAYERS.items():\n",
    "    hooks.append(layer.register_forward_hook(make_hook(name)))\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(img_tensor)\n",
    "\n",
    "for h in hooks:\n",
    "    h.remove()\n",
    "\n",
    "# Plot 8 channels from each pooling block\n",
    "N_CHANNELS = 8\n",
    "fig, axes = plt.subplots(len(HOOK_LAYERS), N_CHANNELS,\n",
    "                         figsize=(N_CHANNELS * 1.5, len(HOOK_LAYERS) * 1.5))\n",
    "\n",
    "for row, (name, act) in enumerate(activations.items()):\n",
    "    for col in range(N_CHANNELS):\n",
    "        ch = act[0, col].numpy()\n",
    "        axes[row, col].imshow(ch, cmap='viridis')\n",
    "        axes[row, col].axis('off')\n",
    "        if col == 0:\n",
    "            axes[row, col].set_title(name, fontsize=7, loc='left')\n",
    "\n",
    "fig.suptitle('Intermediate activations — first 8 channels per pooling layer', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('activations.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "for name, act in activations.items():\n",
    "    print(f'{name}: shape={tuple(act.shape)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Technique 2: Filter visualization via gradient ascent ---\n",
    "#\n",
    "# Start from random noise and update the input so that one specific\n",
    "# convolutional filter's mean activation is maximised.\n",
    "\n",
    "def visualize_filter(model, layer_idx: int, filter_idx: int,\n",
    "                     n_steps=60, lr=0.05, size=128) -> np.ndarray:\n",
    "    \"\"\"Return a (size, size, 3) uint8 image that maximally excites filter_idx.\"\"\"\n",
    "    x = torch.randn(1, 3, size, size, device=DEVICE) * 0.1\n",
    "    x.requires_grad_(True)\n",
    "\n",
    "    captured = {}\n",
    "    def fwd_hook(module, inp, out):\n",
    "        captured['act'] = out\n",
    "\n",
    "    hook = model.features[layer_idx].register_forward_hook(fwd_hook)\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        if x.grad is not None:\n",
    "            x.grad.zero_()\n",
    "        model.features(x)\n",
    "        loss = -captured['act'][0, filter_idx].mean()\n",
    "        loss.backward()\n",
    "        x.data += lr * x.grad / (x.grad.std() + 1e-8)\n",
    "\n",
    "    hook.remove()\n",
    "\n",
    "    img = x.detach().squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    img -= img.min()\n",
    "    mx = img.max()\n",
    "    if mx > 0:\n",
    "        img /= mx\n",
    "    return (img * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "# 4 filters each from block1 conv1 (layer 0) and block3 conv1 (layer 14)\n",
    "configs = [\n",
    "    (0,  0), (0,  4), (0,  8), (0, 12),   # block1 — edge detectors\n",
    "    (14, 0), (14, 4), (14, 8), (14, 12),   # block3 — textures\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(10, 5))\n",
    "for ax, (layer_idx, filt_idx) in zip(axes.flat, configs):\n",
    "    vis = visualize_filter(model, layer_idx, filt_idx)\n",
    "    block = 'block1' if layer_idx == 0 else 'block3'\n",
    "    ax.imshow(vis)\n",
    "    ax.set_title(f'{block} f{filt_idx}', fontsize=8)\n",
    "    ax.axis('off')\n",
    "\n",
    "fig.suptitle('Filter visualization — gradient ascent (top: block1, bottom: block3)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('filter_visualization.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Technique 3: Grad-CAM ---\n#\n# Gradient-weighted Class Activation Mapping (Selvaraju et al., 2017).\n# Weights each feature-map channel by the global average of its gradient\n# w.r.t. the target class score, then applies ReLU and upsamples.\n\ndef grad_cam(model, img_tensor: torch.Tensor,\n             target_class: int, target_layer) -> np.ndarray:\n    \"\"\"Return a (224, 224) heat map in [0, 1].\"\"\"\n    fmaps, grads = {}, {}\n\n    def fwd_hook(m, inp, out):\n        fmaps['A'] = out\n\n    def bwd_hook(m, grad_in, grad_out):\n        grads['dA'] = grad_out[0]\n\n    h1 = target_layer.register_forward_hook(fwd_hook)\n    h2 = target_layer.register_full_backward_hook(bwd_hook)\n\n    out = model(img_tensor)\n    model.zero_grad()\n    out[0, target_class].backward()\n\n    h1.remove()\n    h2.remove()\n\n    # alpha_k = global-average gradient per channel\n    alpha = grads['dA'][0].mean(dim=(1, 2), keepdim=True)  # (C, 1, 1)\n    cam = torch.relu((alpha * fmaps['A'][0]).sum(0))        # (H, W)\n\n    cam = F.interpolate(\n        cam.unsqueeze(0).unsqueeze(0),\n        size=(224, 224), mode='bilinear', align_corners=False\n    ).squeeze().detach().cpu().numpy()\n\n    cam -= cam.min()\n    if cam.max() > 0:\n        cam /= cam.max()\n    return cam\n\n\n# Disable inplace ReLU — required for register_full_backward_hook to work\nfor m in model.modules():\n    if isinstance(m, torch.nn.ReLU):\n        m.inplace = False\n\n# Last conv layer of VGG16 features: features[28]\nheatmap = grad_cam(model, img_tensor, TARGET_CLASS, model.features[28])\n\nrgb = np.array(pil_img.resize((224, 224))).astype(np.float32) / 255.0\ncolormap = cm.get_cmap('jet')(heatmap)[..., :3]\noverlay = (0.55 * rgb + 0.45 * colormap).clip(0, 1)\n\nfig, axes = plt.subplots(1, 3, figsize=(12, 4))\naxes[0].imshow(rgb);              axes[0].set_title('Input');    axes[0].axis('off')\naxes[1].imshow(heatmap, cmap='jet'); axes[1].set_title('Grad-CAM'); axes[1].axis('off')\naxes[2].imshow(overlay);          axes[2].set_title('Overlay');  axes[2].axis('off')\n\nfig.suptitle(f'Grad-CAM — target class: \"{LABELS[TARGET_CLASS]}\"')\nplt.tight_layout()\nplt.savefig('gradcam.png', dpi=120, bbox_inches='tight')\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Technique 4: Occlusion sensitivity ---\n",
    "#\n",
    "# Slide a grey patch across the image and record how much the\n",
    "# target-class confidence drops at each position.\n",
    "# Large drops indicate regions that were important to the prediction.\n",
    "\n",
    "def occlusion_sensitivity(model, img_tensor: torch.Tensor,\n",
    "                           target_class: int,\n",
    "                           patch: int = 40, stride: int = 20) -> np.ndarray:\n",
    "    \"\"\"Return a (H, W) map of confidence drop when each patch is occluded.\"\"\"\n",
    "    _, _, H, W = img_tensor.shape\n",
    "\n",
    "    with torch.no_grad():\n",
    "        base_prob = F.softmax(model(img_tensor), dim=1)[0, target_class].item()\n",
    "\n",
    "    sensitivity = np.zeros((H, W), dtype=np.float32)\n",
    "    counts      = np.zeros((H, W), dtype=np.float32)\n",
    "\n",
    "    for y in range(0, H - patch + 1, stride):\n",
    "        for x in range(0, W - patch + 1, stride):\n",
    "            occluded = img_tensor.clone()\n",
    "            occluded[:, :, y:y+patch, x:x+patch] = 0.0  # mid-grey in normalised space\n",
    "            with torch.no_grad():\n",
    "                prob = F.softmax(model(occluded), dim=1)[0, target_class].item()\n",
    "            drop = base_prob - prob\n",
    "            sensitivity[y:y+patch, x:x+patch] += drop\n",
    "            counts[y:y+patch, x:x+patch]      += 1.0\n",
    "\n",
    "    counts = np.where(counts == 0, 1, counts)\n",
    "    return sensitivity / counts\n",
    "\n",
    "\n",
    "print('Running occlusion sensitivity (patch=40, stride=20) ...')\n",
    "sens_map = occlusion_sensitivity(model, img_tensor, TARGET_CLASS, patch=40, stride=20)\n",
    "\n",
    "rgb = np.array(pil_img.resize((224, 224)))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9, 4))\n",
    "axes[0].imshow(rgb)\n",
    "axes[0].set_title('Input')\n",
    "axes[0].axis('off')\n",
    "\n",
    "vmax = np.abs(sens_map).max()\n",
    "im = axes[1].imshow(sens_map, cmap='RdBu_r', vmin=-vmax, vmax=vmax)\n",
    "axes[1].set_title('Occlusion sensitivity\\n(red = high confidence drop)')\n",
    "axes[1].axis('off')\n",
    "plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "fig.suptitle(f'Occlusion sensitivity — target: \"{LABELS[TARGET_CLASS]}\"')\n",
    "plt.tight_layout()\n",
    "plt.savefig('occlusion.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
