{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: ResNet Skip-Connection Dimensioning and FPN\n",
    "description: Why addition forces strict shape equality in ResNets, how 1×1 projection shortcuts handle dimension mismatches, and the canonical FPN featurizer that unifies backbone channels to d=256.\n",
    "---\n",
    "\n",
    "This notebook does two things:\n",
    "\n",
    "1. Extracts dimensioning-relevant excerpts from the three foundational papers — ResNet (He et al., CVPR 2016), Identity Mappings (He et al., ECCV 2016), and FPN (Lin et al., CVPR 2017).\n",
    "\n",
    "2. Demonstrates the **preferred, modern featurizer** pattern: a ResNet-style backbone with identity shortcuts when shapes match, 1×1 projection shortcuts when they don't, feeding into an FPN neck that unifies all pyramid levels to $d=256$ channels.\n",
    "\n",
    "Papers (arXiv identifiers):\n",
    "- ResNet: arXiv:1512.03385\n",
    "- Identity mappings: arXiv:1603.05027\n",
    "- FPN: arXiv:1612.03144\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1) Dimensioning-relevant excerpts (text) and figure takeaways\n\n### A. ResNet (He et al., CVPR 2016)\n\nShort excerpt (dimension matching via projection; note the explicit stride-2 handling):\n\n> “The projection shortcut … is used to match dimensions (done by 1×1 convolutions).”  \n(He et al., 2016, Sec. 3.3)\n\n> “When the shortcuts go across feature maps of two sizes, they are performed with a stride of 2.”  \n(He et al., 2016, Sec. 3.3)\n\n(He et al., 2016, Sec. 3.3)\n\nShort excerpt (options for increased dimensions):\n\n> “(A) … identity mapping, with extra zero entries padded … (B) … projection shortcut … to match dimensions”  \n(He et al., 2016, Sec. 3.3)\n\nFigure takeaways (do not reproduce the copyrighted figures here; consult the paper figures directly):\n- ResNet Fig. 3 (residual block): illustrates the residual branch $F(x)$ and the shortcut branch being added; addition requires exact shape match.\n- ResNet Fig. 5 (bottleneck block): shows the 1×1–3×3–1×1 pattern that reduces then restores channels (e.g., 256→64→64→256); the shortcut is typically identity when input/output shapes match.\n\n### B. Identity Mappings (He et al., ECCV 2016)\n\nShort excerpt (why identity shortcuts are special in signal propagation):\n\n> “forward and backward signals can be directly propagated … when using identity mappings as the skip connections …”  \n(He et al., 2016, Abstract)\n(He et al., 2016, Abstract)\n\nFigure takeaway:\n- The paper analyzes variants of residual units and shows that moving toward “cleaner” identity shortcuts improves optimization/propagation (conceptual motivation for using projections only when necessary).\n\n### C. FPN (Lin et al., CVPR 2017)\n\nShort excerpt (lateral dimension reduction for addition):\n\n> “the corresponding bottom-up map (which undergoes a 1×1 convolutional layer to reduce channel dimensions) [is merged] by element-wise addition.”  \n(Lin et al., 2017, Sec. 3)\n\nShort excerpt (the canonical $d=256$ design):\n\n> “We set d = 256 … thus all extra convolutional layers have 256-channel outputs.”  \n(Lin et al., 2017, Sec. 3)\n\nFigure takeaways:\n- FPN Fig. 3 (building block): shows **upsample (×2)** + **1×1 lateral conv** then **addition**, followed by a **3×3 “smoothing” conv**. The addition imposes strict spatial and channel alignment.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2) The core dimensional constraint\n\nLet $x \\in \\mathbb{R}^{B \\times C_{in} \\times H \\times W}$. A residual unit computes\n\n\\[\ny = F(x) + \\mathcal{S}(x),\n\\]\n\nand **addition requires identical tensor shapes**:\n\n\\[\nF(x), \\mathcal{S}(x) \\in \\mathbb{R}^{B \\times C_{out} \\times H' \\times W'}.\n\\]\n\nHence the shortcut must handle two mismatches:\n- channel mismatch: $C_{in} \\neq C_{out}$\n- spatial mismatch: $(H,W) \\neq (H',W')$ (typically caused by stride-2 downsampling)\n"
  },
  {
   "cell_type": "markdown",
   "id": "6aea7c6f",
   "metadata": {},
   "source": [
    "### Residual block — shortcut options\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    x[\"x\\n[B, Cᵢₙ, H, W]\"]\n",
    "    x --> c1[\"Conv 3×3\\nstride s\"]\n",
    "    c1 --> b1[\"BN + ReLU\"]\n",
    "    b1 --> c2[\"Conv 3×3\\nstride 1\"]\n",
    "    c2 --> b2[\"BN\"]\n",
    "    b2 --> add[\"⊕ Add\"]\n",
    "    x -->|\"Cᵢₙ=Cₒᵤₜ, s=1\"| id[\"Identity\"]\n",
    "    x -->|\"else\"| proj[\"1×1 Conv, stride s\\nOption B\"]\n",
    "    id --> add\n",
    "    proj --> add\n",
    "    add --> relu[\"ReLU\"]\n",
    "    relu --> y[\"y\\n[B, Cₒᵤₜ, H/s, W/s]\"]\n",
    "```\n",
    "\n",
    "Addition **requires identical tensor shapes**: both the residual branch and the shortcut must produce $[B, C_{out}, H', W']$.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef shape(x):\n    return tuple(x.shape)\n\ndef report(name, x):\n    print(f\"{name}: {shape(x)}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3) ResNet-style block with correct shortcut dimensioning\n\nWe implement a standard BasicBlock with:\n- residual branch: 3×3 conv → BN → ReLU → 3×3 conv → BN\n- shortcut:\n  - identity if stride=1 and $C_{in}=C_{out}$\n  - otherwise a 1×1 conv (projection), with the same stride as the residual branch’s downsampling\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "class BasicBlock(nn.Module):\n    def __init__(self, cin: int, cout: int, stride: int = 1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(cin, cout, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1   = nn.BatchNorm2d(cout)\n        self.conv2 = nn.Conv2d(cout, cout, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2   = nn.BatchNorm2d(cout)\n        self.relu  = nn.ReLU(inplace=True)\n\n        if stride != 1 or cin != cout:\n            # Projection shortcut: matches channels and spatial size.\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(cin, cout, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(cout),\n            )\n        else:\n            self.shortcut = nn.Identity()\n\n    def forward(self, x):\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = out + self.shortcut(x)\n        out = self.relu(out)\n        return out\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.1) What goes wrong if you try to add mismatched tensors?\n\nBelow we show:\n- a block that downsamples and increases channels (stride=2, 64→128)\n- naive identity shortcut fails (shape mismatch)\n- projection shortcut works\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Dummy input\nx = torch.randn(2, 64, 56, 56)\n\n# Residual branch that downsamples and changes channels\nresidual = nn.Sequential(\n    nn.Conv2d(64, 128, 3, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.ReLU(inplace=True),\n    nn.Conv2d(128, 128, 3, stride=1, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n)\n\nFx = residual(x)\nreport(\"x\", x)\nreport(\"F(x)\", Fx)\n\nprint(\"\\nAttempting F(x) + x (naive identity shortcut):\")\ntry:\n    _ = Fx + x\nexcept RuntimeError as e:\n    print(\"RuntimeError:\", str(e).split(\"\\n\")[0])\n\nprint(\"\\nUsing a projection shortcut (1×1 conv, stride=2):\")\nproj = nn.Conv2d(64, 128, 1, stride=2, bias=False)\nSx = proj(x)\nreport(\"S(x)\", Sx)\ny = Fx + Sx\nreport(\"F(x)+S(x)\", y)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.2) Option A vs. Option B (ResNet paper terminology)\n\nIn the ResNet paper’s discussion:\n- Option A: downsample the shortcut (stride 2) and **zero-pad channels** to match $C_{out}$.\n- Option B: downsample and **project with 1×1 conv** to match dimensions.\n\nFor FPN-style backbones, **Option B is the preferred practical choice** because:\n- the feature hierarchy is consumed downstream (e.g., lateral merges), so having a learned projection at stage transitions is robust,\n- and it matches the canonical ResNet-{50,101,152} “option B” design in the CVPR paper.\n\nBelow is a small functional illustration of “Option A-like” padding for the channel mismatch (spatial downsample uses strided slicing for simplicity).\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def option_a_shortcut(x, cout: int, stride: int):\n    # Spatial downsample: emulate stride-2 shortcut by subsampling.\n    if stride == 2:\n        x_ds = x[:, :, ::2, ::2]\n    elif stride == 1:\n        x_ds = x\n    else:\n        raise ValueError(\"This demo only supports stride 1 or 2.\")\n    cin = x_ds.shape[1]\n    if cin > cout:\n        raise ValueError(\"Option A padding demo expects cin <= cout.\")\n    if cin == cout:\n        return x_ds\n    pad_c = cout - cin\n    # Pad channels: (N,C,H,W). We pad on the channel dimension by concatenating zeros.\n    zeros = torch.zeros(x_ds.shape[0], pad_c, x_ds.shape[2], x_ds.shape[3], device=x_ds.device, dtype=x_ds.dtype)\n    return torch.cat([x_ds, zeros], dim=1)\n\n# Demonstrate option A-like shortcut shape matching\nx = torch.randn(2, 64, 56, 56)\nSx_a = option_a_shortcut(x, cout=128, stride=2)\nreport(\"Option-A-like S(x)\", Sx_a)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4) A minimal ResNet-like backbone that exposes {C2, C3, C4, C5}\n\nFPN (Lin et al.) uses the outputs of each ResNet stage’s last block:\n\\{C2, C3, C4, C5\\} with strides \\{4, 8, 16, 32\\} relative to the input.\n\nWe build a small backbone that mirrors this structure (conceptually like a tiny ResNet-18).\n"
  },
  {
   "cell_type": "markdown",
   "id": "d894e061",
   "metadata": {},
   "source": [
    "### Backbone stage layout — strides and channel widths\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    img[\"Image\\n3×224×224\"]\n",
    "    stem[\"Stem\\nConv7 s2 + MaxPool s2\\n64×56×56\"]\n",
    "    c2[\"Stage 1\\n64×56×56\\n→ C2 stride 4\"]\n",
    "    c3[\"Stage 2\\n128×28×28\\n→ C3 stride 8\"]\n",
    "    c4[\"Stage 3\\n256×14×14\\n→ C4 stride 16\"]\n",
    "    c5[\"Stage 4\\n512×7×7\\n→ C5 stride 32\"]\n",
    "    img --> stem --> c2 -->|\"stride 2\"| c3 -->|\"stride 2\"| c4 -->|\"stride 2\"| c5\n",
    "```\n",
    "\n",
    "Each stage transition uses a stride-2 first block with a **1×1 projection shortcut** (Option B) to match dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "class TinyResNetBackbone(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Stem (like ResNet): stride-2 conv + stride-2 maxpool => output stride 4\n        self.stem = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n        )\n        # Stages: produce C2..C5\n        self.layer1 = nn.Sequential(BasicBlock(64,  64, stride=1), BasicBlock(64,  64, stride=1))  # C2, stride 4\n        self.layer2 = nn.Sequential(BasicBlock(64, 128, stride=2), BasicBlock(128, 128, stride=1)) # C3, stride 8\n        self.layer3 = nn.Sequential(BasicBlock(128,256, stride=2), BasicBlock(256, 256, stride=1)) # C4, stride 16\n        self.layer4 = nn.Sequential(BasicBlock(256,512, stride=2), BasicBlock(512, 512, stride=1)) # C5, stride 32\n\n    def forward(self, x):\n        x = self.stem(x)\n        c2 = self.layer1(x)\n        c3 = self.layer2(c2)\n        c4 = self.layer3(c3)\n        c5 = self.layer4(c4)\n        return {\"C2\": c2, \"C3\": c3, \"C4\": c4, \"C5\": c5}\n\nbackbone = TinyResNetBackbone()\nx = torch.randn(1, 3, 224, 224)\nC = backbone(x)\nfor k in [\"C2\",\"C3\",\"C4\",\"C5\"]:\n    report(k, C[k])\n"
  },
  {
   "cell_type": "code",
   "id": "69904b6b",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "stages = ['C2\\n(stride 4)', 'C3\\n(stride 8)', 'C4\\n(stride 16)', 'C5\\n(stride 32)']\n",
    "channels_bb = [64, 128, 256, 512]\n",
    "spatial_bb  = [56, 28, 14, 7]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 4))\n",
    "colors = ['#4e79a7', '#f28e2b', '#e15759', '#76b7b2']\n",
    "\n",
    "bars1 = ax1.bar(stages, channels_bb, color=colors)\n",
    "ax1.set_ylabel('Channels')\n",
    "ax1.set_title('Channel width per backbone stage')\n",
    "for b, v in zip(bars1, channels_bb):\n",
    "    ax1.text(b.get_x() + b.get_width()/2, b.get_height() + 4, str(v),\n",
    "             ha='center', fontweight='bold')\n",
    "\n",
    "bars2 = ax2.bar(stages, spatial_bb, color=colors)\n",
    "ax2.set_ylabel('Spatial size (H = W, pixels)')\n",
    "ax2.set_title('Feature map spatial size per backbone stage\\n(input 224×224)')\n",
    "for b, v in zip(bars2, spatial_bb):\n",
    "    ax2.text(b.get_x() + b.get_width()/2, b.get_height() + 0.4, f'{v}×{v}',\n",
    "             ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('backbone_dimensions.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5) Preferred FPN module (top-down + lateral, with $d=256$)\n\nCanonical FPN design choices (as in Lin et al.):\n- 1×1 lateral conv to unify channels to $d=256$\n- top-down upsample by factor 2 (nearest neighbor is typical)\n- element-wise addition (requires same $H \\times W$ and same $d$)\n- 3×3 conv “smoothing” on each merged map\n- optional $P6$ via stride-2 3×3 conv on $P5$ (common in detection systems)\n"
  },
  {
   "cell_type": "markdown",
   "id": "9b43c2e2",
   "metadata": {},
   "source": [
    "### FPN top-down pathway — lateral merges and channel unification\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    C5[\"C5: 512×7×7\"] -->|\"Lat 1×1\"| M5[\"M5: 256×7×7\"]\n",
    "    C4[\"C4: 256×14×14\"] -->|\"Lat 1×1\"| lat4[\"256×14×14\"]\n",
    "    C3[\"C3: 128×28×28\"] -->|\"Lat 1×1\"| lat3[\"256×28×28\"]\n",
    "    C2[\"C2: 64×56×56\"] -->|\"Lat 1×1\"| lat2[\"256×56×56\"]\n",
    "    M5 -->|\"Up ×2\"| up5[\"256×14×14\"]\n",
    "    up5 -->|\"⊕\"| M4[\"M4: 256×14×14\"]\n",
    "    lat4 --> M4\n",
    "    M4 -->|\"Up ×2\"| up4[\"256×28×28\"]\n",
    "    up4 -->|\"⊕\"| M3[\"M3: 256×28×28\"]\n",
    "    lat3 --> M3\n",
    "    M3 -->|\"Up ×2\"| up3[\"256×56×56\"]\n",
    "    up3 -->|\"⊕\"| M2[\"M2: 256×56×56\"]\n",
    "    lat2 --> M2\n",
    "    M5 -->|\"3×3 Conv\"| P5[\"P5\"]\n",
    "    M4 -->|\"3×3 Conv\"| P4[\"P4\"]\n",
    "    M3 -->|\"3×3 Conv\"| P3[\"P3\"]\n",
    "    M2 -->|\"3×3 Conv\"| P2[\"P2\"]\n",
    "    P5 -->|\"3×3 s2\"| P6[\"P6\"]\n",
    "```\n",
    "\n",
    "The 1×1 lateral convolutions unify **heterogeneous backbone channels** (64/128/256/512) to a **uniform $d=256$** before the element-wise additions. The additions require strict spatial and channel alignment — which the lateral convolutions and upsample guarantee.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "class FPN(nn.Module):\n    def __init__(self, c2: int, c3: int, c4: int, c5: int, d: int = 256, make_p6: bool = True):\n        super().__init__()\n        # Lateral 1×1 convs: Ck -> d\n        self.lat2 = nn.Conv2d(c2, d, kernel_size=1)\n        self.lat3 = nn.Conv2d(c3, d, kernel_size=1)\n        self.lat4 = nn.Conv2d(c4, d, kernel_size=1)\n        self.lat5 = nn.Conv2d(c5, d, kernel_size=1)\n\n        # Smoothing 3×3 convs on each pyramid level\n        self.smooth2 = nn.Conv2d(d, d, kernel_size=3, padding=1)\n        self.smooth3 = nn.Conv2d(d, d, kernel_size=3, padding=1)\n        self.smooth4 = nn.Conv2d(d, d, kernel_size=3, padding=1)\n        self.smooth5 = nn.Conv2d(d, d, kernel_size=3, padding=1)\n\n        self.make_p6 = make_p6\n        self.p6 = nn.Conv2d(d, d, kernel_size=3, stride=2, padding=1) if make_p6 else None\n\n    def forward(self, C):\n        c2, c3, c4, c5 = C[\"C2\"], C[\"C3\"], C[\"C4\"], C[\"C5\"]\n\n        m5 = self.lat5(c5)\n        m4 = self.lat4(c4) + F.interpolate(m5, scale_factor=2.0, mode=\"nearest\")\n        m3 = self.lat3(c3) + F.interpolate(m4, scale_factor=2.0, mode=\"nearest\")\n        m2 = self.lat2(c2) + F.interpolate(m3, scale_factor=2.0, mode=\"nearest\")\n\n        p5 = self.smooth5(m5)\n        p4 = self.smooth4(m4)\n        p3 = self.smooth3(m3)\n        p2 = self.smooth2(m2)\n\n        out = {\"P2\": p2, \"P3\": p3, \"P4\": p4, \"P5\": p5}\n        if self.make_p6:\n            out[\"P6\"] = self.p6(p5)\n        return out\n\nfpn = FPN(c2=64, c3=128, c4=256, c5=512, d=256, make_p6=True)\n\nP = fpn(C)\nfor k in [\"P2\",\"P3\",\"P4\",\"P5\",\"P6\"]:\n    report(k, P[k])\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.1) Sanity checks: the additions are well-defined\n\nEach merge is of the form:\n\\[\nM_\\ell = \\text{Lat}(C_\\ell) + \\text{Upsample}(M_{\\ell+1})\n\\]\nso we assert shape equality at each merge point.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "with torch.no_grad():\n    c2, c3, c4, c5 = C[\"C2\"], C[\"C3\"], C[\"C4\"], C[\"C5\"]\n\n    m5 = fpn.lat5(c5)\n    m4_up = F.interpolate(m5, scale_factor=2.0, mode=\"nearest\")\n    m4_lat = fpn.lat4(c4)\n    assert m4_up.shape == m4_lat.shape, (m4_up.shape, m4_lat.shape)\n\n    m4 = m4_lat + m4_up\n    m3_up = F.interpolate(m4, scale_factor=2.0, mode=\"nearest\")\n    m3_lat = fpn.lat3(c3)\n    assert m3_up.shape == m3_lat.shape, (m3_up.shape, m3_lat.shape)\n\n    m3 = m3_lat + m3_up\n    m2_up = F.interpolate(m3, scale_factor=2.0, mode=\"nearest\")\n    m2_lat = fpn.lat2(c2)\n    assert m2_up.shape == m2_lat.shape, (m2_up.shape, m2_lat.shape)\n\nprint(\"All FPN merge-shape assertions passed.\")\n"
  },
  {
   "cell_type": "code",
   "id": "95cc1f4d",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = ['C2/P2\\n(stride 4)', 'C3/P3\\n(stride 8)', 'C4/P4\\n(stride 16)', 'C5/P5\\n(stride 32)']\n",
    "backbone_ch = [64, 128, 256, 512]\n",
    "fpn_ch      = [256, 256, 256, 256]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "w = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 5))\n",
    "b1 = ax.bar(x - w/2, backbone_ch, w, label='Backbone Cₖ (heterogeneous)', color='#4e79a7', alpha=0.85)\n",
    "b2 = ax.bar(x + w/2, fpn_ch,      w, label='FPN output Pₖ (d = 256)',    color='#59a14f', alpha=0.85)\n",
    "\n",
    "ax.set_ylabel('Number of channels')\n",
    "ax.set_title('FPN channel unification: heterogeneous backbone → uniform 256-channel pyramid')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 600)\n",
    "for b, v in [(b1, backbone_ch), (b2, fpn_ch)]:\n",
    "    for bar, val in zip(b, v):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 6,\n",
    "                str(val), ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fpn_channel_unification.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6) What “preferred approach for FPN” means (operationally)\n\nIn a modern featurizer intended for FPN-style consumption, the pragmatic default is:\n\n1. Backbone (ResNet-style):\n   - Identity shortcut if $(C_{in}, H, W)$ matches $(C_{out}, H', W')$\n   - 1×1 projection shortcut (with stride=2 when downsampling) otherwise  \n   This matches the ResNet paper’s “projection to match dimensions” guidance and the widespread “option B” practice in deep variants.\n\n2. FPN neck:\n   - 1×1 lateral convs to unify all $C2..C5$ to $d=256$ channels\n   - top-down nearest-neighbor upsample by 2\n   - elementwise addition\n   - 3×3 smoothing conv\n   - optional $P6$ from $P5$ via stride-2 3×3 conv\n\nThe key theme is the same in both ResNet and FPN: **addition enforces strict shape equality**, so dimensioning is not a detail—it is the design constraint.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## References (primary sources)\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. *Deep Residual Learning for Image Recognition*. CVPR 2016. arXiv:1512.03385.\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. *Identity Mappings in Deep Residual Networks*. ECCV 2016. arXiv:1603.05027.\n- Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie. *Feature Pyramid Networks for Object Detection*. CVPR 2017. arXiv:1612.03144.\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
