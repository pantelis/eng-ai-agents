{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28d06a9a",
   "metadata": {
    "papermill": {
     "duration": 0.005299,
     "end_time": "2026-02-12T20:17:32.905507",
     "exception": false,
     "start_time": "2026-02-12T20:17:32.900208",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Batch Normalization\n",
    "\n",
    "## Input Normalization and its Limitations\n",
    "We have been traditionally normalizing the input data to have a mean of 0 and a standard deviation of 1. This is done to ensure that the input data is centered around 0 and has a similar scale to ensure that the optimization process is stable and converges faster. To see why this helps, consider a limiting example of two parameters as shown below. \n",
    "\n",
    "![Contour of the loss with respect to two parameters and SGD trajectory](images/sgd-convergence2.png)\n",
    "\n",
    "In this contour plot of the loss, the SGD trajectory shown is not smooth which means that the algorithm converges very slowly.  As shown in the backpropagation exercise with the single neuron, the gradient of the neuron output with respect to the weight is proportional to the input $x$ and the proportionality factor can be very small if the dot product is either very large or too small. In the case where all inputs are positive, the changes to the weights are all of the same sign across parameters. The reason is that there is a much larger dynamic range in the x-axis and this means that the gradient with respect to one of the parameters will dominate its _direction_ creating a zig-zag pattern. \n",
    "\n",
    "The best way to correct the situation is to normalize the input data around a mean of 0 and this will result into a much faster SGD convergence. After normalization you can get a more rounded (bivariate in this case) distribution where gradient directions can be  diverse. \n",
    "\n",
    "Normalizing the input is effective but it is not enough. The reason is that the input data is not the only thing that changes as the data propagates through the network. The distribution of activations also changes which is affected by the value of the parameters (weights).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Colab Environment Setup ---\n",
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    %pip install -q matplotlib seaborn scikit-learn scipy tqdm\n",
    "    print(\"Colab dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822a91d0",
   "metadata": {
    "papermill": {
     "duration": 0.004144,
     "end_time": "2026-02-12T20:17:32.914796",
     "exception": false,
     "start_time": "2026-02-12T20:17:32.910652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Parameter initialization\n",
    "\n",
    "Various techniques have been proposed to address this issue, such as careful initialization of the weights, or using activation functions that are less sensitive to the scale of the weights. In the code below we can see how various initializations can affect the output of a fully connected layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1850ead",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T20:17:32.920849Z",
     "iopub.status.busy": "2026-02-12T20:17:32.920660Z",
     "iopub.status.idle": "2026-02-12T20:17:36.699662Z",
     "shell.execute_reply": "2026-02-12T20:17:36.698948Z"
    },
    "papermill": {
     "duration": 3.782937,
     "end_time": "2026-02-12T20:17:36.700308",
     "exception": false,
     "start_time": "2026-02-12T20:17:32.917371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set up parameters\n",
    "n_input = 784\n",
    "n_dense = 256\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    _wandb_ok = bool(os.environ.get(\"WANDB_API_KEY\"))\n",
    "except ImportError:\n",
    "    wandb = None\n",
    "    _wandb_ok = False\n",
    "\n",
    "# Custom weight and bias initializers\n",
    "class RandomNormalInitializer:\n",
    "    def __init__(self, mean=0.0, std=1.0):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return nn.init.normal_(tensor, mean=self.mean, std=self.std)\n",
    "\n",
    "class ZerosInitializer:\n",
    "    def __call__(self, tensor):\n",
    "        return nn.init.zeros_(tensor)\n",
    "\n",
    "class GlorotNormalInitializer:\n",
    "    def __call__(self, tensor):\n",
    "        return nn.init.xavier_normal_(tensor)\n",
    "\n",
    "class GlorotUniformInitializer:\n",
    "    def __call__(self, tensor):\n",
    "        return nn.init.xavier_uniform_(tensor)\n",
    "\n",
    "class HeNormalInitializer:\n",
    "    def __call__(self, tensor):\n",
    "        return nn.init.kaiming_normal_(tensor, nonlinearity='relu')\n",
    "\n",
    "class HeUniformInitializer:\n",
    "    def __call__(self, tensor):\n",
    "        return nn.init.kaiming_uniform_(tensor, nonlinearity='relu')\n",
    "\n",
    "# Create a simple MLP model\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, n_input, n_dense, w_init, b_init):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc = nn.Linear(n_input, n_dense)\n",
    "        # Initialize weights and biases\n",
    "        w_init(self.fc.weight)\n",
    "        b_init(self.fc.bias)\n",
    "        self.activation = nn.ReLU() #nn.Sigmoid()  # You can change to Tanh or ReLU if needed\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "w_init =  HeNormalInitializer() #RandomNormalInitializer(std=1.0)  # Replace with desired initializer\n",
    "b_init = ZerosInitializer()\n",
    "model = SimpleMLP(n_input, n_dense, w_init, b_init)\n",
    "\n",
    "# Generate random input values\n",
    "x = torch.randn((1, n_input))\n",
    "\n",
    "# Forward propagate through the network\n",
    "a = model(x)\n",
    "\n",
    "x_np = x.detach().numpy()  # Convert to numpy for plotting\n",
    "_ = plt.hist(x_np.T)\n",
    "plt.title(\"Input Distribution\")\n",
    "plt.xlabel(\"Output Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e498c53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T20:17:36.706938Z",
     "iopub.status.busy": "2026-02-12T20:17:36.706494Z",
     "iopub.status.idle": "2026-02-12T20:17:36.811969Z",
     "shell.execute_reply": "2026-02-12T20:17:36.811241Z"
    },
    "papermill": {
     "duration": 0.110224,
     "end_time": "2026-02-12T20:17:36.812699",
     "exception": false,
     "start_time": "2026-02-12T20:17:36.702475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the output\n",
    "a_np = a.detach().numpy()  # Convert to numpy for plotting\n",
    "_ = plt.hist(a_np.T)\n",
    "plt.title(\"Output Distribution\")\n",
    "plt.xlabel(\"Output Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fc5ef4",
   "metadata": {
    "papermill": {
     "duration": 0.00218,
     "end_time": "2026-02-12T20:17:36.817359",
     "exception": false,
     "start_time": "2026-02-12T20:17:36.815179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bach Normalization\n",
    "\n",
    "Batch Normalization [@Bjorck2018-vc] alleviates a lot of headaches with properly initializing neural networks by explicitly forcing the activations throughout a network to have a specific distribution during training. Applying this technique amounts to insert the BatchNorm layer immediately after fully connected layers (or convolutional layers), and before activations although empirically we have found that good training behavior is obtained of batch normalization is applied after the activation function [@Santurkar2018-pu]. It involves two steps: \n",
    "\n",
    "  \n",
    "\n",
    "### Normalization\n",
    "\n",
    "It normalizes the input to _each layer_ such that the activations have zero mean and unit variance. The key idea here is to explicitly control and stabilize the distribution of the intermediate activations. For each mini-batch during training, the activations are normalized by subtracting the mean and dividing by the standard deviation of the mini-batch. This helps bring the data back to a more consistent distribution with a mean of zero and variance of one.\n",
    "\n",
    "   $$\n",
    "   \\hat{x} = \\frac{x - \\mu_{\\text{batch}}}{\\sqrt{\\sigma_{\\text{batch}}^2 + \\epsilon}}\n",
    "   $$\n",
    "\n",
    "   Where:\n",
    "   - $\\mu_{\\text{batch}}$ is the mean of the mini-batch.\n",
    "\n",
    "   - $\\sigma_{\\text{batch}}^2$ is the variance of the mini-batch.\n",
    "   \n",
    "   - $\\epsilon$ is a small value added to prevent division by zero.\n",
    "\n",
    "### Scaling and Shifting\n",
    "\n",
    "After normalization, the activations are scaled and shifted using two learnable parameters: $\\gamma$ and $\\beta$. This step allows the network to recover the original representation of the data if necessary and prevents over-restricting the learned features.\n",
    "\n",
    "   $$\n",
    "   y = \\gamma \\hat{x} + \\beta\n",
    "   $$\n",
    "\n",
    "This scaling and shifting ensure that the transformation is expressive enough to recover any input distribution while keeping it more stable.\n",
    "\n",
    "The main benefit of Batch Normalization is that of faster convergence, allowing for higher learning rates since it helps to stabilize the training. It also reduces the sensitivity to the initialization of the network parameters. \n",
    "\n",
    "## Intuition of BN\n",
    "\n",
    "We saw earlier during the treatment of backprop that for a single neuron, the gradient of its  output with respect to its parameters is proportional to the _input_. Extrapolating this observations to a layer of neurons is straightforward. Therefore controlling the statistics of what produced by the previous layer that is input to the current layer is important to ensure that the gradients are just right.  We dont know what is just right but we allow the network to let us know by introducing the learnable parameters $\\gamma$ and $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9e2f65",
   "metadata": {
    "papermill": {
     "duration": 0.002367,
     "end_time": "2026-02-12T20:17:36.821854",
     "exception": false,
     "start_time": "2026-02-12T20:17:36.819487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### An Example of BN in CNNs\n",
    "\n",
    "In the provided code, Batch Normalization is implemented manually after each convolutional layer (`conv1` and `conv2`). \n",
    "\n",
    "- During training, the mean and variance are computed based on the mini-batch.\n",
    "- The running mean and variance are updated using a momentum term.\n",
    "- During inference, the running mean and variance are used to normalize the activations instead of the batch statistics since its typical the batch size during training and inference to have different setting.\n",
    "\n",
    "\n",
    "In practice networks that use Batch Normalization are significantly more robust to parameter initialization. Additionally, batch normalization can be interpreted as doing preprocessing at every layer of the network, but integrated into the network itself in a differentiable manner. \n",
    "\n",
    "The effects of BN is reflected clearly in the distribution of the gradients for the same set of parameters as shown below.\n",
    "\n",
    "![Histograms over the gradients at initialization for (midpoint) layer 55 of a network with BN (left) and\n",
    "without (right). For the unnormalized network, the gradients are distributed with heavy tails, whereas for the normalized networks the gradients are concentrated around the mean.](images/histograms-bn.png)\n",
    "\n",
    "\n",
    "<!-- ![learning-rate-bn](images/learning-rate-bn.png)\n",
    "_BN allows us to learn much faster and operate efficiently at deeper architectures compared to without_ -->\n",
    "\n",
    "The following code demonstrates the effect of Batch Normalization on a simple neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b600353d",
   "metadata": {
    "papermill": {
     "duration": 0.001984,
     "end_time": "2026-02-12T20:17:36.826040",
     "exception": false,
     "start_time": "2026-02-12T20:17:36.824056",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<!-- ::: {.callout-note appearance=\"simple\"}\n",
    "Consider running [this excellent notebook](https://colab.research.google.com/github/davidcpage/cifar10-fast/blob/master/batch_norm_post.ipynb#scrollTo=a22VPg31CLSM) together with [this paper](https://arxiv.org/abs/1806.02375). \n",
    "\n",
    "::: -->\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72cae03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T20:17:36.832076Z",
     "iopub.status.busy": "2026-02-12T20:17:36.831769Z",
     "iopub.status.idle": "2026-02-12T20:22:27.811883Z",
     "shell.execute_reply": "2026-02-12T20:22:27.810513Z"
    },
    "papermill": {
     "duration": 290.98461,
     "end_time": "2026-02-12T20:22:27.812614",
     "exception": false,
     "start_time": "2026-02-12T20:17:36.828004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define a custom Batch Normalization layer\n",
    "class CustomBatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        super(CustomBatchNorm, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('running_var', torch.ones(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            # Calculate batch mean and variance\n",
    "            batch_mean = x.mean(dim=[0, 2, 3], keepdim=True)\n",
    "            batch_var = x.var(dim=[0, 2, 3], keepdim=True, unbiased=False)\n",
    "            # Normalize\n",
    "            x_hat = (x - batch_mean) / torch.sqrt(batch_var + self.eps)\n",
    "            # Scale and shift\n",
    "            out = self.gamma.view(1, -1, 1, 1) * x_hat + self.beta.view(1, -1, 1, 1)\n",
    "            # Update running statistics\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean.view(-1)\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var.view(-1)\n",
    "        else:\n",
    "            # Use running mean and variance during inference\n",
    "            x_hat = (x - self.running_mean.view(1, -1, 1, 1)) / torch.sqrt(self.running_var.view(1, -1, 1, 1) + self.eps)\n",
    "            out = self.gamma.view(1, -1, 1, 1) * x_hat + self.beta.view(1, -1, 1, 1)\n",
    "        return out\n",
    "\n",
    "# Define a simple CNN model with custom Batch Normalization\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.bn1 = CustomBatchNorm(10)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.bn2 = CustomBatchNorm(20)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Set up training parameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "weight_decay = 1e-4  # L2 regularization parameter\n",
    "patience = 20  # Early stopping patience\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = datasets.MNIST('/tmp/data', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_data, val_data = train_test_split(train_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check if GPU is available and use it\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Init W&B run\n",
    "_wb_run = None\n",
    "if _wandb_ok and wandb is not None:\n",
    "    try:\n",
    "        _wb_run = wandb.init(\n",
    "            settings=wandb.Settings(init_timeout=120),\n",
    "            project=\"eng-ai-agents\",\n",
    "            entity=\"pantelis\",\n",
    "            id=\"train-batch-norm-mnist\",\n",
    "            resume=\"allow\",\n",
    "            name=\"batch-norm-mnist\",\n",
    "            group=\"optimization\",\n",
    "            tags=[\"optimization\", \"batch-normalization\"],\n",
    "            job_type=\"training\",\n",
    "            config={\n",
    "                \"batch_size\": batch_size,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"weight_decay\": weight_decay,\n",
    "                \"patience\": patience,\n",
    "                \"num_epochs\": 100,\n",
    "            },\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"W&B init failed (non-fatal): {e}\")\n",
    "        _wb_run = None\n",
    "\n",
    "# Training loop with Early Stopping and TQDM for Epoch Progress\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "min_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "\n",
    "try:\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epoch Progress\", position=0):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            output = model(data)  # Forward pass\n",
    "            loss = criterion(output, target)  # Compute the loss\n",
    "            loss.backward()  # Backpropagate the gradients\n",
    "            optimizer.step()  # Update the weights\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f'Epoch {epoch + 1}: Train Loss: {avg_train_loss:.6f}')\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(f'Epoch {epoch + 1}: Validation Loss: {avg_val_loss:.6f}')\n",
    "\n",
    "        # Log to W&B\n",
    "        if _wb_run is not None:\n",
    "            _wb_run.log({\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"val_loss\": avg_val_loss,\n",
    "                \"epoch\": epoch,\n",
    "            })\n",
    "\n",
    "        # Early stopping check\n",
    "        if avg_val_loss < min_val_loss:\n",
    "            min_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping triggered after {epoch + 1} epochs.')\n",
    "                break\n",
    "\n",
    "    # Load the best model state (if early stopping was triggered)\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Log final summary to W&B\n",
    "    if _wb_run is not None:\n",
    "        _wb_run.summary[\"best_val_loss\"] = min_val_loss\n",
    "        _wb_run.summary[\"epochs_completed\"] = len(train_losses)\n",
    "        _wb_run.summary[\"early_stopped\"] = patience_counter >= patience\n",
    "finally:\n",
    "    if _wb_run is not None:\n",
    "        _wb_run.finish()\n",
    "\n",
    "# Plotting Train and Validation Loss vs Epochs\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Validation Loss vs Epochs with Dropout, Batch Normalization, and Early Stopping')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 298.739174,
   "end_time": "2026-02-12T20:22:30.745829",
   "environment_variables": {},
   "exception": null,
   "input_path": "notebooks/optimization/batch-normalization/index.ipynb",
   "output_path": "notebooks/optimization/batch-normalization/index-executed.ipynb",
   "parameters": {},
   "start_time": "2026-02-12T20:17:32.006655",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
