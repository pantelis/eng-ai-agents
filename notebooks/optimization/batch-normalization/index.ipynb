{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# Set up parameters\nn_input = 784\nn_dense = 256\n\ntry:\n    import wandb\n    _wandb_ok = bool(os.environ.get(\"WANDB_API_KEY\"))\nexcept ImportError:\n    wandb = None\n    _wandb_ok = False\n\n# Custom weight and bias initializers\nclass RandomNormalInitializer:\n    def __init__(self, mean=0.0, std=1.0):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        return nn.init.normal_(tensor, mean=self.mean, std=self.std)\n\nclass ZerosInitializer:\n    def __call__(self, tensor):\n        return nn.init.zeros_(tensor)\n\nclass GlorotNormalInitializer:\n    def __call__(self, tensor):\n        return nn.init.xavier_normal_(tensor)\n\nclass GlorotUniformInitializer:\n    def __call__(self, tensor):\n        return nn.init.xavier_uniform_(tensor)\n\nclass HeNormalInitializer:\n    def __call__(self, tensor):\n        return nn.init.kaiming_normal_(tensor, nonlinearity='relu')\n\nclass HeUniformInitializer:\n    def __call__(self, tensor):\n        return nn.init.kaiming_uniform_(tensor, nonlinearity='relu')\n\n# Create a simple MLP model\nclass SimpleMLP(nn.Module):\n    def __init__(self, n_input, n_dense, w_init, b_init):\n        super(SimpleMLP, self).__init__()\n        self.fc = nn.Linear(n_input, n_dense)\n        # Initialize weights and biases\n        w_init(self.fc.weight)\n        b_init(self.fc.bias)\n        self.activation = nn.ReLU() #nn.Sigmoid()  # You can change to Tanh or ReLU if needed\n\n    def forward(self, x):\n        x = self.fc(x)\n        x = self.activation(x)\n        return x\n\n# Initialize the model\nw_init =  HeNormalInitializer() #RandomNormalInitializer(std=1.0)  # Replace with desired initializer\nb_init = ZerosInitializer()\nmodel = SimpleMLP(n_input, n_dense, w_init, b_init)\n\n# Generate random input values\nx = torch.randn((1, n_input))\n\n# Forward propagate through the network\na = model(x)\n\nx_np = x.detach().numpy()  # Convert to numpy for plotting\n_ = plt.hist(x_np.T)\nplt.title(\"Input Distribution\")\nplt.xlabel(\"Output Value\")\nplt.ylabel(\"Frequency\")\nplt.show()\n",
   "outputs": [],
   "id": "b4fc6816"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot the output\n",
    "a_np = a.detach().numpy()  # Convert to numpy for plotting\n",
    "_ = plt.hist(a_np.T)\n",
    "plt.title(\"Output Distribution\")\n",
    "plt.xlabel(\"Output Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "id": "27529a88"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom tqdm import tqdm\n\n# Define a custom Batch Normalization layer\nclass CustomBatchNorm(nn.Module):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n        super(CustomBatchNorm, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        self.gamma = nn.Parameter(torch.ones(num_features))\n        self.beta = nn.Parameter(torch.zeros(num_features))\n        self.register_buffer('running_mean', torch.zeros(num_features))\n        self.register_buffer('running_var', torch.ones(num_features))\n\n    def forward(self, x):\n        if self.training:\n            # Calculate batch mean and variance\n            batch_mean = x.mean(dim=[0, 2, 3], keepdim=True)\n            batch_var = x.var(dim=[0, 2, 3], keepdim=True, unbiased=False)\n            # Normalize\n            x_hat = (x - batch_mean) / torch.sqrt(batch_var + self.eps)\n            # Scale and shift\n            out = self.gamma.view(1, -1, 1, 1) * x_hat + self.beta.view(1, -1, 1, 1)\n            # Update running statistics\n            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean.view(-1)\n            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var.view(-1)\n        else:\n            # Use running mean and variance during inference\n            x_hat = (x - self.running_mean.view(1, -1, 1, 1)) / torch.sqrt(self.running_var.view(1, -1, 1, 1) + self.eps)\n            out = self.gamma.view(1, -1, 1, 1) * x_hat + self.beta.view(1, -1, 1, 1)\n        return out\n\n# Define a simple CNN model with custom Batch Normalization\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.bn1 = CustomBatchNorm(10)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.bn2 = CustomBatchNorm(20)\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n        self.dropout = nn.Dropout(p=0.5)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = self.bn1(x)\n        x = self.dropout(x)\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = self.bn2(x)\n        x = self.dropout(x)\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n# Set up training parameters\nbatch_size = 64\nlearning_rate = 0.01\nweight_decay = 1e-4  # L2 regularization parameter\npatience = 20  # Early stopping patience\n\n# Load the dataset\ntrain_dataset = datasets.MNIST('/tmp/data', train=True, download=True, transform=transforms.ToTensor())\ntrain_data, val_data = train_test_split(train_dataset, test_size=0.2, random_state=42)\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n\n# Check if GPU is available and use it\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Initialize the model, loss function, and optimizer\nmodel = SimpleCNN().to(device)\ncriterion = nn.NLLLoss()\noptimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\n# Init W&B run\n_wb_run = None\nif _wandb_ok and wandb is not None:\n    try:\n        _wb_run = wandb.init(\n            settings=wandb.Settings(init_timeout=120),\n            project=\"eng-ai-agents\",\n            entity=\"pantelis\",\n            id=\"train-batch-norm-mnist\",\n            resume=\"allow\",\n            name=\"batch-norm-mnist\",\n            group=\"optimization\",\n            tags=[\"optimization\", \"batch-normalization\"],\n            job_type=\"training\",\n            config={\n                \"batch_size\": batch_size,\n                \"learning_rate\": learning_rate,\n                \"weight_decay\": weight_decay,\n                \"patience\": patience,\n                \"num_epochs\": 100,\n            },\n        )\n    except Exception as e:\n        print(f\"W&B init failed (non-fatal): {e}\")\n        _wb_run = None\n\n# Training loop with Early Stopping and TQDM for Epoch Progress\nnum_epochs = 100\ntrain_losses = []\nval_losses = []\nmin_val_loss = np.inf\npatience_counter = 0\n\ntry:\n    for epoch in tqdm(range(num_epochs), desc=\"Epoch Progress\", position=0):\n        model.train()\n        total_train_loss = 0\n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()  # Zero the gradients\n            output = model(data)  # Forward pass\n            loss = criterion(output, target)  # Compute the loss\n            loss.backward()  # Backpropagate the gradients\n            optimizer.step()  # Update the weights\n            total_train_loss += loss.item()\n        avg_train_loss = total_train_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n        print(f'Epoch {epoch + 1}: Train Loss: {avg_train_loss:.6f}')\n\n        # Validation loss\n        model.eval()\n        total_val_loss = 0\n        with torch.no_grad():\n            for data, target in val_loader:\n                data, target = data.to(device), target.to(device)\n                output = model(data)\n                loss = criterion(output, target)\n                total_val_loss += loss.item()\n        avg_val_loss = total_val_loss / len(val_loader)\n        val_losses.append(avg_val_loss)\n        print(f'Epoch {epoch + 1}: Validation Loss: {avg_val_loss:.6f}')\n\n        # Log to W&B\n        if _wb_run is not None:\n            _wb_run.log({\n                \"train_loss\": avg_train_loss,\n                \"val_loss\": avg_val_loss,\n                \"epoch\": epoch,\n            })\n\n        # Early stopping check\n        if avg_val_loss < min_val_loss:\n            min_val_loss = avg_val_loss\n            patience_counter = 0\n            best_model_state = model.state_dict()\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f'Early stopping triggered after {epoch + 1} epochs.')\n                break\n\n    # Load the best model state (if early stopping was triggered)\n    model.load_state_dict(best_model_state)\n\n    # Log final summary to W&B\n    if _wb_run is not None:\n        _wb_run.summary[\"best_val_loss\"] = min_val_loss\n        _wb_run.summary[\"epochs_completed\"] = len(train_losses)\n        _wb_run.summary[\"early_stopped\"] = patience_counter >= patience\nfinally:\n    if _wb_run is not None:\n        _wb_run.finish()\n\n# Plotting Train and Validation Loss vs Epochs\nplt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\nplt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Train and Validation Loss vs Epochs with Dropout, Batch Normalization, and Early Stopping')\nplt.legend()\nplt.grid(True)\nplt.show()",
   "outputs": [],
   "id": "2edff08a"
  }
 ]
}