{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f114f53",
   "metadata": {
    "papermill": {
     "duration": 0.004297,
     "end_time": "2026-02-12T20:22:42.293570",
     "exception": false,
     "start_time": "2026-02-12T20:22:42.289273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Regularization in Deep Neural Networks\n",
    "\n",
    "In this chapter we look at the training aspects of DNNs and investigate schemes that can help us avoid overfitting.  \n",
    "\n",
    "## L2 regularization\n",
    "\n",
    "This is perhaps the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. \n",
    "\n",
    "$$\\lambda J_{penalty} = \\lambda \\left(\\sum_l W_{(l)}^2 \\right) $$\n",
    "\n",
    "where $l$ is the hidden layer index and $W$ is the weight tensor. \n",
    "\n",
    "The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors.  Due to multiplicative interactions between weights and inputs this has the appealing property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot. The following figure presents a computational graph of a regularized DNN. \n",
    "\n",
    "![Regularized DNN computational graph](images/regularized-dnn-comp-graph.png)\n",
    "\n",
    "\n",
    "## L1 regularization \n",
    "\n",
    "This is another relatively common form of regularization, where for each weight $w$ we add the term $\\lambda  \\mid w \\mid$ to the objective. The L1 regularization has the intriguing property that it leads the weight vectors to become sparse during optimization (i.e. exactly zero). In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the \"noisy\" inputs. In comparison, final weight vectors from L2 regularization are usually diffuse, small numbers. In practice, if you are not concerned with explicit feature selection, L2 regularization can be expected to give superior performance over L1. Tools that do model size optimization (e.g. quantization of the model parameters) are typically involved and close to zero parameters are eliminated. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Colab Environment Setup ---\n",
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    %pip install -q matplotlib seaborn scikit-learn scipy tqdm\n",
    "    print(\"Colab dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b6517e",
   "metadata": {
    "papermill": {
     "duration": 0.003456,
     "end_time": "2026-02-12T20:22:42.301167",
     "exception": false,
     "start_time": "2026-02-12T20:22:42.297711",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Example of applying L2 regularization\n",
    "\n",
    "In the CNN MNIST example below notice that the application of L2 regualization is simply done  by adding the `weight_decay` parameter to the optimizer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc75391",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T20:22:42.308618Z",
     "iopub.status.busy": "2026-02-12T20:22:42.308184Z",
     "iopub.status.idle": "2026-02-12T20:33:05.570951Z",
     "shell.execute_reply": "2026-02-12T20:33:05.569405Z"
    },
    "papermill": {
     "duration": 623.268033,
     "end_time": "2026-02-12T20:33:05.572183",
     "exception": false,
     "start_time": "2026-02-12T20:22:42.304150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "\n",
    "    _wandb_ok = bool(os.environ.get(\"WANDB_API_KEY\"))\n",
    "except ImportError:\n",
    "    wandb = None\n",
    "    _wandb_ok = False\n",
    "\n",
    "# Check if GPU is available and use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# Set up training parameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "weight_decay = 1e-4  # L2 regularization parameter\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"/tmp/data\", train=True, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "# Note that we purposefully limit the number of training data to overfit the model\n",
    "train_data, val_data = train_test_split(train_dataset, test_size=0.99, random_state=42)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Initialize the model, loss function, and optimizers (with and without L2 regularization)\n",
    "model_with_l2 = SimpleCNN().to(device)\n",
    "model_without_l2 = SimpleCNN().to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer_with_l2 = optim.SGD(\n",
    "    model_with_l2.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    ")\n",
    "optimizer_without_l2 = optim.SGD(\n",
    "    model_without_l2.parameters(), lr=learning_rate, weight_decay=0\n",
    ")\n",
    "\n",
    "# Training loop for both optimizers\n",
    "num_epochs = 500\n",
    "train_losses_with_l2 = []\n",
    "val_losses_with_l2 = []\n",
    "train_losses_without_l2 = []\n",
    "val_losses_without_l2 = []\n",
    "\n",
    "# Init W&B run for L2 comparison\n",
    "_wb_run = None\n",
    "if _wandb_ok and wandb is not None:\n",
    "    try:\n",
    "        _wb_run = wandb.init(\n",
    "            settings=wandb.Settings(init_timeout=120),\n",
    "            project=\"eng-ai-agents\",\n",
    "            entity=\"pantelis\",\n",
    "            id=\"train-regularization-l2-comparison\",\n",
    "            resume=\"allow\",\n",
    "            name=\"regularization-l2-comparison\",\n",
    "            group=\"optimization\",\n",
    "            tags=[\"optimization\", \"regularization\", \"l2\"],\n",
    "            job_type=\"training\",\n",
    "            config={\n",
    "                \"batch_size\": batch_size,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"weight_decay\": weight_decay,\n",
    "                \"num_epochs\": num_epochs,\n",
    "            },\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"W&B init failed (non-fatal): {e}\")\n",
    "        _wb_run = None\n",
    "\n",
    "try:\n",
    "    # Training with L2 Regularization\n",
    "    model_with_l2.train()\n",
    "    for epoch in tqdm(\n",
    "        range(num_epochs), desc=\"L2 Reg Model Epoch Progress\", position=0\n",
    "    ):\n",
    "        total_train_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer_with_l2.zero_grad()  # Zero the gradients\n",
    "            output = model_with_l2(data)  # Forward pass\n",
    "            loss = criterion(output, target)  # Compute the loss\n",
    "            loss.backward()  # Backpropagate the gradients\n",
    "            optimizer_with_l2.step()  # Update the weights\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses_with_l2.append(avg_train_loss)\n",
    "\n",
    "        # Validation loss\n",
    "        model_with_l2.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model_with_l2(data)\n",
    "                loss = criterion(output, target)\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses_with_l2.append(avg_val_loss)\n",
    "\n",
    "        if _wb_run is not None:\n",
    "            _wb_run.log(\n",
    "                {\n",
    "                    \"l2_train_loss\": avg_train_loss,\n",
    "                    \"l2_val_loss\": avg_val_loss,\n",
    "                    \"epoch\": epoch,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Training without L2 Regularization\n",
    "    model_without_l2.train()\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Unreg Model Epoch Progress\", position=0):\n",
    "        total_train_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer_without_l2.zero_grad()  # Zero the gradients\n",
    "            output = model_without_l2(data)  # Forward pass\n",
    "            loss = criterion(output, target)  # Compute the loss\n",
    "            loss.backward()  # Backpropagate the gradients\n",
    "            optimizer_without_l2.step()  # Update the weights\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses_without_l2.append(avg_train_loss)\n",
    "\n",
    "        # Validation loss\n",
    "        model_without_l2.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model_without_l2(data)\n",
    "                loss = criterion(output, target)\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses_without_l2.append(avg_val_loss)\n",
    "\n",
    "        if _wb_run is not None:\n",
    "            _wb_run.log(\n",
    "                {\n",
    "                    \"unreg_train_loss\": avg_train_loss,\n",
    "                    \"unreg_val_loss\": avg_val_loss,\n",
    "                    \"epoch\": num_epochs + epoch,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if _wb_run is not None:\n",
    "        _wb_run.summary[\"l2_final_val_loss\"] = val_losses_with_l2[-1]\n",
    "        _wb_run.summary[\"unreg_final_val_loss\"] = val_losses_without_l2[-1]\n",
    "finally:\n",
    "    if _wb_run is not None:\n",
    "        _wb_run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d0addb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T20:33:05.654591Z",
     "iopub.status.busy": "2026-02-12T20:33:05.654341Z",
     "iopub.status.idle": "2026-02-12T20:33:05.795206Z",
     "shell.execute_reply": "2026-02-12T20:33:05.794485Z"
    },
    "papermill": {
     "duration": 0.18327,
     "end_time": "2026-02-12T20:33:05.795841",
     "exception": false,
     "start_time": "2026-02-12T20:33:05.612571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting Train and Validation Loss vs Epochs for both cases\n",
    "plt.plot(\n",
    "    range(1, num_epochs + 1),\n",
    "    train_losses_with_l2,\n",
    "    label=\"Train Loss With L2 Regularization\",\n",
    ")\n",
    "plt.plot(\n",
    "    range(1, num_epochs + 1),\n",
    "    val_losses_with_l2,\n",
    "    label=\"Validation Loss With L2 Regularization\",\n",
    ")\n",
    "plt.plot(\n",
    "    range(1, num_epochs + 1),\n",
    "    train_losses_without_l2,\n",
    "    label=\"Train Loss Without L2 Regularization\",\n",
    ")\n",
    "plt.plot(\n",
    "    range(1, num_epochs + 1),\n",
    "    val_losses_without_l2,\n",
    "    label=\"Validation Loss Without L2 Regularization\",\n",
    ")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train and Validation Loss vs Epochs With and Without L2 Regularization\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead0e183",
   "metadata": {
    "papermill": {
     "duration": 0.043264,
     "end_time": "2026-02-12T20:33:05.881419",
     "exception": false,
     "start_time": "2026-02-12T20:33:05.838155",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dropout\n",
    "This is an extremely effective, simple regularization technique by Srivastava et al. in [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) that complements the other methods (L1, L2). While training, dropout is implemented by only keeping a neuron active with some probability $p$ (a hyperparameter), or setting it to zero otherwise.\n",
    "\n",
    "![During training, Dropout can be interpreted as sampling a Neural Network within the full Neural Network, and only updating the parameters of the sampled network based on the input data. (However, the exponential number of possible sampled networks are not independent because they share the parameters.) During testing there is no dropout applied, with the interpretation of evaluating an averaged prediction across the exponentially-sized ensemble of all sub-networks.](images/dropout.jpeg)\n",
    "\n",
    "Vanilla dropout in an example 3-layer Neural Network would be implemented as follows:\n",
    "\n",
    "```python\n",
    "\"\"\" Vanilla Dropout: Not recommended implementation (see notes below) \"\"\"\n",
    "\n",
    "p = 0.5 # probability of keeping a unit active. higher = less dropout\n",
    "\n",
    "def train_step(X):\n",
    "  \"\"\" X contains the data \"\"\"\n",
    "  \n",
    "  # forward pass for example 3-layer neural network\n",
    "  H1 = np.maximum(0, np.dot(W1, X) + b1)\n",
    "  U1 = np.random.rand(*H1.shape) < p # first dropout mask\n",
    "  H1 *= U1 # drop!\n",
    "  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n",
    "  U2 = np.random.rand(*H2.shape) < p # second dropout mask\n",
    "  H2 *= U2 # drop!\n",
    "  out = np.dot(W3, H2) + b3\n",
    "  \n",
    "  # backward pass: compute gradients... (not shown)\n",
    "  # perform parameter update... (not shown)\n",
    "  \n",
    "def predict(X):\n",
    "  # ensembled forward pass\n",
    "  H1 = np.maximum(0, np.dot(W1, X) + b1) * p # NOTE: scale the activations\n",
    "  H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # NOTE: scale the activations\n",
    "  out = np.dot(W3, H2) + b3\n",
    "```\n",
    "\n",
    "In the code above, inside the ```train_step``` function we have performed dropout twice: on the first hidden layer and on the second hidden layer. It is also possible to perform dropout right on the input layer, in which case we would also create a binary mask for the input $X$. The backward pass remains unchanged, but of course has to take into account the generated masks $U1,U2$. \n",
    "\n",
    "Crucially, note that in the ```predict```  function we are not dropping anymore, but we are performing a scaling of both hidden layer outputs by $p$. This is important because at test time all neurons see all their inputs, so we want the outputs of neurons at test time to be identical to their expected outputs at training time. For example, in case of $p = 0.5$, the neurons must halve their outputs at test time to have the same output as they had during training time (in expectation). To see this, consider an output of a neuron before dropout, lets call it $x$. With dropout, the expected output from this neuron will become $px + (1-p)0$, because the neuron's output will be set to zero with probability $1-p$. At test time, when we keep the neuron always active, we must adjust $x \\rightarrow px$ to keep the same expected output. \n",
    "\n",
    "<Note>\n",
    "It can also be shown that performing this attenuation at test time can be related to the process of iterating over all the possible binary masks (and therefore all the exponentially many sub-networks) and computing their **ensemble prediction**.\n",
    "</Note>\n",
    "\n",
    "However, it is not very desirable to scale the activations by $p$ during inference expending a computational penalty when we serve the model and therefore we use **inverted dropout**, which performs the scaling at train time, leaving the forward pass at test time untouched. Additionally, this has the appealing property that the prediction code can remain untouched when you decide to tweak where you apply dropout, or if at all. Inverted dropout looks as follows:\n",
    "\n",
    "```python\n",
    "\"\"\" \n",
    "Inverted Dropout: Recommended implementation example.\n",
    "We drop and scale at train time and don't do anything at test time.\n",
    "\"\"\"\n",
    "\n",
    "p = 0.5 # probability of keeping a unit active. higher = less dropout\n",
    "\n",
    "def train_step(X):\n",
    "  # forward pass for example 3-layer neural network\n",
    "  H1 = np.maximum(0, np.dot(W1, X) + b1)\n",
    "  U1 = (np.random.rand(*H1.shape) < p) / p # first dropout mask. Notice /p!\n",
    "  H1 *= U1 # drop!\n",
    "  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n",
    "  U2 = (np.random.rand(*H2.shape) < p) / p # second dropout mask. Notice /p!\n",
    "  H2 *= U2 # drop!\n",
    "  out = np.dot(W3, H2) + b3\n",
    "  \n",
    "  # backward pass: compute gradients... (not shown)\n",
    "  # perform parameter update... (not shown)\n",
    "  \n",
    "def predict(X):\n",
    "  # ensembled forward pass\n",
    "  H1 = np.maximum(0, np.dot(W1, X) + b1) # no scaling necessary\n",
    "  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n",
    "  out = np.dot(W3, H2) + b3\n",
    "```\n",
    "\n",
    "Dropout falls into a more general category of methods that introduce stochastic behavior in the forward pass of the network. During testing, the noise is marginalized over *analytically* (as is the case with dropout when multiplying by $p$), or *numerically* (e.g. via sampling, by performing several forward passes with different random decisions and then averaging over them). An example of other research in this direction includes [DropConnect](http://cs.nyu.edu/~wanli/dropc/), where a random set of weights is instead set to zero during forward pass. As foreshadowing, Convolutional Neural Networks also take advantage of this theme with methods such as stochastic pooling, fractional pooling, and data augmentation. We will go into details of these methods later.\n",
    "\n",
    "In practice, it is most common to use a single, global L2 regularization strength that is cross-validated. It is also common to combine this with dropout applied after all layers. The value of $p = 0.5$ is a reasonable default, but this can be tuned on validation data. Note that dropout's usage has been limited by another technique called Batch Normalization and there is some [interesting interference](https://arxiv.org/pdf/1801.05134.pdf) between the two for those that want to dig further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad51597e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T20:33:05.967172Z",
     "iopub.status.busy": "2026-02-12T20:33:05.966903Z",
     "iopub.status.idle": "2026-02-12T20:38:13.769084Z",
     "shell.execute_reply": "2026-02-12T20:38:13.768504Z"
    },
    "papermill": {
     "duration": 307.846804,
     "end_time": "2026-02-12T20:38:13.769763",
     "exception": false,
     "start_time": "2026-02-12T20:33:05.922959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# Set up training parameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "weight_decay = 1e-4  # L2 regularization parameter\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"/tmp/data\", train=True, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "train_data, val_data = train_test_split(train_dataset, test_size=0.99, random_state=42)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check if GPU is available and use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Init W&B run for dropout training\n",
    "_wb_run = None\n",
    "if _wandb_ok and wandb is not None:\n",
    "    try:\n",
    "        _wb_run = wandb.init(\n",
    "            settings=wandb.Settings(init_timeout=120),\n",
    "            project=\"eng-ai-agents\",\n",
    "            entity=\"pantelis\",\n",
    "            id=\"train-regularization-dropout\",\n",
    "            resume=\"allow\",\n",
    "            name=\"regularization-dropout\",\n",
    "            group=\"optimization\",\n",
    "            tags=[\"optimization\", \"regularization\", \"dropout\"],\n",
    "            job_type=\"training\",\n",
    "            config={\n",
    "                \"batch_size\": batch_size,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"weight_decay\": weight_decay,\n",
    "                \"num_epochs\": 500,\n",
    "                \"dropout\": 0.5,\n",
    "            },\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"W&B init failed (non-fatal): {e}\")\n",
    "        _wb_run = None\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 500\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "try:\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epoch Progress\", position=0):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            output = model(data)  # Forward pass\n",
    "            loss = criterion(output, target)  # Compute the loss\n",
    "            loss.backward()  # Backpropagate the gradients\n",
    "            optimizer.step()  # Update the weights\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        if _wb_run is not None:\n",
    "            _wb_run.log(\n",
    "                {\"train_loss\": avg_train_loss, \"val_loss\": avg_val_loss, \"epoch\": epoch}\n",
    "            )\n",
    "\n",
    "    if _wb_run is not None:\n",
    "        _wb_run.summary[\"final_val_loss\"] = val_losses[-1]\n",
    "finally:\n",
    "    if _wb_run is not None:\n",
    "        _wb_run.finish()\n",
    "\n",
    "# Plotting Train and Validation Loss vs Epochs\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train and Validation Loss vs Epochs with Dropout\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4847e9",
   "metadata": {
    "papermill": {
     "duration": 0.042536,
     "end_time": "2026-02-12T20:38:13.855190",
     "exception": false,
     "start_time": "2026-02-12T20:38:13.812654",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Early stopping\n",
    "\n",
    " In these notes we focused on approaches that have some conceptual depth. We avoid treating extensively techniques that belong to the category of experiment management. For example early stopping is based on the experiment manager that is monitoring the validation loss and stops training when  it observes that the validation error increased while at the same time retrieves the best model that has been trained to the data scientist. This does not stop the approach being one of the most popular regularization approaches as it can be seen as an L2 regularizer as shown below.\n",
    "\n",
    " ![Early stopping trajectory vs L2 regularization trajectory](images/early-stopping2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472cba32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T20:38:13.940546Z",
     "iopub.status.busy": "2026-02-12T20:38:13.940348Z",
     "iopub.status.idle": "2026-02-12T20:38:54.364276Z",
     "shell.execute_reply": "2026-02-12T20:38:54.363642Z"
    },
    "papermill": {
     "duration": 40.467973,
     "end_time": "2026-02-12T20:38:54.365127",
     "exception": false,
     "start_time": "2026-02-12T20:38:13.897154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# Set up training parameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "weight_decay = 1e-4  # L2 regularization parameter\n",
    "patience = 3  # Early stopping patience\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"/tmp/data\", train=True, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "train_data, val_data = train_test_split(train_dataset, test_size=0.95, random_state=42)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check if GPU is available and use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Init W&B run for dropout + early stopping\n",
    "_wb_run = None\n",
    "if _wandb_ok and wandb is not None:\n",
    "    try:\n",
    "        _wb_run = wandb.init(\n",
    "            settings=wandb.Settings(init_timeout=120),\n",
    "            project=\"eng-ai-agents\",\n",
    "            entity=\"pantelis\",\n",
    "            id=\"train-regularization-dropout-early-stop\",\n",
    "            resume=\"allow\",\n",
    "            name=\"regularization-dropout-early-stop\",\n",
    "            group=\"optimization\",\n",
    "            tags=[\"optimization\", \"regularization\", \"dropout\", \"early-stopping\"],\n",
    "            job_type=\"training\",\n",
    "            config={\n",
    "                \"batch_size\": batch_size,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"weight_decay\": weight_decay,\n",
    "                \"num_epochs\": 500,\n",
    "                \"dropout\": 0.5,\n",
    "                \"patience\": patience,\n",
    "            },\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"W&B init failed (non-fatal): {e}\")\n",
    "        _wb_run = None\n",
    "\n",
    "# Training loop with Early Stopping\n",
    "num_epochs = 500\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "min_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "\n",
    "try:\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epoch Progress\", position=0):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            output = model(data)  # Forward pass\n",
    "            loss = criterion(output, target)  # Compute the loss\n",
    "            loss.backward()  # Backpropagate the gradients\n",
    "            optimizer.step()  # Update the weights\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        if _wb_run is not None:\n",
    "            _wb_run.log(\n",
    "                {\"train_loss\": avg_train_loss, \"val_loss\": avg_val_loss, \"epoch\": epoch}\n",
    "            )\n",
    "\n",
    "        # Early stopping check\n",
    "        if avg_val_loss < min_val_loss:\n",
    "            min_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                break\n",
    "\n",
    "    # Load the best model state (if early stopping was triggered)\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    if _wb_run is not None:\n",
    "        _wb_run.summary[\"best_val_loss\"] = min_val_loss\n",
    "        _wb_run.summary[\"epochs_completed\"] = len(train_losses)\n",
    "        _wb_run.summary[\"early_stopped\"] = patience_counter >= patience\n",
    "finally:\n",
    "    if _wb_run is not None:\n",
    "        _wb_run.finish()\n",
    "\n",
    "# Plotting Train and Validation Loss vs Epochs\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train and Validation Loss vs Epochs with Dropout and Early Stopping\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 975.720538,
   "end_time": "2026-02-12T20:38:56.901593",
   "environment_variables": {},
   "exception": null,
   "input_path": "notebooks/optimization/regularization/index.ipynb",
   "output_path": "notebooks/optimization/regularization/index-executed.ipynb",
   "parameters": {},
   "start_time": "2026-02-12T20:22:41.181055",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
