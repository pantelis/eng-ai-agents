{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\ntry:\n    import wandb\n    _wandb_ok = bool(os.environ.get(\"WANDB_API_KEY\"))\nexcept ImportError:\n    wandb = None\n    _wandb_ok = False\n\n# Check if GPU is available and use it\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Define a simple CNN model\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n# Set up training parameters\nbatch_size = 64\nlearning_rate = 0.01\nweight_decay = 1e-4  # L2 regularization parameter\n\n# Load the dataset\ntrain_dataset = datasets.MNIST('/tmp/data', train=True, download=True, transform=transforms.ToTensor())\n# Note that we purposefully limit the number of training data to overfit the model\ntrain_data, val_data = train_test_split(train_dataset, test_size=0.99, random_state=42)\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n\n\n# Initialize the model, loss function, and optimizers (with and without L2 regularization)\nmodel_with_l2 = SimpleCNN().to(device)\nmodel_without_l2 = SimpleCNN().to(device)\ncriterion = nn.NLLLoss()\noptimizer_with_l2 = optim.SGD(model_with_l2.parameters(), lr=learning_rate, weight_decay=weight_decay)\noptimizer_without_l2 = optim.SGD(model_without_l2.parameters(), lr=learning_rate, weight_decay=0)\n\n# Training loop for both optimizers\nnum_epochs = 500\ntrain_losses_with_l2 = []\nval_losses_with_l2 = []\ntrain_losses_without_l2 = []\nval_losses_without_l2 = []\n\n# Init W&B run for L2 comparison\n_wb_run = None\nif _wandb_ok and wandb is not None:\n    try:\n        _wb_run = wandb.init(\n            settings=wandb.Settings(init_timeout=120),\n            project=\"eng-ai-agents\",\n            entity=\"pantelis\",\n            id=\"train-regularization-l2-comparison\",\n            resume=\"allow\",\n            name=\"regularization-l2-comparison\",\n            group=\"optimization\",\n            tags=[\"optimization\", \"regularization\", \"l2\"],\n            job_type=\"training\",\n            config={\"batch_size\": batch_size, \"learning_rate\": learning_rate, \"weight_decay\": weight_decay, \"num_epochs\": num_epochs},\n        )\n    except Exception as e:\n        print(f\"W&B init failed (non-fatal): {e}\")\n        _wb_run = None\n\ntry:\n    # Training with L2 Regularization\n    model_with_l2.train()\n    for epoch in tqdm(range(num_epochs), desc=\"L2 Reg Model Epoch Progress\", position=0):\n        total_train_loss = 0\n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(device), target.to(device)\n            optimizer_with_l2.zero_grad()  # Zero the gradients\n            output = model_with_l2(data)  # Forward pass\n            loss = criterion(output, target)  # Compute the loss\n            loss.backward()  # Backpropagate the gradients\n            optimizer_with_l2.step()  # Update the weights\n            total_train_loss += loss.item()\n        avg_train_loss = total_train_loss / len(train_loader)\n        train_losses_with_l2.append(avg_train_loss)\n\n        # Validation loss\n        model_with_l2.eval()\n        total_val_loss = 0\n        with torch.no_grad():\n            for data, target in val_loader:\n                data, target = data.to(device), target.to(device)\n                output = model_with_l2(data)\n                loss = criterion(output, target)\n                total_val_loss += loss.item()\n        avg_val_loss = total_val_loss / len(val_loader)\n        val_losses_with_l2.append(avg_val_loss)\n\n        if _wb_run is not None:\n            _wb_run.log({\"l2_train_loss\": avg_train_loss, \"l2_val_loss\": avg_val_loss, \"epoch\": epoch})\n\n    # Training without L2 Regularization\n    model_without_l2.train()\n    for epoch in tqdm(range(num_epochs), desc=\"Unreg Model Epoch Progress\", position=0):\n        total_train_loss = 0\n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(device), target.to(device)\n            optimizer_without_l2.zero_grad()  # Zero the gradients\n            output = model_without_l2(data)  # Forward pass\n            loss = criterion(output, target)  # Compute the loss\n            loss.backward()  # Backpropagate the gradients\n            optimizer_without_l2.step()  # Update the weights\n            total_train_loss += loss.item()\n        avg_train_loss = total_train_loss / len(train_loader)\n        train_losses_without_l2.append(avg_train_loss)\n\n        # Validation loss\n        model_without_l2.eval()\n        total_val_loss = 0\n        with torch.no_grad():\n            for data, target in val_loader:\n                data, target = data.to(device), target.to(device)\n                output = model_without_l2(data)\n                loss = criterion(output, target)\n                total_val_loss += loss.item()\n        avg_val_loss = total_val_loss / len(val_loader)\n        val_losses_without_l2.append(avg_val_loss)\n\n        if _wb_run is not None:\n            _wb_run.log({\"unreg_train_loss\": avg_train_loss, \"unreg_val_loss\": avg_val_loss, \"epoch\": num_epochs + epoch})\n\n    if _wb_run is not None:\n        _wb_run.summary[\"l2_final_val_loss\"] = val_losses_with_l2[-1]\n        _wb_run.summary[\"unreg_final_val_loss\"] = val_losses_without_l2[-1]\nfinally:\n    if _wb_run is not None:\n        _wb_run.finish()",
   "outputs": [],
   "id": "e4a2b0ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plotting Train and Validation Loss vs Epochs for both cases\n",
    "plt.plot(range(1, num_epochs + 1), train_losses_with_l2, label='Train Loss With L2 Regularization')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses_with_l2, label='Validation Loss With L2 Regularization')\n",
    "plt.plot(range(1, num_epochs + 1), train_losses_without_l2, label='Train Loss Without L2 Regularization')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses_without_l2, label='Validation Loss Without L2 Regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Validation Loss vs Epochs With and Without L2 Regularization')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "id": "0c8e23e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n# Define a simple CNN model\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n        self.dropout = nn.Dropout(p=0.5)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = self.dropout(x)\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = self.dropout(x)\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n# Set up training parameters\nbatch_size = 64\nlearning_rate = 0.01\nweight_decay = 1e-4  # L2 regularization parameter\n\n# Load the dataset\ntrain_dataset = datasets.MNIST('/tmp/data', train=True, download=True, transform=transforms.ToTensor())\ntrain_data, val_data = train_test_split(train_dataset, test_size=0.99, random_state=42)\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n\n# Check if GPU is available and use it\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Initialize the model, loss function, and optimizer\nmodel = SimpleCNN().to(device)\ncriterion = nn.NLLLoss()\noptimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\n# Init W&B run for dropout training\n_wb_run = None\nif _wandb_ok and wandb is not None:\n    try:\n        _wb_run = wandb.init(\n            settings=wandb.Settings(init_timeout=120),\n            project=\"eng-ai-agents\",\n            entity=\"pantelis\",\n            id=\"train-regularization-dropout\",\n            resume=\"allow\",\n            name=\"regularization-dropout\",\n            group=\"optimization\",\n            tags=[\"optimization\", \"regularization\", \"dropout\"],\n            job_type=\"training\",\n            config={\"batch_size\": batch_size, \"learning_rate\": learning_rate, \"weight_decay\": weight_decay, \"num_epochs\": 500, \"dropout\": 0.5},\n        )\n    except Exception as e:\n        print(f\"W&B init failed (non-fatal): {e}\")\n        _wb_run = None\n\n# Training loop\nnum_epochs = 500\ntrain_losses = []\nval_losses = []\n\ntry:\n    for epoch in tqdm(range(num_epochs), desc=\"Epoch Progress\", position=0):\n        model.train()\n        total_train_loss = 0\n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()  # Zero the gradients\n            output = model(data)  # Forward pass\n            loss = criterion(output, target)  # Compute the loss\n            loss.backward()  # Backpropagate the gradients\n            optimizer.step()  # Update the weights\n            total_train_loss += loss.item()\n        avg_train_loss = total_train_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n\n        # Validation loss\n        model.eval()\n        total_val_loss = 0\n        with torch.no_grad():\n            for data, target in val_loader:\n                data, target = data.to(device), target.to(device)\n                output = model(data)\n                loss = criterion(output, target)\n                total_val_loss += loss.item()\n        avg_val_loss = total_val_loss / len(val_loader)\n        val_losses.append(avg_val_loss)\n\n        if _wb_run is not None:\n            _wb_run.log({\"train_loss\": avg_train_loss, \"val_loss\": avg_val_loss, \"epoch\": epoch})\n\n    if _wb_run is not None:\n        _wb_run.summary[\"final_val_loss\"] = val_losses[-1]\nfinally:\n    if _wb_run is not None:\n        _wb_run.finish()\n\n# Plotting Train and Validation Loss vs Epochs\nplt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\nplt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Train and Validation Loss vs Epochs with Dropout')\nplt.legend()\nplt.grid(True)\nplt.show()",
   "outputs": [],
   "id": "c5ab52c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Define a simple CNN model\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n        self.dropout = nn.Dropout(p=0.5)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = self.dropout(x)\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = self.dropout(x)\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n# Set up training parameters\nbatch_size = 64\nlearning_rate = 0.01\nweight_decay = 1e-4  # L2 regularization parameter\npatience = 3  # Early stopping patience\n\n# Load the dataset\ntrain_dataset = datasets.MNIST('/tmp/data', train=True, download=True, transform=transforms.ToTensor())\ntrain_data, val_data = train_test_split(train_dataset, test_size=0.95, random_state=42)\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n\n# Check if GPU is available and use it\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Initialize the model, loss function, and optimizer\nmodel = SimpleCNN().to(device)\ncriterion = nn.NLLLoss()\noptimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\n# Init W&B run for dropout + early stopping\n_wb_run = None\nif _wandb_ok and wandb is not None:\n    try:\n        _wb_run = wandb.init(\n            settings=wandb.Settings(init_timeout=120),\n            project=\"eng-ai-agents\",\n            entity=\"pantelis\",\n            id=\"train-regularization-dropout-early-stop\",\n            resume=\"allow\",\n            name=\"regularization-dropout-early-stop\",\n            group=\"optimization\",\n            tags=[\"optimization\", \"regularization\", \"dropout\", \"early-stopping\"],\n            job_type=\"training\",\n            config={\"batch_size\": batch_size, \"learning_rate\": learning_rate, \"weight_decay\": weight_decay, \"num_epochs\": 500, \"dropout\": 0.5, \"patience\": patience},\n        )\n    except Exception as e:\n        print(f\"W&B init failed (non-fatal): {e}\")\n        _wb_run = None\n\n# Training loop with Early Stopping\nnum_epochs = 500\ntrain_losses = []\nval_losses = []\nmin_val_loss = np.inf\npatience_counter = 0\n\ntry:\n    for epoch in tqdm(range(num_epochs), desc=\"Epoch Progress\", position=0):\n        model.train()\n        total_train_loss = 0\n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()  # Zero the gradients\n            output = model(data)  # Forward pass\n            loss = criterion(output, target)  # Compute the loss\n            loss.backward()  # Backpropagate the gradients\n            optimizer.step()  # Update the weights\n            total_train_loss += loss.item()\n        avg_train_loss = total_train_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n\n        # Validation loss\n        model.eval()\n        total_val_loss = 0\n        with torch.no_grad():\n            for data, target in val_loader:\n                data, target = data.to(device), target.to(device)\n                output = model(data)\n                loss = criterion(output, target)\n                total_val_loss += loss.item()\n        avg_val_loss = total_val_loss / len(val_loader)\n        val_losses.append(avg_val_loss)\n\n        if _wb_run is not None:\n            _wb_run.log({\"train_loss\": avg_train_loss, \"val_loss\": avg_val_loss, \"epoch\": epoch})\n\n        # Early stopping check\n        if avg_val_loss < min_val_loss:\n            min_val_loss = avg_val_loss\n            patience_counter = 0\n            best_model_state = model.state_dict()\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f'Early stopping triggered after {epoch + 1} epochs.')\n                break\n\n    # Load the best model state (if early stopping was triggered)\n    model.load_state_dict(best_model_state)\n\n    if _wb_run is not None:\n        _wb_run.summary[\"best_val_loss\"] = min_val_loss\n        _wb_run.summary[\"epochs_completed\"] = len(train_losses)\n        _wb_run.summary[\"early_stopped\"] = patience_counter >= patience\nfinally:\n    if _wb_run is not None:\n        _wb_run.finish()\n\n# Plotting Train and Validation Loss vs Epochs\nplt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\nplt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Train and Validation Loss vs Epochs with Dropout and Early Stopping')\nplt.legend()\nplt.grid(True)\nplt.show()",
   "outputs": [],
   "id": "06117ab9"
  }
 ]
}