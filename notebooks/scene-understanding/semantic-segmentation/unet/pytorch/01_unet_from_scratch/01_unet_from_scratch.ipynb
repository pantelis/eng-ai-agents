{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet from Scratch in PyTorch\n",
    "\n",
    "A self-contained implementation of [UNet](https://arxiv.org/abs/1505.04597) (Ronneberger et al., 2015) for semantic segmentation.\n",
    "\n",
    "**What you will build**:\n",
    "1. `DoubleConv` — two 3x3 convolutions with BatchNorm + ReLU\n",
    "2. `Down` — max-pool then DoubleConv (encoder)\n",
    "3. `Up` — bilinear upsample + skip connection + DoubleConv (decoder)\n",
    "4. `OutConv` — 1x1 convolution to output class logits\n",
    "5. `UNet` — full encoder-decoder with skip connections matching the original paper\n",
    "6. Training loop on FoodSeg103 (streamed from HuggingFace) with Dice + CE loss\n",
    "7. Predicted mask visualisation vs ground truth\n",
    "\n",
    "Dataset: [EduardoPacheco/FoodSeg103](https://huggingface.co/datasets/EduardoPacheco/FoodSeg103) streamed from Hugging Face — no local download required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once in container)\n",
    "!pip install datasets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "IMG_SIZE    = 256          # UNet input resolution (power of 2 for clean pooling)\n",
    "NUM_CLASSES = 103          # FoodSeg103: 103 food semantic categories\n",
    "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MEAN        = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "STD         = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "print(f'Device: {DEVICE}')\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(f'GPU : {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "\n",
    "We stream [FoodSeg103](https://huggingface.co/datasets/EduardoPacheco/FoodSeg103) directly from the Hugging Face Hub.\n",
    "Each sample contains:\n",
    "- `image` — RGB photograph of a food dish\n",
    "- `label` — per-pixel class mask in [0, 102] (103 food categories)\n",
    "\n",
    "We resize both image and mask to `IMG_SIZE x IMG_SIZE` using nearest-neighbour interpolation for the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodSeg103StreamDataset(IterableDataset):\n",
    "    \"\"\"Stream FoodSeg103 from HuggingFace and resize to IMG_SIZE x IMG_SIZE.\"\"\"\n",
    "\n",
    "    def __init__(self, split: str = 'train', max_samples: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.ds = load_dataset('EduardoPacheco/FoodSeg103', split=split, streaming=True)\n",
    "        self.max_samples = max_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        for sample in self.ds:\n",
    "            if self.max_samples and count >= self.max_samples:\n",
    "                break\n",
    "            img  = sample['image'].convert('RGB').resize((IMG_SIZE, IMG_SIZE))\n",
    "            mask = sample['label'].resize((IMG_SIZE, IMG_SIZE), Image.NEAREST)\n",
    "            img_t  = TF.to_tensor(img)                                    # (3, H, W)\n",
    "            img_t  = (img_t - MEAN) / STD                                # ImageNet normalisation\n",
    "            mask_t = torch.from_numpy(np.array(mask)).long()             # (H, W) in [0, 102]\n",
    "            mask_t = mask_t.clamp(0, NUM_CLASSES - 1)\n",
    "            count += 1\n",
    "            yield img_t, mask_t\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "ds_check = FoodSeg103StreamDataset(split='train', max_samples=2)\n",
    "imgs_c, masks_c = [], []\n",
    "for img, mask in ds_check:\n",
    "    imgs_c.append(img); masks_c.append(mask)\n",
    "\n",
    "print(f'Image shape : {imgs_c[0].shape}')\n",
    "print(f'Mask shape  : {masks_c[0].shape}')\n",
    "print(f'Unique mask values (sample): {masks_c[0].unique().tolist()[:10]} ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. UNet architecture\n",
    "\n",
    "The original UNet paper (Ronneberger et al., 2015) uses:\n",
    "- **Encoder**: 4 x (DoubleConv -> MaxPool), doubling channels each level: 64 -> 128 -> 256 -> 512\n",
    "- **Bottleneck**: DoubleConv at 1024 channels\n",
    "- **Decoder**: 4 x (Upsample + skip concat + DoubleConv), halving channels each level\n",
    "- **Output**: 1x1 convolution to `NUM_CLASSES` channels\n",
    "\n",
    "We use bilinear upsampling (rather than transposed convolutions) to avoid checkerboard artefacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"Two sequential 3x3 Conv -> BatchNorm -> ReLU blocks.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,  out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Encoder step: 2x2 MaxPool then DoubleConv.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.pool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.pool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder step: bilinear upsample, concatenate skip, then DoubleConv.\n",
    "\n",
    "    in_channels = upsampled_channels + skip_channels (channels after cat).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.up   = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, skip: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.up(x)\n",
    "        # Pad if spatial dims are odd\n",
    "        dh = skip.size(2) - x.size(2)\n",
    "        dw = skip.size(3) - x.size(3)\n",
    "        if dh > 0 or dw > 0:\n",
    "            x = F.pad(x, [dw // 2, dw - dw // 2, dh // 2, dh - dh // 2])\n",
    "        x = torch.cat([skip, x], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    \"\"\"1x1 convolution to produce per-pixel class logits.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet encoder-decoder with skip connections (bilinear upsample variant).\n",
    "\n",
    "    Channel flow:\n",
    "        Input (3) -> enc1(64) -> enc2(128) -> enc3(256) -> enc4(512) -> bottleneck(1024)\n",
    "        Decoder (channels after cat = upsampled + skip):\n",
    "            dec4: 1024+512=1536 -> 512\n",
    "            dec3:  512+256= 768 -> 256\n",
    "            dec2:  256+128= 384 -> 128\n",
    "            dec1:  128+ 64= 192 ->  64\n",
    "        out: 64 -> num_classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int = 3, num_classes: int = NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        self.enc1       = DoubleConv(in_channels, 64)\n",
    "        self.enc2       = Down(64,   128)\n",
    "        self.enc3       = Down(128,  256)\n",
    "        self.enc4       = Down(256,  512)\n",
    "        self.bottleneck = Down(512, 1024)\n",
    "        self.dec4       = Up(1536,  512)  # 1024 + 512\n",
    "        self.dec3       = Up(768,   256)  #  512 + 256\n",
    "        self.dec2       = Up(384,   128)  #  256 + 128\n",
    "        self.dec1       = Up(192,    64)  #  128 +  64\n",
    "        self.out        = OutConv(64, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Encoder\n",
    "        s1 = self.enc1(x)          # (B,  64, H,    W   )\n",
    "        s2 = self.enc2(s1)         # (B, 128, H/2,  W/2 )\n",
    "        s3 = self.enc3(s2)         # (B, 256, H/4,  W/4 )\n",
    "        s4 = self.enc4(s3)         # (B, 512, H/8,  W/8 )\n",
    "        x  = self.bottleneck(s4)   # (B,1024, H/16, W/16)\n",
    "        # Decoder\n",
    "        x  = self.dec4(x, s4)     # cat(1024,512)->1536 -> 512\n",
    "        x  = self.dec3(x, s3)     # cat(512, 256)-> 768 -> 256\n",
    "        x  = self.dec2(x, s2)     # cat(256, 128)-> 384 -> 128\n",
    "        x  = self.dec1(x, s1)     # cat(128,  64)-> 192 ->  64\n",
    "        return self.out(x)         # (B, num_classes, H, W)\n",
    "\n",
    "\n",
    "# Parameter count and shape check\n",
    "model   = UNet(in_channels=3, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "total   = sum(p.numel() for p in model.parameters())\n",
    "print(f'UNet parameters: {total:,}  (~{total/1e6:.1f}M)')\n",
    "\n",
    "x_dummy = torch.randn(2, 3, IMG_SIZE, IMG_SIZE, device=DEVICE)\n",
    "logits  = model(x_dummy)\n",
    "print(f'Input  : {tuple(x_dummy.shape)}')\n",
    "print(f'Output : {tuple(logits.shape)}  (B, C, H, W)')\n",
    "assert logits.shape == (2, NUM_CLASSES, IMG_SIZE, IMG_SIZE), 'Shape mismatch!'\n",
    "print('Shape check passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loss function\n",
    "\n",
    "We combine **cross-entropy loss** (pixel-wise classification) with **Dice loss** (overlap metric that handles class imbalance):\n",
    "\n",
    "$$\\mathcal{L} = \\mathcal{L}_{CE} + \\mathcal{L}_{\\text{Dice}}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\mathcal{L}_{\\text{Dice}} = 1 - \\frac{2 \\sum p_i \\, g_i + \\varepsilon}{\\sum p_i + \\sum g_i + \\varepsilon}$$\n",
    "\n",
    "and $p_i$ are the softmax probabilities for the true class, $g_i$ are the one-hot ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(logits: torch.Tensor, targets: torch.Tensor,\n",
    "              num_classes: int = NUM_CLASSES, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Soft multiclass Dice loss.\n",
    "\n",
    "    Args:\n",
    "        logits  : (B, C, H, W) raw logits\n",
    "        targets : (B, H, W)    long class indices in [0, C)\n",
    "    Returns:\n",
    "        Scalar mean Dice loss over classes.\n",
    "    \"\"\"\n",
    "    probs      = logits.softmax(dim=1)                        # (B, C, H, W)\n",
    "    targets_oh = F.one_hot(targets, num_classes)              # (B, H, W, C)\n",
    "    targets_oh = targets_oh.permute(0, 3, 1, 2).float()      # (B, C, H, W)\n",
    "    inter      = (probs * targets_oh).sum(dim=(2, 3))         # (B, C)\n",
    "    union      = probs.sum(dim=(2, 3)) + targets_oh.sum(dim=(2, 3))\n",
    "    dice       = (2.0 * inter + eps) / (union + eps)          # (B, C)\n",
    "    return 1.0 - dice.mean()\n",
    "\n",
    "\n",
    "def combined_loss(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    return F.cross_entropy(logits, targets) + dice_loss(logits, targets)\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "dummy_logits  = torch.randn(2, NUM_CLASSES, IMG_SIZE, IMG_SIZE)\n",
    "dummy_targets = torch.randint(0, NUM_CLASSES, (2, IMG_SIZE, IMG_SIZE))\n",
    "print(f'Combined loss (random init): {combined_loss(dummy_logits, dummy_targets).item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training loop\n",
    "\n",
    "We run a short training demo (10 gradient steps) to verify the full forward + backward pass.\n",
    "A real training run would use hundreds of epochs; this demo confirms shapes and that loss decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE  = 4\n",
    "TRAIN_STEPS = 10   # Demo: 10 steps to verify forward + backward pass\n",
    "LR          = 1e-3\n",
    "\n",
    "train_ds     = FoodSeg103StreamDataset(split='train', max_samples=BATCH_SIZE * TRAIN_STEPS)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "model     = UNet(in_channels=3, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "amp_on    = DEVICE.type == 'cuda'\n",
    "scaler    = torch.cuda.amp.GradScaler(enabled=amp_on)\n",
    "\n",
    "model.train()\n",
    "history = []\n",
    "\n",
    "for step, (imgs, masks) in enumerate(train_loader):\n",
    "    imgs  = imgs.to(DEVICE)\n",
    "    masks = masks.to(DEVICE)\n",
    "    optimizer.zero_grad()\n",
    "    with torch.cuda.amp.autocast(enabled=amp_on):\n",
    "        logits = model(imgs)\n",
    "        loss   = combined_loss(logits, masks)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    history.append(loss.item())\n",
    "    print(f'Step {step+1:2d}/{TRAIN_STEPS}  loss={loss.item():.4f}')\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(range(1, len(history) + 1), history, marker='o')\n",
    "plt.xlabel('Step'); plt.ylabel('Loss (CE + Dice)')\n",
    "plt.title('Training loss — UNet demo (FoodSeg103)'); plt.grid(True)\n",
    "plt.tight_layout(); plt.savefig('training_loss.png', dpi=120); plt.show()\n",
    "print('Saved: training_loss.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualisation\n",
    "\n",
    "We run inference on a small validation split and overlay the predicted segmentation masks on the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SAMPLES = 4\n",
    "\n",
    "val_ds = FoodSeg103StreamDataset(split='validation', max_samples=VAL_SAMPLES)\n",
    "val_imgs, val_masks = [], []\n",
    "for img, mask in val_ds:\n",
    "    val_imgs.append(img); val_masks.append(mask)\n",
    "\n",
    "val_batch = torch.stack(val_imgs).to(DEVICE)\n",
    "val_gt    = torch.stack(val_masks).to(DEVICE)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_logits = model(val_batch)\n",
    "val_preds = val_logits.argmax(dim=1).cpu().numpy()\n",
    "val_gt_np = val_gt.cpu().numpy()\n",
    "\n",
    "pixel_acc = (val_preds == val_gt_np).mean()\n",
    "print(f'Pixel accuracy ({VAL_SAMPLES} val samples, 10-step model): {pixel_acc:.3f}')\n",
    "\n",
    "\n",
    "def unnorm(t: torch.Tensor) -> np.ndarray:\n",
    "    return (t.cpu() * STD + MEAN).permute(1, 2, 0).numpy().clip(0, 1)\n",
    "\n",
    "\n",
    "CMAP = plt.cm.get_cmap('tab20', NUM_CLASSES)\n",
    "fig, axes = plt.subplots(VAL_SAMPLES, 3, figsize=(10, VAL_SAMPLES * 3))\n",
    "for i in range(VAL_SAMPLES):\n",
    "    axes[i, 0].imshow(unnorm(val_imgs[i]));                                    axes[i, 0].set_title('Input')\n",
    "    axes[i, 1].imshow(val_gt_np[i],  cmap=CMAP, vmin=0, vmax=NUM_CLASSES-1);  axes[i, 1].set_title('Ground truth')\n",
    "    axes[i, 2].imshow(val_preds[i],  cmap=CMAP, vmin=0, vmax=NUM_CLASSES-1);  axes[i, 2].set_title('Prediction')\n",
    "    for ax in axes[i]: ax.axis('off')\n",
    "\n",
    "plt.suptitle('UNet — input / ground truth / predicted mask (FoodSeg103)', fontsize=13)\n",
    "plt.tight_layout(); plt.savefig('unet_predictions.png', dpi=120); plt.show()\n",
    "print('Saved: unet_predictions.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
