{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "---\n",
    "title: Batch Normalization in ResNets\n",
    "description: PyTorch implementation showing how batch normalization enables deeper residual networks, with CIFAR-10 training experiments comparing models with and without BN.\n",
    "---\n",
    "\n",
    "# Batch Normalization in ResNets\n",
    "\n",
    "Batch normalization (BN) is a critical ingredient of modern residual networks. In this notebook we:\n",
    "\n",
    "1. Build a minimal ResNet block **with** and **without** batch normalization\n",
    "2. Train both variants on CIFAR-10 and compare convergence speed, final accuracy, and gradient health\n",
    "3. Visualize how BN stabilizes the distribution of intermediate activations across training\n",
    "\n",
    "The canonical ResNet block from He et al. (2016) is:\n",
    "\n",
    "$$y = f(x) + x, \\quad f(x) = W_2 * \\text{ReLU}(\\text{BN}(W_1 * x))$$\n",
    "\n",
    "where $*$ denotes convolution and $\\text{BN}$ normalizes the pre-activation tensor to have zero mean and unit variance, then scales and shifts with learnable $\\gamma$ and $\\beta$:\n",
    "\n",
    "$$\\hat x = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}, \\quad y = \\gamma \\hat x + \\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(f'  GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "## CIFAR-10 data loaders\n",
    "\n",
    "We use standard CIFAR-10 normalisation (channel mean and std computed from the training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='/tmp/cifar10', train=True, download=True, transform=train_transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='/tmp/cifar10', train=False, download=True, transform=test_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=2, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f'Train batches: {len(train_loader)},  Test batches: {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8",
   "metadata": {},
   "source": [
    "## ResNet building blocks\n",
    "\n",
    "We implement two variants of the basic residual block:\n",
    "\n",
    "- **`ResBlock`** — with batch normalization (`use_bn=True`, default)\n",
    "- **`ResBlock`** — without batch normalization (`use_bn=False`)\n",
    "\n",
    "The skip connection uses a $1\\times 1$ convolution when the spatial dimensions or channel count change (the *projection shortcut*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"Basic residual block (two 3x3 convs) with optional batch normalization.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, use_bn=True):\n",
    "        super().__init__()\n",
    "        self.use_bn = use_bn\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3,\n",
    "                               stride=stride, padding=1, bias=not use_bn)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels) if use_bn else nn.Identity()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3,\n",
    "                               stride=1, padding=1, bias=not use_bn)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels) if use_bn else nn.Identity()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Projection shortcut when dimensions change\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            layers = [nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=not use_bn)]\n",
    "            if use_bn:\n",
    "                layers.append(nn.BatchNorm2d(out_channels))\n",
    "            self.shortcut = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.relu(out + self.shortcut(x))\n",
    "        return out\n",
    "\n",
    "\n",
    "class SmallResNet(nn.Module):\n",
    "    \"\"\"Small ResNet for CIFAR-10 (6 residual blocks, 3 stages).\"\"\"\n",
    "\n",
    "    def __init__(self, use_bn=True, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.use_bn = use_bn\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, stride=1, padding=1, bias=not use_bn),\n",
    "            nn.BatchNorm2d(16) if use_bn else nn.Identity(),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.layer1 = self._make_layer(16, 16, 2, stride=1, use_bn=use_bn)\n",
    "        self.layer2 = self._make_layer(16, 32, 2, stride=2, use_bn=use_bn)\n",
    "        self.layer3 = self._make_layer(32, 64, 2, stride=2, use_bn=use_bn)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc   = nn.Linear(64, num_classes)\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_layer(in_ch, out_ch, n_blocks, stride, use_bn):\n",
    "        layers = [ResBlock(in_ch, out_ch, stride=stride, use_bn=use_bn)]\n",
    "        for _ in range(1, n_blocks):\n",
    "            layers.append(ResBlock(out_ch, out_ch, stride=1, use_bn=use_bn))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.pool(x).flatten(1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Count parameters\n",
    "model_bn    = SmallResNet(use_bn=True).to(DEVICE)\n",
    "model_no_bn = SmallResNet(use_bn=False).to(DEVICE)\n",
    "n_params = sum(p.numel() for p in model_bn.parameters())\n",
    "print(f'SmallResNet parameters: {n_params:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "We train both variants for the same number of epochs with the same SGD + cosine-annealing schedule and compare:\n",
    "\n",
    "- **Training loss** and **test accuracy** per epoch\n",
    "- **Gradient norms** at the stem layer (a proxy for gradient health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        correct    += outputs.argmax(1).eq(targets).sum().item()\n",
    "        total      += inputs.size(0)\n",
    "    return total_loss / total, 100.0 * correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        correct    += outputs.argmax(1).eq(targets).sum().item()\n",
    "        total      += inputs.size(0)\n",
    "    return total_loss / total, 100.0 * correct / total\n",
    "\n",
    "\n",
    "def grad_norm(model):\n",
    "    \"\"\"L2 norm of gradients at the stem conv layer.\"\"\"\n",
    "    p = model.stem[0].weight\n",
    "    return p.grad.norm().item() if p.grad is not None else 0.0\n",
    "\n",
    "\n",
    "def run_experiment(use_bn, n_epochs=30):\n",
    "    model = SmallResNet(use_bn=use_bn).to(DEVICE)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1,\n",
    "                          momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    history = {'train_loss': [], 'test_acc': [], 'grad_norm': []}\n",
    "    label = 'with BN' if use_bn else 'no BN'\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs), desc=f'Training ({label})', leave=True):\n",
    "        tr_loss, _ = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        # capture gradient norm after last training step\n",
    "        gn = grad_norm(model)\n",
    "        _, te_acc = evaluate(model, test_loader, criterion)\n",
    "        scheduler.step()\n",
    "\n",
    "        history['train_loss'].append(tr_loss)\n",
    "        history['test_acc'].append(te_acc)\n",
    "        history['grad_norm'].append(gn)\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "N_EPOCHS = 30\n",
    "hist_bn    = run_experiment(use_bn=True,  n_epochs=N_EPOCHS)\n",
    "hist_no_bn = run_experiment(use_bn=False, n_epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "## Results: training loss, test accuracy, and gradient norms\n",
    "\n",
    "The three plots below summarise the effect of batch normalisation on a residual network trained on CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, N_EPOCHS + 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Training loss\n",
    "axes[0].plot(epochs, hist_bn['train_loss'],    label='with BN',  color='steelblue')\n",
    "axes[0].plot(epochs, hist_no_bn['train_loss'], label='no BN',    color='tomato', linestyle='--')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Cross-entropy loss')\n",
    "axes[0].set_title('Training loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test accuracy\n",
    "axes[1].plot(epochs, hist_bn['test_acc'],    label='with BN',  color='steelblue')\n",
    "axes[1].plot(epochs, hist_no_bn['test_acc'], label='no BN',    color='tomato', linestyle='--')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Test accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient norms at stem\n",
    "axes[2].plot(epochs, hist_bn['grad_norm'],    label='with BN',  color='steelblue')\n",
    "axes[2].plot(epochs, hist_no_bn['grad_norm'], label='no BN',    color='tomato', linestyle='--')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Gradient L2 norm')\n",
    "axes[2].set_title('Stem layer gradient norm')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Residual network on CIFAR-10: with vs without batch normalisation', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig('bn_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'Final test accuracy  —  with BN: {hist_bn[\"test_acc\"][-1]:.1f}%  |  no BN: {hist_no_bn[\"test_acc\"][-1]:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4",
   "metadata": {},
   "source": [
    "## Activation distribution across training\n",
    "\n",
    "To see why BN helps, we capture the distribution of activations at the output of `layer1` at epochs 1, 15, and 30.  Without BN the distribution drifts and widens; with BN it stays anchored near zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer1_activations(model, loader, n_batches=3):\n",
    "    \"\"\"Collect a sample of layer1 output activations.\"\"\"\n",
    "    model.eval()\n",
    "    acts = []\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, _) in enumerate(loader):\n",
    "            if i >= n_batches:\n",
    "                break\n",
    "            x = inputs.to(DEVICE)\n",
    "            x = model.stem(x)\n",
    "            x = model.layer1(x)\n",
    "            acts.append(x.cpu().numpy().flatten())\n",
    "    return np.concatenate(acts)\n",
    "\n",
    "\n",
    "def train_and_capture(use_bn, epochs_to_capture, n_epochs=30):\n",
    "    model = SmallResNet(use_bn=use_bn).to(DEVICE)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    captured = {}\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        scheduler.step()\n",
    "        if epoch in epochs_to_capture:\n",
    "            captured[epoch] = get_layer1_activations(model, test_loader)\n",
    "    return captured\n",
    "\n",
    "\n",
    "CAPTURE_EPOCHS = {1, 15, 30}\n",
    "acts_bn    = train_and_capture(use_bn=True,  epochs_to_capture=CAPTURE_EPOCHS)\n",
    "acts_no_bn = train_and_capture(use_bn=False, epochs_to_capture=CAPTURE_EPOCHS)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(13, 7), sharey=False)\n",
    "\n",
    "for col, ep in enumerate(sorted(CAPTURE_EPOCHS)):\n",
    "    for row, (acts, label, color) in enumerate([\n",
    "            (acts_bn,    'with BN',  'steelblue'),\n",
    "            (acts_no_bn, 'no BN',    'tomato')]):\n",
    "        ax = axes[row][col]\n",
    "        ax.hist(np.clip(acts[ep], -5, 5), bins=80, color=color, alpha=0.75, density=True)\n",
    "        ax.set_title(f'Epoch {ep} — {label}')\n",
    "        ax.set_xlabel('Activation value')\n",
    "        if col == 0:\n",
    "            ax.set_ylabel('Density')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Layer 1 activation distributions over training', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig('activation_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Aspect | Without BN | With BN |\n",
    "|--------|-----------|--------|\n",
    "| Convergence speed | Slower, noisier loss | Faster, smoother |\n",
    "| Final test accuracy | Lower | Higher |\n",
    "| Gradient norms | Erratic, can vanish | Stable throughout |\n",
    "| Activation distribution | Drifts and widens | Stays near $\\mathcal{N}(0,1)$ |\n",
    "| Sensitivity to lr | High (requires careful tuning) | Low (tolerates higher lr) |\n",
    "\n",
    "These results confirm why every modern ResNet variant (ResNet-50, ResNeXt, Wide-ResNet) applies batch normalization after each convolution before the non-linearity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
