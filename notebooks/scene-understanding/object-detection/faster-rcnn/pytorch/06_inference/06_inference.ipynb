{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Faster RCNN Inference and Evaluation\n\n*Notebook 6 of 6 in the Faster RCNN from-scratch series*\n\nWe load the checkpoint saved in notebook 05, run inference on COCO validation\nimages streamed from Hugging Face, and visualise detections. Because the\ncheckpoint was trained for only 5 steps (demo), predictions are random — the\nnotebook focuses on the **inference pipeline** rather than accuracy.\n\nTopics covered:\n- Loading and verifying a checkpoint\n- Running `model.eval()` forward pass (proposal generation + postprocessing)\n- Visualising class-agnostic proposals and final detections\n- Measuring per-image inference latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c6d7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms.functional as TF\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom datasets import load_dataset\nfrom torch.utils.data import IterableDataset\nfrom torch.utils.checkpoint import checkpoint as grad_ckpt\nfrom typing import List, Tuple, Optional\nimport time, os\n\nIMG_SIZE    = 400   # must match notebook 05\nNUM_CLASSES = 81\nDEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {DEVICE}\")\n\nIMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\nIMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n\n# COCO 80-class names (1-indexed; 0 = background)\nCOCO_NAMES = [\n    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n    'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant',\n    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\n    'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass',\n    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n    'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n    'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n    'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n    'hair drier', 'toothbrush',\n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d9e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Re-define model components (self-contained) ──────────────────────────────\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n    def __init__(self, in_ch, out_ch, stride=1, downsample=None):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n        self.bn1   = nn.BatchNorm2d(out_ch)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n        self.bn2   = nn.BatchNorm2d(out_ch)\n        self.conv3 = nn.Conv2d(out_ch, out_ch * 4, 1, bias=False)\n        self.bn3   = nn.BatchNorm2d(out_ch * 4)\n        self.downsample = downsample\n    def forward(self, x):\n        identity = x\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        if self.downsample: identity = self.downsample(x)\n        return F.relu(out + identity)\n\nclass ResNet50(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.stem   = nn.Sequential(\n            nn.Conv2d(3, 64, 7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n            nn.MaxPool2d(3, stride=2, padding=1))\n        self.layer1 = self._make(  64,  64, 3, 1)\n        self.layer2 = self._make( 256, 128, 4, 2)\n        self.layer3 = self._make( 512, 256, 6, 2)\n        self.layer4 = self._make(1024, 512, 3, 2)\n    def _make(self, in_ch, out_ch, blocks, stride):\n        ds = None\n        if stride != 1 or in_ch != out_ch*4:\n            ds = nn.Sequential(nn.Conv2d(in_ch, out_ch*4, 1, stride=stride, bias=False),\n                               nn.BatchNorm2d(out_ch*4))\n        layers = [Bottleneck(in_ch, out_ch, stride, ds)]\n        for _ in range(1, blocks): layers.append(Bottleneck(out_ch*4, out_ch))\n        return nn.Sequential(*layers)\n    def forward(self, x):\n        x = self.stem(x); c2 = self.layer1(x); c3 = self.layer2(c2)\n        c4 = grad_ckpt(self.layer3, c3, use_reentrant=False)\n        c5 = grad_ckpt(self.layer4, c4, use_reentrant=False)\n        return c2, c3, c4, c5\n\nclass FPN(nn.Module):\n    def __init__(self, in_channels=(256,512,1024,2048), out_channels=256):\n        super().__init__()\n        self.lateral = nn.ModuleList([nn.Conv2d(c, out_channels, 1) for c in in_channels])\n        self.output  = nn.ModuleList([nn.Conv2d(out_channels, out_channels, 3, padding=1)\n                                      for _ in in_channels])\n        self.p6 = nn.MaxPool2d(1, stride=2)\n    def forward(self, features):\n        c2, c3, c4, c5 = features\n        p5 = self.lateral[3](c5)\n        p4 = self.lateral[2](c4) + F.interpolate(p5, size=c4.shape[-2:], mode='nearest')\n        p3 = self.lateral[1](c3) + F.interpolate(p4, size=c3.shape[-2:], mode='nearest')\n        p2 = self.lateral[0](c2) + F.interpolate(p3, size=c2.shape[-2:], mode='nearest')\n        outs = [self.output[i](p) for i, p in enumerate([p2, p3, p4, p5])]\n        outs.append(self.p6(outs[-1]))\n        return outs\n\ndef decode_boxes(anchors, deltas):\n    aw=anchors[:,2]-anchors[:,0]; ah=anchors[:,3]-anchors[:,1]\n    ax=anchors[:,0]+0.5*aw; ay=anchors[:,1]+0.5*ah\n    dx,dy=deltas[:,0],deltas[:,1]; dw=deltas[:,2].clamp(max=4.); dh=deltas[:,3].clamp(max=4.)\n    px=dx*aw+ax; py=dy*ah+ay; pw=torch.exp(dw)*aw; ph=torch.exp(dh)*ah\n    return torch.stack([px-0.5*pw, py-0.5*ph, px+0.5*pw, py+0.5*ph], dim=1)\n\nclass AnchorGenerator(nn.Module):\n    def __init__(self,sizes=(32,64,128,256,512),ratios=(0.5,1.,2.),strides=(4,8,16,32,64)):\n        super().__init__()\n        self.sizes=sizes; self.ratios=ratios; self.strides=strides\n    def _base(self,sz):\n        return torch.tensor([[-sz*(r**.5)/2,-sz/(r**.5)/2,sz*(r**.5)/2,sz/(r**.5)/2]\n                              for r in self.ratios],dtype=torch.float32)\n    def forward(self,fmaps,img_sz):\n        out=[]\n        for fm,sz,st in zip(fmaps,self.sizes,self.strides):\n            _,_,fh,fw=fm.shape; base=self._base(sz)\n            sx=(torch.arange(fw,device=fm.device)+0.5)*st\n            sy=(torch.arange(fh,device=fm.device)+0.5)*st\n            sy,sx=torch.meshgrid(sy,sx,indexing='ij')\n            shifts=torch.stack([sx,sy,sx,sy],dim=-1).reshape(-1,4)\n            out.append((shifts[:,None,:]+base.to(fm.device)[None,:,:]).reshape(-1,4))\n        return torch.cat(out,0)\n\nclass RPNHead(nn.Module):\n    def __init__(self,in_ch=256,k=3):\n        super().__init__()\n        self.conv=nn.Conv2d(in_ch,in_ch,3,padding=1)\n        self.cls=nn.Conv2d(in_ch,k,1); self.box=nn.Conv2d(in_ch,k*4,1)\n        for l in [self.conv,self.cls,self.box]:\n            nn.init.normal_(l.weight,std=0.01); nn.init.zeros_(l.bias)\n    def forward(self,features):\n        cls_o,box_o=[],[]\n        for f in features:\n            t=F.relu(self.conv(f)); cls_o.append(self.cls(t)); box_o.append(self.box(t))\n        return cls_o,box_o\n\nclass RegionProposalNetwork(nn.Module):\n    def __init__(self,head,anchor_gen,pre_nms=2000,post_nms=1000,nms_thr=0.7,min_sz=16):\n        super().__init__()\n        self.head=head; self.anchor_gen=anchor_gen\n        self.pre_nms=pre_nms; self.post_nms=post_nms; self.nms_thr=nms_thr; self.min_sz=min_sz\n    def _filter(self,props,scores,img_size):\n        H,W=img_size\n        props[:,[0,2]]=props[:,[0,2]].clamp(0,W); props[:,[1,3]]=props[:,[1,3]].clamp(0,H)\n        keep=(props[:,2]-props[:,0]>=self.min_sz)&(props[:,3]-props[:,1]>=self.min_sz)\n        props,scores=props[keep],scores[keep]\n        scores,order=scores.topk(min(self.pre_nms,len(scores)))\n        props=props[order]\n        keep=self._nms(props,scores,self.nms_thr)[:self.post_nms]\n        return props[keep],scores[keep]\n    @staticmethod\n    def _nms(boxes,scores,thr):\n        x1,y1,x2,y2=boxes.unbind(1); areas=(x2-x1)*(y2-y1)\n        order=scores.argsort(descending=True); keep=[]\n        while order.numel()>0:\n            i=order[0].item(); keep.append(i)\n            if order.numel()==1: break\n            xx1=x1[order[1:]].clamp(min=x1[i]); yy1=y1[order[1:]].clamp(min=y1[i])\n            xx2=x2[order[1:]].clamp(max=x2[i]); yy2=y2[order[1:]].clamp(max=y2[i])\n            inter=(xx2-xx1).clamp(0)*(yy2-yy1).clamp(0)\n            iou=inter/(areas[i]+areas[order[1:]]-inter).clamp(1e-6)\n            order=order[1:][iou<=thr]\n        return torch.tensor(keep,dtype=torch.long)\n    def forward(self,features,image_size,targets=None):\n        cls_o,box_o=self.head(features); anchors=self.anchor_gen(features,image_size)\n        all_scores=torch.cat([c.permute(0,2,3,1).reshape(c.shape[0],-1) for c in cls_o],1)\n        all_deltas=torch.cat([b.permute(0,2,3,1).reshape(b.shape[0],-1,4) for b in box_o],1)\n        props=[]\n        for i in range(all_scores.shape[0]):\n            sc=all_scores[i].sigmoid(); pr=decode_boxes(anchors,all_deltas[i])\n            pr,_=self._filter(pr.detach(),sc.detach(),image_size); props.append(pr)\n        return props,{}\n\nclass ROIAlign(nn.Module):\n    def __init__(self,out_size=7,k0=4,k_min=2,k_max=5):\n        super().__init__()\n        self.out_size=out_size; self.k0=k0; self.k_min=k_min; self.k_max=k_max\n    def _level(self,boxes):\n        areas=((boxes[:,2]-boxes[:,0])*(boxes[:,3]-boxes[:,1])).clamp(1e-6).sqrt()\n        return torch.floor(self.k0+torch.log2(areas/224.)).long().clamp(self.k_min,self.k_max)-self.k_min\n    def forward(self,fmaps,proposals,image_size):\n        H,W=image_size; all_feats=[]\n        for bi,props in enumerate(proposals):\n            if len(props)==0: continue\n            levels=self._level(props)\n            feats=torch.zeros(len(props),fmaps[0].shape[1],self.out_size,self.out_size,device=props.device)\n            for lvl,fm in enumerate(fmaps[:4]):\n                mask=levels==lvl\n                if not mask.any(): continue\n                lp=props[mask]; n=len(lp)\n                x1=lp[:,0]/W*2-1; y1=lp[:,1]/H*2-1; x2=lp[:,2]/W*2-1; y2=lp[:,3]/H*2-1\n                gx=torch.linspace(0,1,self.out_size,device=props.device)\n                gy=torch.linspace(0,1,self.out_size,device=props.device)\n                gy_g,gx_g=torch.meshgrid(gy,gx,indexing='ij')\n                gx_g=x1[:,None,None]+(x2-x1)[:,None,None]*gx_g[None]\n                gy_g=y1[:,None,None]+(y2-y1)[:,None,None]*gy_g[None]\n                grid=torch.stack([gx_g,gy_g],dim=-1)\n                crops=F.grid_sample(fm[bi:bi+1].expand(n,-1,-1,-1),grid,\n                                    align_corners=True,mode='bilinear',padding_mode='border')\n                feats[mask]=crops\n            all_feats.append(feats)\n        if not all_feats:\n            return torch.zeros(0,fmaps[0].shape[1],self.out_size,self.out_size,device=fmaps[0].device)\n        return torch.cat(all_feats,0)\n\nclass TwoMLPHead(nn.Module):\n    def __init__(self,in_channels=256*7*7,fc_dim=1024):\n        super().__init__()\n        self.fc1=nn.Linear(in_channels,fc_dim); self.fc2=nn.Linear(fc_dim,fc_dim)\n    def forward(self,x): return F.relu(self.fc2(F.relu(self.fc1(x.flatten(1)))))\n\nclass FastRCNNPredictor(nn.Module):\n    def __init__(self,in_channels=1024,num_classes=81):\n        super().__init__()\n        self.cls=nn.Linear(in_channels,num_classes); self.box=nn.Linear(in_channels,num_classes*4)\n        nn.init.normal_(self.cls.weight,std=0.01); nn.init.zeros_(self.cls.bias)\n        nn.init.normal_(self.box.weight,std=0.001); nn.init.zeros_(self.box.bias)\n    def forward(self,x): return self.cls(x),self.box(x)\n\nclass FasterRCNN(nn.Module):\n    SCORE_THR=0.05; NMS_THR=0.5; MAX_DETS=100\n    def __init__(self,num_classes=81):\n        super().__init__()\n        self.num_classes=num_classes\n        self.backbone=ResNet50(); self.fpn=FPN()\n        self.rpn=RegionProposalNetwork(RPNHead(),AnchorGenerator())\n        self.roi_align=ROIAlign(out_size=7)\n        self.box_head=TwoMLPHead(); self.predictor=FastRCNNPredictor(num_classes=num_classes)\n    def _postprocess(self,cls_logits,bbox_preds,proposals_list,image_size):\n        H,W=image_size; C=self.num_classes; results=[]; offset=0\n        for props in proposals_list:\n            n=len(props)\n            if n==0:\n                results.append({'boxes':torch.zeros(0,4),'scores':torch.zeros(0),\n                                'labels':torch.zeros(0,dtype=torch.long)}); continue\n            logits=cls_logits[offset:offset+n]; deltas=bbox_preds[offset:offset+n]; offset+=n\n            scores=F.softmax(logits,-1)\n            all_b,all_s,all_l=[],[],[]\n            for ci in range(1,C):\n                boxes=decode_boxes(props,deltas.view(n,C,4)[:,ci,:])\n                boxes[:,[0,2]]=boxes[:,[0,2]].clamp(0,W); boxes[:,[1,3]]=boxes[:,[1,3]].clamp(0,H)\n                sc=scores[:,ci]; mask=sc>self.SCORE_THR\n                if not mask.any(): continue\n                keep=RegionProposalNetwork._nms(boxes[mask],sc[mask],self.NMS_THR)\n                all_b.append(boxes[mask][keep]); all_s.append(sc[mask][keep])\n                all_l.append(torch.full((len(keep),),ci,dtype=torch.long,device=props.device))\n            if all_b:\n                b=torch.cat(all_b); s=torch.cat(all_s); l=torch.cat(all_l)\n                top=s.argsort(descending=True)[:self.MAX_DETS]\n                results.append({'boxes':b[top],'scores':s[top],'labels':l[top]})\n            else:\n                results.append({'boxes':torch.zeros(0,4),'scores':torch.zeros(0),\n                                'labels':torch.zeros(0,dtype=torch.long)})\n        return results\n    def forward(self,images,targets=None):\n        img_sz=(images.shape[2],images.shape[3])\n        feats=self.backbone(images); fpn_fs=self.fpn(feats)\n        props,_=self.rpn(fpn_fs,img_sz)\n        roi_feats=self.roi_align(fpn_fs[:4],props,img_sz)\n        box_feats=self.box_head(roi_feats)\n        cls_logits,bbox_preds=self.predictor(box_feats)\n        return self._postprocess(cls_logits,bbox_preds,props,img_sz), props\n\nprint(\"Model architecture defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Load checkpoint ──────────────────────────────────────────────────────────\n\nCKPT_PATH = 'checkpoints/faster_rcnn_demo.pth'\nassert os.path.exists(CKPT_PATH), f\"Checkpoint not found at {CKPT_PATH}. Run notebook 05 first.\"\n\nmodel = FasterRCNN(num_classes=NUM_CLASSES).to(DEVICE)\nckpt  = torch.load(CKPT_PATH, map_location=DEVICE)\nmodel.load_state_dict(ckpt['model_state_dict'])\nmodel.eval()\n\nprint(f\"Checkpoint loaded: {CKPT_PATH}\")\nprint(f\"  Trained for {ckpt['steps_trained']} steps\")\nprint(f\"  Final losses: { {k: f'{v:.4f}' for k,v in ckpt['final_losses'].items()} }\")\ntotal = sum(p.numel() for p in model.parameters())\nprint(f\"  Parameters: {total/1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Inference on 4 validation images ─────────────────────────────────────────\n\nval_ds = load_dataset('detection-datasets/coco', split='val', streaming=True)\n\nimages_pil, results_list, latencies = [], [], []\nNUM_IMAGES = 4\n\nwith torch.no_grad():\n    for i, sample in enumerate(val_ds):\n        if i >= NUM_IMAGES:\n            break\n        img_pil = sample['image'].convert('RGB')\n        images_pil.append(img_pil)\n        t = ((TF.to_tensor(img_pil.resize((IMG_SIZE, IMG_SIZE))) - IMAGENET_MEAN) / IMAGENET_STD)\n        t = t.unsqueeze(0).to(DEVICE)\n\n        t0 = time.perf_counter()\n        dets, proposals = model(t)\n        if DEVICE.type == 'cuda': torch.cuda.synchronize()\n        latencies.append((time.perf_counter() - t0) * 1000)\n        results_list.append(dets[0])\n\nprint(f\"Mean latency: {sum(latencies)/len(latencies):.1f} ms  ({IMG_SIZE}x{IMG_SIZE} input)\")\nprint(\"Detections per image:\", [len(r['boxes']) for r in results_list])\nprint(\"(Detections are random — model trained only 5 steps)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a8b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Visualise proposals + detections ─────────────────────────────────────────\n\nfig, axes = plt.subplots(2, NUM_IMAGES, figsize=(5*NUM_IMAGES, 10))\nTOP_K_PROPS = 30\n\nwith torch.no_grad():\n    for col, sample in enumerate(load_dataset('detection-datasets/coco',\n                                              split='val', streaming=True)):\n        if col >= NUM_IMAGES: break\n        img_pil = sample['image'].convert('RGB')\n        img_res = img_pil.resize((IMG_SIZE, IMG_SIZE))\n        t = ((TF.to_tensor(img_res) - IMAGENET_MEAN) / IMAGENET_STD).unsqueeze(0).to(DEVICE)\n\n        _, proposals = model(t)\n        props = proposals[0].cpu()[:TOP_K_PROPS]\n        dets  = results_list[col]\n\n        # Row 0: RPN proposals\n        ax = axes[0][col]\n        ax.imshow(img_res); ax.axis('off')\n        ax.set_title(f'Image {col+1}: top-{TOP_K_PROPS} proposals', fontsize=9)\n        for box in props.tolist():\n            x1,y1,x2,y2=box\n            ax.add_patch(patches.Rectangle((x1,y1),x2-x1,y2-y1,\n                                            linewidth=1,edgecolor='cyan',facecolor='none'))\n\n        # Row 1: final detections\n        ax = axes[1][col]\n        ax.imshow(img_res); ax.axis('off')\n        n_det = len(dets['boxes'])\n        ax.set_title(f'Image {col+1}: {n_det} detections', fontsize=9)\n        for box,score,label in zip(dets['boxes'].tolist(),\n                                   dets['scores'].tolist(),\n                                   dets['labels'].tolist()):\n            x1,y1,x2,y2=box\n            cls_name = COCO_NAMES[label] if label < len(COCO_NAMES) else str(label)\n            ax.add_patch(patches.Rectangle((x1,y1),x2-x1,y2-y1,\n                                            linewidth=1.5,edgecolor='red',facecolor='none'))\n            ax.text(x1,y1-2,f'{cls_name} {score:.2f}',\n                    color='white',fontsize=6,backgroundcolor='red')\n\nplt.suptitle('Row 1: RPN proposals  |  Row 2: Final detections (5-step model)', y=1.01)\nplt.tight_layout()\nos.makedirs('images', exist_ok=True)\nplt.savefig('images/inference_results.png', dpi=100, bbox_inches='tight')\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g8h9i0j1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Latency bar chart ─────────────────────────────────────────────────────────\n\nfig, ax = plt.subplots(figsize=(7, 4))\nax.bar(range(1, NUM_IMAGES+1), latencies, color='steelblue', edgecolor='white')\nax.axhline(sum(latencies)/len(latencies), color='red', linestyle='--', label='mean')\nax.set_xlabel('Image index'); ax.set_ylabel('Latency (ms)')\nax.set_title(f'Per-image inference latency on {str(DEVICE).upper()} ({IMG_SIZE}x{IMG_SIZE})')\nax.legend()\nplt.tight_layout()\nplt.savefig('images/latency.png', dpi=100, bbox_inches='tight')\nplt.show()\nprint(\"\\nSeries complete. Faster RCNN from scratch — all 6 notebooks executed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
