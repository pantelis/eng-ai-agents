{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO Data Pipeline for Faster RCNN\n",
    "\n",
    "*Notebook 1 of 6 in the Faster RCNN from-scratch series*\n",
    "\n",
    "Dataset: [COCO 2017](https://huggingface.co/datasets/detection-datasets/coco) streamed from Hugging Face — no local download required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once in container)\n",
    "!pip install datasets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Configuration\n",
    "IMG_SIZE = 800          # Faster RCNN uses 800x800 (vs 640 for YOLO)\n",
    "NUM_CLASSES = 80        # COCO categories\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD  = [0.229, 0.224, 0.225]\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# COCO category names (80 categories, 0-indexed as in HF dataset)\n",
    "COCO_NAMES = [\n",
    "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck',\n",
    "    'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench',\n",
    "    'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra',\n",
    "    'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "    'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n",
    "    'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',\n",
    "    'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
    "    'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "    'hair drier', 'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We stream COCO 2017 directly from [detection-datasets/coco](https://huggingface.co/datasets/detection-datasets/coco) on the Hugging Face Hub — no local download or annotation files required.\n",
    "\n",
    "The HF dataset provides bounding boxes in COCO format `[x, y, w, h]` (pixels, top-left corner) with **0-indexed** category labels. Faster RCNN requires:\n",
    "- Boxes in `[x1, y1, x2, y2]` pixel coordinates, scaled to the resized image\n",
    "- **1-indexed** labels (0 = background, 1–80 = COCO categories)\n",
    "- ImageNet-normalized image tensors of shape `(3, 800, 800)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOStreamDataset(IterableDataset):\n",
    "    \"\"\"Stream COCO from Hugging Face and yield Faster RCNN-format samples.\n",
    "\n",
    "    Each sample is resized to img_size x img_size and normalized with\n",
    "    ImageNet statistics. Bounding boxes are converted from COCO\n",
    "    [x, y, w, h] pixel format to [x1, y1, x2, y2] scaled to the\n",
    "    resized image. Labels are 1-indexed (0 = background).\n",
    "\n",
    "    Returns (image_tensor, target) where target is a dict with:\n",
    "        boxes    — (N, 4) float32 [x1, y1, x2, y2] in pixel coords\n",
    "        labels   — (N,) int64, 1-indexed category IDs\n",
    "        image_id — int\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split: str = 'train', max_samples: int = None,\n",
    "                 img_size: int = 800):\n",
    "        self.split = split\n",
    "        self.max_samples = max_samples\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        ds = load_dataset('detection-datasets/coco',\n",
    "                          split=self.split, streaming=True)\n",
    "        count = 0\n",
    "        for example in ds:\n",
    "            if self.max_samples and count >= self.max_samples:\n",
    "                break\n",
    "\n",
    "            img = example['image'].convert('RGB')\n",
    "            orig_w, orig_h = img.size\n",
    "\n",
    "            # Skip images with no annotations\n",
    "            bboxes = example['objects']['bbox']\n",
    "            cats   = example['objects']['category']\n",
    "            if not bboxes:\n",
    "                continue\n",
    "\n",
    "            # Resize to fixed square\n",
    "            img = img.resize((self.img_size, self.img_size), Image.BILINEAR)\n",
    "            scale_x = self.img_size / orig_w\n",
    "            scale_y = self.img_size / orig_h\n",
    "\n",
    "            # Normalize\n",
    "            img_arr = np.array(img, dtype=np.float32) / 255.0\n",
    "            img_arr = (img_arr - MEAN) / STD\n",
    "            img_tensor = torch.from_numpy(img_arr).permute(2, 0, 1)  # CHW\n",
    "\n",
    "            # Convert boxes: [x, y, w, h] pixels -> [x1, y1, x2, y2] scaled\n",
    "            boxes, labels = [], []\n",
    "            for bbox, cat_id in zip(bboxes, cats):\n",
    "                x, y, w, h = bbox\n",
    "                x1 = x * scale_x\n",
    "                y1 = y * scale_y\n",
    "                x2 = (x + w) * scale_x\n",
    "                y2 = (y + h) * scale_y\n",
    "                if (x2 - x1) > 1 and (y2 - y1) > 1:\n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "                    labels.append(int(cat_id) + 1)  # 0-indexed -> 1-indexed\n",
    "\n",
    "            if not boxes:\n",
    "                continue\n",
    "\n",
    "            target = {\n",
    "                'boxes':    torch.tensor(boxes,  dtype=torch.float32),\n",
    "                'labels':   torch.tensor(labels, dtype=torch.int64),\n",
    "                'image_id': example['image_id'],\n",
    "            }\n",
    "            yield img_tensor, target\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frcnn_collate_fn(batch):\n",
    "    \"\"\"Stack images; keep targets as a list (variable box count per image).\"\"\"\n",
    "    images  = torch.stack([b[0] for b in batch])\n",
    "    targets = [b[1] for b in batch]\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnchorTargetGenerator:\n",
    "    \"\"\"Assign GT labels and regression targets to pre-computed anchors.\n",
    "\n",
    "    For each image in a batch:\n",
    "    - Positive anchor: IoU with any GT >= 0.7 (or highest-IoU anchor per GT)\n",
    "    - Negative anchor: IoU with all GTs < 0.3\n",
    "    - Neutral anchor: everything else (ignored during loss)\n",
    "\n",
    "    Samples 256 anchors per image at 1:1 pos/neg ratio.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pos_iou: float = 0.7, neg_iou: float = 0.3,\n",
    "                 total_samples: int = 256, pos_fraction: float = 0.5):\n",
    "        self.pos_iou = pos_iou\n",
    "        self.neg_iou = neg_iou\n",
    "        self.total_samples = total_samples\n",
    "        self.n_pos = int(total_samples * pos_fraction)\n",
    "\n",
    "    def compute_iou(self, anchors: torch.Tensor, gt_boxes: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute IoU matrix: (N_anchors, N_gt).\"\"\"\n",
    "        ax1, ay1, ax2, ay2 = anchors.unbind(1)\n",
    "        gx1, gy1, gx2, gy2 = gt_boxes.unbind(1)\n",
    "\n",
    "        inter_x1 = torch.max(ax1[:, None], gx1[None, :])\n",
    "        inter_y1 = torch.max(ay1[:, None], gy1[None, :])\n",
    "        inter_x2 = torch.min(ax2[:, None], gx2[None, :])\n",
    "        inter_y2 = torch.min(ay2[:, None], gy2[None, :])\n",
    "\n",
    "        inter_w = (inter_x2 - inter_x1).clamp(min=0)\n",
    "        inter_h = (inter_y2 - inter_y1).clamp(min=0)\n",
    "        inter = inter_w * inter_h\n",
    "\n",
    "        area_a = (ax2 - ax1) * (ay2 - ay1)\n",
    "        area_g = (gx2 - gx1) * (gy2 - gy1)\n",
    "        union = area_a[:, None] + area_g[None, :] - inter\n",
    "\n",
    "        return inter / union.clamp(min=1e-6)\n",
    "\n",
    "    def __call__(self, anchors: torch.Tensor, gt_boxes: torch.Tensor\n",
    "                 ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            labels: (N_anchors,) — 1=positive, 0=negative, -1=neutral/ignore\n",
    "            matched_gt: (N_anchors, 4) — GT box matched to each anchor\n",
    "        \"\"\"\n",
    "        N = len(anchors)\n",
    "        labels = torch.full((N,), -1, dtype=torch.int64)\n",
    "        matched_gt = torch.zeros((N, 4), dtype=torch.float32)\n",
    "\n",
    "        if len(gt_boxes) == 0:\n",
    "            neg_idx = torch.randperm(N)[:self.total_samples]\n",
    "            labels[neg_idx] = 0\n",
    "            return labels, matched_gt\n",
    "\n",
    "        iou = self.compute_iou(anchors, gt_boxes)  # (N, M)\n",
    "\n",
    "        max_iou_per_anchor, best_gt_idx = iou.max(dim=1)\n",
    "        _, best_anchor_per_gt = iou.max(dim=0)\n",
    "\n",
    "        labels[max_iou_per_anchor >= self.pos_iou] = 1\n",
    "        labels[max_iou_per_anchor < self.neg_iou] = 0\n",
    "        labels[best_anchor_per_gt] = 1  # force-positive best anchors\n",
    "\n",
    "        pos_idx = torch.where(labels == 1)[0]\n",
    "        neg_idx = torch.where(labels == 0)[0]\n",
    "\n",
    "        n_pos = min(len(pos_idx), self.n_pos)\n",
    "        n_neg = min(len(neg_idx), self.total_samples - n_pos)\n",
    "\n",
    "        pos_idx = pos_idx[torch.randperm(len(pos_idx))[:n_pos]]\n",
    "        neg_idx = neg_idx[torch.randperm(len(neg_idx))[:n_neg]]\n",
    "\n",
    "        keep = torch.zeros(N, dtype=torch.bool)\n",
    "        keep[pos_idx] = True\n",
    "        keep[neg_idx] = True\n",
    "        labels[~keep] = -1\n",
    "\n",
    "        matched_gt = gt_boxes[best_gt_idx]\n",
    "        return labels, matched_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream a small batch for inspection (no local data needed)\n",
    "dataset = COCOStreamDataset(split='train', max_samples=64, img_size=IMG_SIZE)\n",
    "loader  = DataLoader(dataset, batch_size=2, collate_fn=frcnn_collate_fn,\n",
    "                     num_workers=0)\n",
    "\n",
    "imgs, targets = next(iter(loader))\n",
    "print(f\"Image batch: {imgs.shape}\")                # [2, 3, 800, 800]\n",
    "print(f\"Boxes[0]:   {targets[0]['boxes'].shape}\")  # [N, 4]\n",
    "print(f\"Labels[0]:  {targets[0]['labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspection: visualize 2 images with GT boxes\n",
    "cat_names = {i + 1: name for i, name in enumerate(COCO_NAMES)}  # 1-indexed\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 9))\n",
    "for i, ax in enumerate(axes):\n",
    "    img = imgs[i].permute(1, 2, 0).numpy()\n",
    "    img = img * STD + MEAN      # denormalize\n",
    "    img = img.clip(0, 1)\n",
    "    ax.imshow(img)\n",
    "    for box, lbl in zip(targets[i]['boxes'], targets[i]['labels']):\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                   linewidth=2, edgecolor='lime', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        name = cat_names.get(lbl.item(), str(lbl.item()))\n",
    "        ax.text(x1, y1 - 4, name, color='white', fontsize=7,\n",
    "                bbox=dict(facecolor='green', alpha=0.6, pad=1))\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Sample {i} — {len(targets[i]['boxes'])} objects\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/gt_visualization.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspection: anchor label distribution on first image in batch\n",
    "atg = AnchorTargetGenerator()\n",
    "dummy_anchors = torch.rand(200, 4) * IMG_SIZE\n",
    "dummy_anchors[:, 2:] = dummy_anchors[:, :2] + torch.rand(200, 2) * 200\n",
    "anchor_labels, _ = atg(dummy_anchors, targets[0]['boxes'])\n",
    "\n",
    "pos = (anchor_labels == 1).sum().item()\n",
    "neg = (anchor_labels == 0).sum().item()\n",
    "neu = (anchor_labels == -1).sum().item()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.bar(['positive', 'negative', 'neutral'], [pos, neg, neu],\n",
    "       color=['green', 'red', 'gray'])\n",
    "ax.set_title('Anchor sampling (200 dummy anchors, sample image)')\n",
    "ax.set_ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/anchor_stats.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Pos: {pos}, Neg: {neg}, Neutral: {neu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspection: class distribution across 200 streamed samples\n",
    "sample_ds = COCOStreamDataset(split='train', max_samples=200, img_size=IMG_SIZE)\n",
    "all_labels = []\n",
    "for _, t in sample_ds:\n",
    "    all_labels.extend(t['labels'].tolist())\n",
    "\n",
    "counter = Counter(all_labels)\n",
    "top20 = sorted(counter.items(), key=lambda x: -x[1])[:20]\n",
    "names  = [cat_names.get(k, str(k)) for k, _ in top20]\n",
    "counts = [v for _, v in top20]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "ax.bar(names, counts)\n",
    "ax.set_title('Top-20 categories by annotation count (200 COCO train samples)')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/class_distribution.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
