{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Region Proposal Network (RPN)\n",
    "\n",
    "*Notebook 3 of 6 in the Faster RCNN from-scratch series*\n",
    "\n",
    "The RPN is the key innovation of Faster RCNN: a small network that slides over\n",
    "FPN feature maps and proposes object-containing regions using anchor boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnchorGenerator(nn.Module):\n",
    "    \"\"\"Generate anchor boxes for each FPN level.\n",
    "\n",
    "    Produces anchors at the following scales x aspect ratios per level:\n",
    "        scales: (32, 64, 128, 256, 512) — one scale per FPN level P2-P6\n",
    "        ratios: (0.5, 1.0, 2.0)\n",
    "    k = 3 anchors per location.\n",
    "\n",
    "    Output: (N_total, 4) tensor of [x1, y1, x2, y2] anchors in image pixel coords.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 anchor_sizes:  Tuple[int, ...]   = (32, 64, 128, 256, 512),\n",
    "                 aspect_ratios: Tuple[float, ...] = (0.5, 1.0, 2.0),\n",
    "                 strides:       Tuple[int, ...]   = (4, 8, 16, 32, 64)):\n",
    "        super().__init__()\n",
    "        self.anchor_sizes  = anchor_sizes\n",
    "        self.aspect_ratios = aspect_ratios\n",
    "        self.strides       = strides\n",
    "\n",
    "    def _base_anchors(self, size: int) -> torch.Tensor:\n",
    "        \"\"\"Create k=3 base anchors centred at origin for a given scale.\"\"\"\n",
    "        anchors = []\n",
    "        for ratio in self.aspect_ratios:\n",
    "            w = size * (ratio ** 0.5)\n",
    "            h = size / (ratio ** 0.5)\n",
    "            anchors.append([-w / 2, -h / 2, w / 2, h / 2])\n",
    "        return torch.tensor(anchors, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, feature_maps: List[torch.Tensor],\n",
    "                image_size: Tuple[int, int]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature_maps: list of (B, C, H_i, W_i) tensors (P2-P6)\n",
    "            image_size:   (H, W) of the original image\n",
    "        Returns:\n",
    "            all_anchors: (N_total, 4) [x1, y1, x2, y2] in image pixel coords\n",
    "        \"\"\"\n",
    "        all_anchors = []\n",
    "        for fm, size, stride in zip(feature_maps, self.anchor_sizes, self.strides):\n",
    "            _, _, fh, fw = fm.shape\n",
    "            base = self._base_anchors(size)          # (3, 4)\n",
    "\n",
    "            shift_x = (torch.arange(fw) + 0.5) * stride\n",
    "            shift_y = (torch.arange(fh) + 0.5) * stride\n",
    "            sy, sx  = torch.meshgrid(shift_y, shift_x, indexing='ij')\n",
    "            shifts  = torch.stack([sx, sy, sx, sy], dim=-1).reshape(-1, 4)  # (H*W, 4)\n",
    "\n",
    "            anchors = shifts[:, None, :] + base[None, :, :]  # (H*W, 3, 4)\n",
    "            all_anchors.append(anchors.reshape(-1, 4))\n",
    "\n",
    "        return torch.cat(all_anchors, dim=0)\n",
    "\n",
    "\n",
    "# Shape verification\n",
    "gen = AnchorGenerator()\n",
    "fps = [\n",
    "    torch.zeros(1, 256, 200, 200),   # P2 stride 4\n",
    "    torch.zeros(1, 256, 100, 100),   # P3 stride 8\n",
    "    torch.zeros(1, 256,  50,  50),   # P4 stride 16\n",
    "    torch.zeros(1, 256,  25,  25),   # P5 stride 32\n",
    "    torch.zeros(1, 256,  13,  13),   # P6 stride 64\n",
    "]\n",
    "anchors = gen(fps, (800, 800))\n",
    "print(f\"Total anchors: {len(anchors):,}\")\n",
    "# P2: 200*200*3=120,000 | P3: 30,000 | P4: 7,500 | P5: 1,875 | P6: 507\n",
    "# Expected: 159,882"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPNHead(nn.Module):\n",
    "    \"\"\"Shared 3x3 conv followed by two sibling 1x1 outputs.\n",
    "\n",
    "    Applied independently to each FPN level.\n",
    "    Outputs: objectness logits and bbox deltas.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int = 256, num_anchors: int = 3):\n",
    "        super().__init__()\n",
    "        self.conv       = nn.Conv2d(in_channels, in_channels, 3, padding=1)\n",
    "        self.cls_logits = nn.Conv2d(in_channels, num_anchors,     1)\n",
    "        self.bbox_pred  = nn.Conv2d(in_channels, num_anchors * 4, 1)\n",
    "\n",
    "        for layer in [self.conv, self.cls_logits, self.bbox_pred]:\n",
    "            nn.init.normal_(layer.weight, std=0.01)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: list of (B, 256, H_i, W_i) tensors\n",
    "        Returns:\n",
    "            cls_logits: list of (B, k, H_i, W_i) per level\n",
    "            bbox_preds: list of (B, k*4, H_i, W_i) per level\n",
    "        \"\"\"\n",
    "        cls_logits, bbox_preds = [], []\n",
    "        for feat in features:\n",
    "            t = F.relu(self.conv(feat))\n",
    "            cls_logits.append(self.cls_logits(t))\n",
    "            bbox_preds.append(self.bbox_pred(t))\n",
    "        return cls_logits, bbox_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_boxes(anchors: torch.Tensor, deltas: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Apply predicted deltas to anchors to get proposal boxes (RCNN encoding).\"\"\"\n",
    "    aw = anchors[:, 2] - anchors[:, 0]\n",
    "    ah = anchors[:, 3] - anchors[:, 1]\n",
    "    ax = anchors[:, 0] + 0.5 * aw\n",
    "    ay = anchors[:, 1] + 0.5 * ah\n",
    "\n",
    "    dx, dy, dw, dh = deltas[:, 0], deltas[:, 1], deltas[:, 2], deltas[:, 3]\n",
    "    dw = dw.clamp(max=4.0)\n",
    "    dh = dh.clamp(max=4.0)\n",
    "\n",
    "    px = dx * aw + ax\n",
    "    py = dy * ah + ay\n",
    "    pw = torch.exp(dw) * aw\n",
    "    ph = torch.exp(dh) * ah\n",
    "\n",
    "    return torch.stack([px - 0.5 * pw, py - 0.5 * ph,\n",
    "                        px + 0.5 * pw, py + 0.5 * ph], dim=1)\n",
    "\n",
    "\n",
    "class RegionProposalNetwork(nn.Module):\n",
    "    \"\"\"Full RPN: generates, scores, and filters proposals.\"\"\"\n",
    "\n",
    "    def __init__(self, rpn_head: RPNHead, anchor_gen: AnchorGenerator,\n",
    "                 pre_nms_top_n: int = 2000, post_nms_top_n: int = 1000,\n",
    "                 nms_thresh: float = 0.7, min_size: int = 16):\n",
    "        super().__init__()\n",
    "        self.head          = rpn_head\n",
    "        self.anchor_gen    = anchor_gen\n",
    "        self.pre_nms_top_n = pre_nms_top_n\n",
    "        self.post_nms_top_n = post_nms_top_n\n",
    "        self.nms_thresh    = nms_thresh\n",
    "        self.min_size      = min_size\n",
    "\n",
    "    def _filter_proposals(self, proposals: torch.Tensor, scores: torch.Tensor,\n",
    "                          img_size: Tuple[int, int]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        H, W = img_size\n",
    "        proposals[:, [0, 2]] = proposals[:, [0, 2]].clamp(0, W)\n",
    "        proposals[:, [1, 3]] = proposals[:, [1, 3]].clamp(0, H)\n",
    "\n",
    "        ws = proposals[:, 2] - proposals[:, 0]\n",
    "        hs = proposals[:, 3] - proposals[:, 1]\n",
    "        keep = (ws >= self.min_size) & (hs >= self.min_size)\n",
    "        proposals, scores = proposals[keep], scores[keep]\n",
    "\n",
    "        k = min(self.pre_nms_top_n, len(scores))\n",
    "        scores, order = scores.topk(k)\n",
    "        proposals = proposals[order]\n",
    "\n",
    "        keep = self._nms(proposals, scores, self.nms_thresh)\n",
    "        keep = keep[:self.post_nms_top_n]\n",
    "        return proposals[keep], scores[keep]\n",
    "\n",
    "    @staticmethod\n",
    "    def _nms(boxes: torch.Tensor, scores: torch.Tensor,\n",
    "             thresh: float) -> torch.Tensor:\n",
    "        \"\"\"Greedy NMS — pure PyTorch, no torchvision dependency.\"\"\"\n",
    "        x1, y1, x2, y2 = boxes.unbind(1)\n",
    "        areas = (x2 - x1) * (y2 - y1)\n",
    "        order = scores.argsort(descending=True)\n",
    "        keep  = []\n",
    "        while order.numel() > 0:\n",
    "            i = order[0].item()\n",
    "            keep.append(i)\n",
    "            if order.numel() == 1:\n",
    "                break\n",
    "            xx1 = x1[order[1:]].clamp(min=x1[i])\n",
    "            yy1 = y1[order[1:]].clamp(min=y1[i])\n",
    "            xx2 = x2[order[1:]].clamp(max=x2[i])\n",
    "            yy2 = y2[order[1:]].clamp(max=y2[i])\n",
    "            inter = (xx2 - xx1).clamp(min=0) * (yy2 - yy1).clamp(min=0)\n",
    "            iou   = inter / (areas[i] + areas[order[1:]] - inter).clamp(min=1e-6)\n",
    "            order = order[1:][iou <= thresh]\n",
    "        return torch.tensor(keep, dtype=torch.long)\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor],\n",
    "                image_size: Tuple[int, int],\n",
    "                targets=None) -> List[torch.Tensor]:\n",
    "        cls_logits, bbox_preds = self.head(features)\n",
    "        anchors = self.anchor_gen(features, image_size)\n",
    "\n",
    "        # Flatten across levels: (B, N_total)\n",
    "        all_scores = torch.cat([\n",
    "            c.permute(0, 2, 3, 1).reshape(c.shape[0], -1)\n",
    "            for c in cls_logits\n",
    "        ], dim=1)\n",
    "        all_deltas = torch.cat([\n",
    "            b.permute(0, 2, 3, 1).reshape(b.shape[0], -1, 4)\n",
    "            for b in bbox_preds\n",
    "        ], dim=1)\n",
    "\n",
    "        proposals_list = []\n",
    "        for i in range(all_scores.shape[0]):\n",
    "            scores_i = all_scores[i].sigmoid()\n",
    "            props_i  = decode_boxes(anchors, all_deltas[i])\n",
    "            props_i, _ = self._filter_proposals(props_i, scores_i, image_size)\n",
    "            proposals_list.append(props_i)\n",
    "\n",
    "        return proposals_list\n",
    "\n",
    "\n",
    "# Smoke test\n",
    "head = RPNHead()\n",
    "rpn  = RegionProposalNetwork(head, gen)\n",
    "feat_maps = [\n",
    "    torch.randn(1, 256, 200, 200),\n",
    "    torch.randn(1, 256, 100, 100),\n",
    "    torch.randn(1, 256,  50,  50),\n",
    "    torch.randn(1, 256,  25,  25),\n",
    "    torch.randn(1, 256,  13,  13),\n",
    "]\n",
    "proposals = rpn(feat_maps, (800, 800))\n",
    "print(f\"Proposals per image: {[len(p) for p in proposals]}\")\n",
    "# Expected: [~1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspection: anchor grid at P3 (stride 8) — sample every 8th cell\n",
    "stride, fh, fw = 8, 100, 100\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.set_xlim(0, 800); ax.set_ylim(800, 0)\n",
    "ax.set_facecolor('#1a1a2e')\n",
    "ax.set_title('Anchor centres at P3 (stride 8), every 8th cell')\n",
    "for r in range(0, fh, 8):\n",
    "    for c in range(0, fw, 8):\n",
    "        ax.plot((c + 0.5) * stride, (r + 0.5) * stride, 'c.', markersize=2)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/anchor_grid.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"P3 total locations: {fh * fw:,}  |  total anchors: {fh * fw * 3:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspection: objectness score distribution (random weights)\n",
    "# Re-run head on feat_maps to get scores\n",
    "cls_logits_list, bbox_preds_list = head(feat_maps)\n",
    "all_scores_flat = torch.cat([\n",
    "    c.permute(0, 2, 3, 1).reshape(-1) for c in cls_logits_list\n",
    "]).sigmoid().detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.hist(all_scores_flat, bins=80, color='steelblue', alpha=0.8, edgecolor='none')\n",
    "ax.set_xlabel('Objectness score'); ax.set_ylabel('Anchor count')\n",
    "ax.set_title('Objectness score distribution (random-weight RPN)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/objectness_dist.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspection: top-50 proposals before NMS on a blank canvas\n",
    "anchors_all = gen(feat_maps, (800, 800))\n",
    "all_deltas_flat = torch.cat([\n",
    "    b.permute(0, 2, 3, 1).reshape(-1, 4) for b in bbox_preds_list\n",
    "]).detach()\n",
    "all_scores_1d = torch.tensor(all_scores_flat)\n",
    "\n",
    "top50_idx  = all_scores_1d.argsort(descending=True)[:50]\n",
    "top50_props = decode_boxes(anchors_all[top50_idx], all_deltas_flat[top50_idx])\n",
    "top50_props = top50_props.clamp(0, 800)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.set_xlim(0, 800); ax.set_ylim(800, 0)\n",
    "ax.set_facecolor('#1a1a2e')\n",
    "ax.set_title('Top-50 proposals (before NMS, random weights)')\n",
    "for box in top50_props.tolist():\n",
    "    x1, y1, x2, y2 = box\n",
    "    rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                               linewidth=1, edgecolor='cyan', facecolor='none', alpha=0.5)\n",
    "    ax.add_patch(rect)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/top50_proposals.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
