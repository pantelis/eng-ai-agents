{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROI Head: ROI Align + Classification and Box Regression\n",
    "\n",
    "*Notebook 4 of 6 in the Faster RCNN from-scratch series*\n",
    "\n",
    "Given proposals from the RPN, we extract fixed-size features via ROI Align,\n",
    "then classify each proposal and refine its bounding box.\n",
    "\n",
    "**Mask RCNN extension point**: this notebook also demonstrates the 14×14\n",
    "ROI Align variant used by the mask head (notebook 07)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROIAlign(nn.Module):\n",
    "    \"\"\"ROI Align using bilinear interpolation via F.grid_sample.\n",
    "\n",
    "    Extracts a fixed (out_size x out_size) feature crop for each proposal,\n",
    "    selecting the FPN level based on box area (Lin et al. FPN level assignment):\n",
    "\n",
    "        k = clip(k0 + floor(log2(sqrt(wh) / 224)), k_min, k_max)\n",
    "        k0=4, k_min=2 (P2), k_max=5 (P5)  ->  index 0..3\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, out_size: int = 7,\n",
    "                 k0: int = 4, k_min: int = 2, k_max: int = 5):\n",
    "        super().__init__()\n",
    "        self.out_size = out_size\n",
    "        self.k0    = k0\n",
    "        self.k_min = k_min\n",
    "        self.k_max = k_max\n",
    "\n",
    "    def _assign_level(self, boxes: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Return 0-indexed FPN level (0=P2, 1=P3, 2=P4, 3=P5) per box.\"\"\"\n",
    "        ws = boxes[:, 2] - boxes[:, 0]\n",
    "        hs = boxes[:, 3] - boxes[:, 1]\n",
    "        areas = (ws * hs).clamp(min=1e-6).sqrt()\n",
    "        levels = torch.floor(self.k0 + torch.log2(areas / 224.0)).long()\n",
    "        return levels.clamp(self.k_min, self.k_max) - self.k_min\n",
    "\n",
    "    def forward(self, feature_maps: List[torch.Tensor],\n",
    "                proposals: List[torch.Tensor],\n",
    "                image_size: Tuple[int, int]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature_maps: [P2, P3, P4, P5] — (B, 256, H_i, W_i) each\n",
    "            proposals:    list of (N_i, 4) per image, pixel coords\n",
    "            image_size:   (H, W)\n",
    "        Returns:\n",
    "            roi_features: (sum(N_i), 256, out_size, out_size)\n",
    "        \"\"\"\n",
    "        H, W = image_size\n",
    "        strides = [4, 8, 16, 32]\n",
    "        all_features = []\n",
    "\n",
    "        for batch_idx, props in enumerate(proposals):\n",
    "            if len(props) == 0:\n",
    "                continue\n",
    "            levels = self._assign_level(props)\n",
    "            feats  = torch.zeros(len(props), feature_maps[0].shape[1],\n",
    "                                 self.out_size, self.out_size,\n",
    "                                 device=props.device)\n",
    "\n",
    "            for lvl, (fm, stride) in enumerate(zip(feature_maps, strides)):\n",
    "                mask = levels == lvl\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "                lvl_props = props[mask]\n",
    "                n = len(lvl_props)\n",
    "\n",
    "                # Normalise box coords to [-1, 1] for grid_sample\n",
    "                x1 = lvl_props[:, 0] / W * 2 - 1\n",
    "                y1 = lvl_props[:, 1] / H * 2 - 1\n",
    "                x2 = lvl_props[:, 2] / W * 2 - 1\n",
    "                y2 = lvl_props[:, 3] / H * 2 - 1\n",
    "\n",
    "                gx = torch.linspace(0, 1, self.out_size, device=props.device)\n",
    "                gy = torch.linspace(0, 1, self.out_size, device=props.device)\n",
    "                gy_g, gx_g = torch.meshgrid(gy, gx, indexing='ij')\n",
    "\n",
    "                gx_g = x1[:, None, None] + (x2 - x1)[:, None, None] * gx_g[None]\n",
    "                gy_g = y1[:, None, None] + (y2 - y1)[:, None, None] * gy_g[None]\n",
    "                grid = torch.stack([gx_g, gy_g], dim=-1)\n",
    "\n",
    "                fm_exp = fm[batch_idx:batch_idx + 1].expand(n, -1, -1, -1)\n",
    "                crops  = F.grid_sample(fm_exp, grid, align_corners=True,\n",
    "                                       mode='bilinear', padding_mode='border')\n",
    "                feats[mask] = crops\n",
    "\n",
    "            all_features.append(feats)\n",
    "\n",
    "        return torch.cat(all_features, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoMLPHead(nn.Module):\n",
    "    \"\"\"Two fully-connected layers applied after ROI Align.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int = 256 * 7 * 7, fc_dim: int = 1024):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_channels, fc_dim)\n",
    "        self.fc2 = nn.Linear(fc_dim, fc_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.flatten(1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class FastRCNNPredictor(nn.Module):\n",
    "    \"\"\"Sibling FC heads: class scores and per-class box deltas.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int = 1024, num_classes: int = 81):\n",
    "        super().__init__()\n",
    "        self.cls_score = nn.Linear(in_channels, num_classes)\n",
    "        self.bbox_pred = nn.Linear(in_channels, num_classes * 4)\n",
    "        nn.init.normal_(self.cls_score.weight, std=0.01)\n",
    "        nn.init.normal_(self.bbox_pred.weight, std=0.001)\n",
    "        nn.init.zeros_(self.cls_score.bias)\n",
    "        nn.init.zeros_(self.bbox_pred.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.cls_score(x), self.bbox_pred(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoke test with dummy feature maps and proposals\n",
    "roi_align = ROIAlign(out_size=7)\n",
    "mlp_head  = TwoMLPHead()\n",
    "predictor = FastRCNNPredictor()\n",
    "\n",
    "feat_maps = [\n",
    "    torch.randn(1, 256, 200, 200),   # P2\n",
    "    torch.randn(1, 256, 100, 100),   # P3\n",
    "    torch.randn(1, 256,  50,  50),   # P4\n",
    "    torch.randn(1, 256,  25,  25),   # P5\n",
    "]\n",
    "proposals = [torch.tensor([\n",
    "    [ 50.,  50., 300., 300.],\n",
    "    [100., 100., 400., 400.],\n",
    "    [200., 200., 600., 600.],\n",
    "])]\n",
    "\n",
    "roi_feats  = roi_align(feat_maps, proposals, (800, 800))\n",
    "box_feats  = mlp_head(roi_feats)\n",
    "cls_logits, bbox_preds = predictor(box_feats)\n",
    "\n",
    "print(f\"ROI features: {roi_feats.shape}\")   # [3, 256, 7, 7]\n",
    "print(f\"Box features: {box_feats.shape}\")   # [3, 1024]\n",
    "print(f\"Class logits: {cls_logits.shape}\")  # [3, 81]\n",
    "print(f\"Box preds:    {bbox_preds.shape}\")  # [3, 324]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspection: mean-channel activation of 7x7 ROI crops\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    crop = roi_feats[i].mean(dim=0).detach().numpy()\n",
    "    im = ax.imshow(crop, cmap='viridis')\n",
    "    ax.set_title(f'ROI {i} — 7×7 (mean over 256 ch)')\n",
    "    ax.axis('off')\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "plt.suptitle('ROI Align 7×7 Crops (dummy feature maps)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/roi_crops.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask RCNN extension point: 14x14 ROI Align\n",
    "mask_roi_align = ROIAlign(out_size=14)\n",
    "mask_roi_feats = mask_roi_align(feat_maps, proposals, (800, 800))\n",
    "print(f\"Mask ROI features (14x14): {mask_roi_feats.shape}\")\n",
    "# Expected: [3, 256, 14, 14]\n",
    "print(\"Extension point ready for Mask RCNN mask head (notebook 07).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
