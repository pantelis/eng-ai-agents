{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a2e4c11",
   "metadata": {},
   "source": [
    "# Training Faster RCNN End-to-End\n",
    "\n",
    "*Notebook 5 of 6 in the Faster RCNN from-scratch series*\n",
    "\n",
    "This notebook assembles all components (backbone + FPN, RPN, ROI head) into a\n",
    "single `FasterRCNN` module and trains it on COCO data streamed from Hugging Face.\n",
    "\n",
    "**Scope**: a short training demo (5 gradient steps) that verifies the full\n",
    "forward + backward pass and saves a checkpoint for notebook 06.\n",
    "\n",
    "**Memory notes**: we use:\n",
    "- 600 × 600 input resolution (vs the canonical 800 × 800) to fit in ~16 GB VRAM\n",
    "- PyTorch AMP (automatic mixed precision) — forward in float16, gradients in float32\n",
    "- Frozen backbone stem + layer1/2 (only layer3, layer4, FPN, RPN, ROI head are trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2a0c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms.functional as TF\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom datasets import load_dataset\nfrom torch.utils.data import IterableDataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom typing import List, Tuple, Dict, Optional\nimport os, math\n\nIMG_SIZE    = 400   # 400x400 — fits in 16 GB VRAM with AMP + frozen backbone\nNUM_CLASSES = 81    # 80 COCO + background\nDEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {DEVICE}\")\nif DEVICE.type == 'cuda':\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM total: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9c5b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1. Data pipeline ─────────────────────────────────────────────────────────\n",
    "\n",
    "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "\n",
    "class COCOStreamDataset(IterableDataset):\n",
    "    \"\"\"Stream COCO from HuggingFace and resize to IMG_SIZE × IMG_SIZE.\"\"\"\n",
    "\n",
    "    def __init__(self, split: str = 'train', max_samples: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.ds          = load_dataset('detection-datasets/coco', split=split,\n",
    "                                        streaming=True)\n",
    "        self.max_samples = max_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        for sample in self.ds:\n",
    "            if self.max_samples and count >= self.max_samples:\n",
    "                break\n",
    "            img = sample['image'].convert('RGB')\n",
    "            W0, H0 = img.size\n",
    "            img = img.resize((IMG_SIZE, IMG_SIZE))\n",
    "            t   = TF.to_tensor(img)\n",
    "            t   = (t - IMAGENET_MEAN) / IMAGENET_STD\n",
    "\n",
    "            sx, sy = IMG_SIZE / W0, IMG_SIZE / H0\n",
    "            boxes, labels = [], []\n",
    "            for ann, cat in zip(sample['objects']['bbox'],\n",
    "                                sample['objects']['category']):\n",
    "                x, y, w, h = ann\n",
    "                x1, y1 = x * sx, y * sy\n",
    "                x2, y2 = (x + w) * sx, (y + h) * sy\n",
    "                if x2 > x1 and y2 > y1:\n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "                    labels.append(int(cat) + 1)  # 0 = background\n",
    "\n",
    "            if not boxes:\n",
    "                continue\n",
    "\n",
    "            count += 1\n",
    "            yield t, {'boxes':  torch.tensor(boxes,  dtype=torch.float32),\n",
    "                      'labels': torch.tensor(labels, dtype=torch.long)}\n",
    "\n",
    "\n",
    "def frcnn_collate_fn(batch):\n",
    "    return torch.stack([b[0] for b in batch]), [b[1] for b in batch]\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "ds  = COCOStreamDataset(split='train', max_samples=2)\n",
    "imgs, tgts = frcnn_collate_fn(list(ds))\n",
    "print(f\"Batch images : {imgs.shape}\")\n",
    "print(f\"GT boxes     : {[t['boxes'].shape for t in tgts]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c2e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.checkpoint import checkpoint as grad_ckpt\n\n# ─── 2. Backbone: ResNet50 + FPN ──────────────────────────────────────────────\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_ch, out_ch, stride=1, downsample=None):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n        self.bn1   = nn.BatchNorm2d(out_ch)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n        self.bn2   = nn.BatchNorm2d(out_ch)\n        self.conv3 = nn.Conv2d(out_ch, out_ch * 4, 1, bias=False)\n        self.bn3   = nn.BatchNorm2d(out_ch * 4)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        if self.downsample:\n            identity = self.downsample(x)\n        return F.relu(out + identity)\n\n\nclass ResNet50(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.stem   = nn.Sequential(\n            nn.Conv2d(3, 64, 7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n            nn.MaxPool2d(3, stride=2, padding=1))\n        self.layer1 = self._make_layer(  64,  64, 3, stride=1)\n        self.layer2 = self._make_layer( 256, 128, 4, stride=2)\n        self.layer3 = self._make_layer( 512, 256, 6, stride=2)\n        self.layer4 = self._make_layer(1024, 512, 3, stride=2)\n\n    def _make_layer(self, in_ch, out_ch, blocks, stride):\n        ds = None\n        if stride != 1 or in_ch != out_ch * 4:\n            ds = nn.Sequential(\n                nn.Conv2d(in_ch, out_ch * 4, 1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_ch * 4))\n        layers = [Bottleneck(in_ch, out_ch, stride, ds)]\n        for _ in range(1, blocks):\n            layers.append(Bottleneck(out_ch * 4, out_ch))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x  = self.stem(x)\n        c2 = self.layer1(x)\n        c3 = self.layer2(c2)\n        c4 = grad_ckpt(self.layer3, c3, use_reentrant=False)\n        c5 = grad_ckpt(self.layer4, c4, use_reentrant=False)\n        return c2, c3, c4, c5\n\n\nclass FPN(nn.Module):\n    def __init__(self, in_channels=(256, 512, 1024, 2048), out_channels=256):\n        super().__init__()\n        self.lateral = nn.ModuleList([nn.Conv2d(c, out_channels, 1) for c in in_channels])\n        self.output  = nn.ModuleList([nn.Conv2d(out_channels, out_channels, 3, padding=1)\n                                      for _ in in_channels])\n        self.p6      = nn.MaxPool2d(1, stride=2)\n\n    def forward(self, features):\n        c2, c3, c4, c5 = features\n        p5 = self.lateral[3](c5)\n        p4 = self.lateral[2](c4) + F.interpolate(p5, size=c4.shape[-2:], mode='nearest')\n        p3 = self.lateral[1](c3) + F.interpolate(p4, size=c3.shape[-2:], mode='nearest')\n        p2 = self.lateral[0](c2) + F.interpolate(p3, size=c2.shape[-2:], mode='nearest')\n        outs = [self.output[i](p) for i, p in enumerate([p2, p3, p4, p5])]\n        outs.append(self.p6(outs[-1]))\n        return outs   # [P2, P3, P4, P5, P6]\n\n\nprint(\"Backbone + FPN defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a7c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 3. RPN ───────────────────────────────────────────────────────────────────\n",
    "\n",
    "def box_iou(boxes_a, boxes_b):\n",
    "    \"\"\"Compute pairwise IoU: (N,4) x (M,4) -> (N,M).\"\"\"\n",
    "    area_a = (boxes_a[:,2]-boxes_a[:,0]) * (boxes_a[:,3]-boxes_a[:,1])\n",
    "    area_b = (boxes_b[:,2]-boxes_b[:,0]) * (boxes_b[:,3]-boxes_b[:,1])\n",
    "    ix1 = torch.max(boxes_a[:,None,0], boxes_b[None,:,0])\n",
    "    iy1 = torch.max(boxes_a[:,None,1], boxes_b[None,:,1])\n",
    "    ix2 = torch.min(boxes_a[:,None,2], boxes_b[None,:,2])\n",
    "    iy2 = torch.min(boxes_a[:,None,3], boxes_b[None,:,3])\n",
    "    inter = (ix2-ix1).clamp(0) * (iy2-iy1).clamp(0)\n",
    "    return inter / (area_a[:,None] + area_b[None,:] - inter + 1e-6)\n",
    "\n",
    "\n",
    "def encode_boxes(proposals, gt_boxes):\n",
    "    pw = proposals[:,2]-proposals[:,0]; ph = proposals[:,3]-proposals[:,1]\n",
    "    px = proposals[:,0]+0.5*pw;        py = proposals[:,1]+0.5*ph\n",
    "    gw = gt_boxes[:,2]-gt_boxes[:,0];  gh = gt_boxes[:,3]-gt_boxes[:,1]\n",
    "    gx = gt_boxes[:,0]+0.5*gw;        gy = gt_boxes[:,1]+0.5*gh\n",
    "    return torch.stack([(gx-px)/pw, (gy-py)/ph,\n",
    "                         torch.log(gw/pw), torch.log(gh/ph)], dim=1)\n",
    "\n",
    "\n",
    "def decode_boxes(anchors, deltas):\n",
    "    aw = anchors[:,2]-anchors[:,0]; ah = anchors[:,3]-anchors[:,1]\n",
    "    ax = anchors[:,0]+0.5*aw;       ay = anchors[:,1]+0.5*ah\n",
    "    dx,dy,dw,dh = deltas[:,0], deltas[:,1], deltas[:,2].clamp(max=4.), deltas[:,3].clamp(max=4.)\n",
    "    px = dx*aw+ax;  py = dy*ah+ay\n",
    "    pw = torch.exp(dw)*aw; ph = torch.exp(dh)*ah\n",
    "    return torch.stack([px-0.5*pw, py-0.5*ph, px+0.5*pw, py+0.5*ph], dim=1)\n",
    "\n",
    "\n",
    "class AnchorGenerator(nn.Module):\n",
    "    def __init__(self, anchor_sizes=(32,64,128,256,512),\n",
    "                 aspect_ratios=(0.5,1.0,2.0), strides=(4,8,16,32,64)):\n",
    "        super().__init__()\n",
    "        self.anchor_sizes = anchor_sizes; self.aspect_ratios = aspect_ratios\n",
    "        self.strides = strides\n",
    "\n",
    "    def _base(self, size):\n",
    "        return torch.tensor([[-size*(r**.5)/2, -size/(r**.5)/2,\n",
    "                               size*(r**.5)/2,  size/(r**.5)/2]\n",
    "                              for r in self.aspect_ratios], dtype=torch.float32)\n",
    "\n",
    "    def forward(self, feature_maps, image_size):\n",
    "        all_anchors = []\n",
    "        for fm, sz, st in zip(feature_maps, self.anchor_sizes, self.strides):\n",
    "            _, _, fh, fw = fm.shape\n",
    "            base = self._base(sz)\n",
    "            sx = (torch.arange(fw, device=fm.device)+0.5)*st\n",
    "            sy = (torch.arange(fh, device=fm.device)+0.5)*st\n",
    "            sy, sx = torch.meshgrid(sy, sx, indexing='ij')\n",
    "            shifts = torch.stack([sx,sy,sx,sy], dim=-1).reshape(-1,4)\n",
    "            all_anchors.append((shifts[:,None,:]+base.to(fm.device)[None,:,:]).reshape(-1,4))\n",
    "        return torch.cat(all_anchors, dim=0)\n",
    "\n",
    "\n",
    "class RPNHead(nn.Module):\n",
    "    def __init__(self, in_ch=256, k=3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, in_ch, 3, padding=1)\n",
    "        self.cls  = nn.Conv2d(in_ch, k,   1)\n",
    "        self.box  = nn.Conv2d(in_ch, k*4, 1)\n",
    "        for l in [self.conv, self.cls, self.box]:\n",
    "            nn.init.normal_(l.weight, std=0.01); nn.init.zeros_(l.bias)\n",
    "\n",
    "    def forward(self, features):\n",
    "        cls_outs, box_outs = [], []\n",
    "        for f in features:\n",
    "            t = F.relu(self.conv(f))\n",
    "            cls_outs.append(self.cls(t)); box_outs.append(self.box(t))\n",
    "        return cls_outs, box_outs\n",
    "\n",
    "\n",
    "class RegionProposalNetwork(nn.Module):\n",
    "    RPN_BATCH=256; POS_FRAC=0.5; POS_THR=0.7; NEG_THR=0.3\n",
    "\n",
    "    def __init__(self, head, anchor_gen,\n",
    "                 pre_nms=2000, post_nms=1000, nms_thr=0.7, min_sz=16):\n",
    "        super().__init__()\n",
    "        self.head=head; self.anchor_gen=anchor_gen\n",
    "        self.pre_nms=pre_nms; self.post_nms=post_nms\n",
    "        self.nms_thr=nms_thr; self.min_sz=min_sz\n",
    "\n",
    "    def _filter(self, props, scores, img_size):\n",
    "        H,W=img_size\n",
    "        props[:,[0,2]]=props[:,[0,2]].clamp(0,W)\n",
    "        props[:,[1,3]]=props[:,[1,3]].clamp(0,H)\n",
    "        keep=(props[:,2]-props[:,0]>=self.min_sz)&(props[:,3]-props[:,1]>=self.min_sz)\n",
    "        props,scores=props[keep],scores[keep]\n",
    "        scores,order=scores.topk(min(self.pre_nms,len(scores)))\n",
    "        props=props[order]\n",
    "        keep=self._nms(props,scores,self.nms_thr)[:self.post_nms]\n",
    "        return props[keep], scores[keep]\n",
    "\n",
    "    @staticmethod\n",
    "    def _nms(boxes, scores, thr):\n",
    "        x1,y1,x2,y2=boxes.unbind(1)\n",
    "        areas=(x2-x1)*(y2-y1)\n",
    "        order=scores.argsort(descending=True)\n",
    "        keep=[]\n",
    "        while order.numel()>0:\n",
    "            i=order[0].item(); keep.append(i)\n",
    "            if order.numel()==1: break\n",
    "            xx1=x1[order[1:]].clamp(min=x1[i]); yy1=y1[order[1:]].clamp(min=y1[i])\n",
    "            xx2=x2[order[1:]].clamp(max=x2[i]); yy2=y2[order[1:]].clamp(max=y2[i])\n",
    "            inter=(xx2-xx1).clamp(0)*(yy2-yy1).clamp(0)\n",
    "            iou=inter/(areas[i]+areas[order[1:]]-inter).clamp(1e-6)\n",
    "            order=order[1:][iou<=thr]\n",
    "        return torch.tensor(keep, dtype=torch.long)\n",
    "\n",
    "    def _assign(self, anchors, gt_boxes):\n",
    "        if gt_boxes.numel()==0:\n",
    "            return torch.full((len(anchors),),-1,dtype=torch.long,device=anchors.device), \\\n",
    "                   torch.zeros_like(anchors)\n",
    "        iou=box_iou(anchors,gt_boxes); max_iou,gi=iou.max(1)\n",
    "        labels=torch.full((len(anchors),),-1,dtype=torch.long,device=anchors.device)\n",
    "        labels[max_iou>=self.POS_THR]=1; labels[max_iou<self.NEG_THR]=0\n",
    "        labels[iou.argmax(0)]=1\n",
    "        n_pos=int(self.RPN_BATCH*self.POS_FRAC)\n",
    "        for val,n in [(1,n_pos),(0,self.RPN_BATCH-n_pos)]:\n",
    "            idx=(labels==val).nonzero(as_tuple=False).squeeze(1)\n",
    "            if len(idx)>n: labels[idx[torch.randperm(len(idx))[n:]]]=-1\n",
    "        return labels, gt_boxes[gi]\n",
    "\n",
    "    def forward(self, features, image_size, targets=None):\n",
    "        cls_outs, box_outs = self.head(features)\n",
    "        anchors = self.anchor_gen(features, image_size)\n",
    "        all_scores = torch.cat([c.permute(0,2,3,1).reshape(c.shape[0],-1)\n",
    "                                 for c in cls_outs], 1)\n",
    "        all_deltas = torch.cat([b.permute(0,2,3,1).reshape(b.shape[0],-1,4)\n",
    "                                 for b in box_outs], 1)\n",
    "        props_list=[]\n",
    "        for i in range(all_scores.shape[0]):\n",
    "            sc=all_scores[i].sigmoid()\n",
    "            pr=decode_boxes(anchors, all_deltas[i])\n",
    "            pr,_=self._filter(pr.detach(), sc.detach(), image_size)\n",
    "            props_list.append(pr)\n",
    "        losses={}\n",
    "        if targets is not None and self.training:\n",
    "            B=all_scores.shape[0]; c_tot=b_tot=0.0\n",
    "            for i in range(B):\n",
    "                gt=targets[i]['boxes'].to(anchors.device)\n",
    "                lbl,mgt=self._assign(anchors,gt)\n",
    "                sam=lbl>=0\n",
    "                c_tot+=F.binary_cross_entropy_with_logits(all_scores[i][sam],lbl[sam].float())\n",
    "                pos=lbl==1\n",
    "                if pos.any():\n",
    "                    b_tot+=F.smooth_l1_loss(all_deltas[i][pos],encode_boxes(anchors[pos],mgt[pos]),beta=1./9)\n",
    "            losses={'rpn_cls':c_tot/B,'rpn_box':b_tot/B}\n",
    "        return props_list, losses\n",
    "\n",
    "\n",
    "print(\"RPN defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c3a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 4. ROI Head ──────────────────────────────────────────────────────────────\n",
    "\n",
    "class ROIAlign(nn.Module):\n",
    "    def __init__(self, out_size=7, k0=4, k_min=2, k_max=5):\n",
    "        super().__init__()\n",
    "        self.out_size=out_size; self.k0=k0; self.k_min=k_min; self.k_max=k_max\n",
    "\n",
    "    def _level(self, boxes):\n",
    "        areas=((boxes[:,2]-boxes[:,0])*(boxes[:,3]-boxes[:,1])).clamp(1e-6).sqrt()\n",
    "        return torch.floor(self.k0+torch.log2(areas/224.)).long().clamp(self.k_min,self.k_max)-self.k_min\n",
    "\n",
    "    def forward(self, fmaps, proposals, image_size):\n",
    "        H,W=image_size; all_feats=[]\n",
    "        for bi,props in enumerate(proposals):\n",
    "            if len(props)==0: continue\n",
    "            levels=self._level(props)\n",
    "            feats=torch.zeros(len(props),fmaps[0].shape[1],self.out_size,self.out_size,device=props.device)\n",
    "            for lvl,fm in enumerate(fmaps[:4]):\n",
    "                mask=levels==lvl\n",
    "                if not mask.any(): continue\n",
    "                lp=props[mask]; n=len(lp)\n",
    "                x1=lp[:,0]/W*2-1; y1=lp[:,1]/H*2-1\n",
    "                x2=lp[:,2]/W*2-1; y2=lp[:,3]/H*2-1\n",
    "                gx=torch.linspace(0,1,self.out_size,device=props.device)\n",
    "                gy=torch.linspace(0,1,self.out_size,device=props.device)\n",
    "                gy_g,gx_g=torch.meshgrid(gy,gx,indexing='ij')\n",
    "                gx_g=x1[:,None,None]+(x2-x1)[:,None,None]*gx_g[None]\n",
    "                gy_g=y1[:,None,None]+(y2-y1)[:,None,None]*gy_g[None]\n",
    "                grid=torch.stack([gx_g,gy_g],dim=-1)\n",
    "                crops=F.grid_sample(fm[bi:bi+1].expand(n,-1,-1,-1),grid,\n",
    "                                    align_corners=True,mode='bilinear',padding_mode='border')\n",
    "                feats[mask]=crops\n",
    "            all_feats.append(feats)\n",
    "        if not all_feats:\n",
    "            return torch.zeros(0,fmaps[0].shape[1],self.out_size,self.out_size,device=fmaps[0].device)\n",
    "        return torch.cat(all_feats,0)\n",
    "\n",
    "\n",
    "class TwoMLPHead(nn.Module):\n",
    "    def __init__(self, in_channels=256*7*7, fc_dim=1024):\n",
    "        super().__init__()\n",
    "        self.fc1=nn.Linear(in_channels,fc_dim); self.fc2=nn.Linear(fc_dim,fc_dim)\n",
    "    def forward(self,x):\n",
    "        return F.relu(self.fc2(F.relu(self.fc1(x.flatten(1)))))\n",
    "\n",
    "\n",
    "class FastRCNNPredictor(nn.Module):\n",
    "    def __init__(self, in_channels=1024, num_classes=81):\n",
    "        super().__init__()\n",
    "        self.cls=nn.Linear(in_channels,num_classes)\n",
    "        self.box=nn.Linear(in_channels,num_classes*4)\n",
    "        nn.init.normal_(self.cls.weight,std=0.01); nn.init.zeros_(self.cls.bias)\n",
    "        nn.init.normal_(self.box.weight,std=0.001); nn.init.zeros_(self.box.bias)\n",
    "    def forward(self,x): return self.cls(x), self.box(x)\n",
    "\n",
    "\n",
    "print(\"ROI head defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e9c3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 5. FasterRCNN module ─────────────────────────────────────────────────────\n\nclass FasterRCNN(nn.Module):\n    ROI_BATCH=512; ROI_POS_FRAC=0.25; ROI_POS_THR=0.5\n    SCORE_THR=0.05; NMS_THR=0.5; MAX_DETS=100\n\n    def __init__(self, num_classes=81):\n        super().__init__()\n        self.num_classes = num_classes\n        self.backbone    = ResNet50()\n        self.fpn         = FPN()\n        self.rpn         = RegionProposalNetwork(RPNHead(), AnchorGenerator())\n        self.roi_align   = ROIAlign(out_size=7)\n        self.box_head    = TwoMLPHead()\n        self.predictor   = FastRCNNPredictor(num_classes=num_classes)\n\n        # Freeze stem + layer1-3 to save VRAM; layer4+FPN+heads are trained\n        for p in list(self.backbone.stem.parameters()) + \\\n                 list(self.backbone.layer1.parameters()) + \\\n                 list(self.backbone.layer2.parameters()) + \\\n                 list(self.backbone.layer3.parameters()):\n            p.requires_grad_(False)\n\n    def _sample_rois(self, proposals, targets):\n        s_props, s_labels, s_gt = [], [], []\n        for props, tgt in zip(proposals, targets):\n            gt_boxes  = tgt['boxes']\n            gt_labels = tgt['labels']\n            all_props = torch.cat([props, gt_boxes]) if len(props) else gt_boxes\n            if len(gt_boxes) == 0:\n                n   = min(self.ROI_BATCH, len(all_props))\n                idx = torch.randperm(len(all_props))[:n]\n                s_props.append(all_props[idx])\n                s_labels.append(torch.zeros(n, dtype=torch.long, device=props.device))\n                s_gt.append(all_props[idx])\n                continue\n            iou = box_iou(all_props, gt_boxes)\n            max_iou, gi = iou.max(1)\n            labels = torch.zeros(len(all_props), dtype=torch.long, device=props.device)\n            pos = max_iou >= self.ROI_POS_THR\n            labels[pos] = gt_labels[gi[pos]]\n            n_pos = int(self.ROI_BATCH * self.ROI_POS_FRAC)\n            n_neg = self.ROI_BATCH - n_pos\n            pos_idx = pos.nonzero(as_tuple=False).squeeze(1)\n            neg_idx = (~pos).nonzero(as_tuple=False).squeeze(1)\n            pos_idx = pos_idx[torch.randperm(len(pos_idx))[:n_pos]]\n            neg_idx = neg_idx[torch.randperm(len(neg_idx))[:n_neg]]\n            sel = torch.cat([pos_idx, neg_idx])\n            s_props.append(all_props[sel])\n            s_labels.append(labels[sel])\n            s_gt.append(gt_boxes[gi[sel]])\n        return s_props, s_labels, s_gt\n\n    def _roi_loss(self, cls_logits, bbox_preds, labels_list, gt_list, props_list):\n        all_labels = torch.cat(labels_list)    # (N,)\n        all_gt     = torch.cat(gt_list)        # (N,4)\n        all_props  = torch.cat(props_list)     # (N,4)\n        cls_loss   = F.cross_entropy(cls_logits, all_labels)\n        pos = all_labels > 0\n        if pos.any():\n            tgt_deltas  = encode_boxes(all_props[pos], all_gt[pos])\n            C           = self.num_classes\n            pred_deltas = bbox_preds[pos].view(-1,C,4)[torch.arange(pos.sum()), all_labels[pos]]\n            box_loss    = F.smooth_l1_loss(pred_deltas, tgt_deltas, beta=1./9)\n        else:\n            box_loss = bbox_preds.sum()*0.\n        return cls_loss, box_loss\n\n    def _postprocess(self, cls_logits, bbox_preds, proposals_list, image_size):\n        H,W = image_size; C = self.num_classes; results=[]; offset=0\n        for props in proposals_list:\n            n = len(props)\n            if n==0:\n                results.append({'boxes':torch.zeros(0,4),'scores':torch.zeros(0),\n                                'labels':torch.zeros(0,dtype=torch.long)})\n                continue\n            logits=cls_logits[offset:offset+n]; deltas=bbox_preds[offset:offset+n]\n            offset+=n\n            scores=F.softmax(logits,-1)\n            all_b,all_s,all_l=[],[],[]\n            for ci in range(1,C):\n                boxes=decode_boxes(props, deltas.view(n,C,4)[:,ci,:])\n                boxes[:,[0,2]]=boxes[:,[0,2]].clamp(0,W); boxes[:,[1,3]]=boxes[:,[1,3]].clamp(0,H)\n                sc=scores[:,ci]; mask=sc>self.SCORE_THR\n                if not mask.any(): continue\n                keep=RegionProposalNetwork._nms(boxes[mask],sc[mask],self.NMS_THR)\n                all_b.append(boxes[mask][keep]); all_s.append(sc[mask][keep])\n                all_l.append(torch.full((len(keep),),ci,dtype=torch.long,device=props.device))\n            if all_b:\n                b=torch.cat(all_b); s=torch.cat(all_s); l=torch.cat(all_l)\n                top=s.argsort(descending=True)[:self.MAX_DETS]\n                results.append({'boxes':b[top],'scores':s[top],'labels':l[top]})\n            else:\n                results.append({'boxes':torch.zeros(0,4),'scores':torch.zeros(0),\n                                'labels':torch.zeros(0,dtype=torch.long)})\n        return results\n\n    def forward(self, images, targets=None):\n        img_sz = (images.shape[2], images.shape[3])\n        feats  = self.backbone(images)\n        fpn_fs = self.fpn(feats)\n        props, rpn_losses = self.rpn(fpn_fs, img_sz, targets)\n        if self.training:\n            s_props, s_labels, s_gt = self._sample_rois(props, targets)\n            roi_feats  = self.roi_align(fpn_fs[:4], s_props, img_sz)\n            box_feats  = self.box_head(roi_feats)\n            cls_logits, bbox_preds = self.predictor(box_feats)\n            cls_loss, box_loss = self._roi_loss(cls_logits, bbox_preds,\n                                                s_labels, s_gt, s_props)\n            return {**rpn_losses, 'roi_cls': cls_loss, 'roi_box': box_loss}\n        else:\n            roi_feats  = self.roi_align(fpn_fs[:4], props, img_sz)\n            box_feats  = self.box_head(roi_feats)\n            cls_logits, bbox_preds = self.predictor(box_feats)\n            return self._postprocess(cls_logits, bbox_preds, props, img_sz)\n\n\n# Quick forward check on CPU (cheaper)\nmodel = FasterRCNN(num_classes=NUM_CLASSES)\nmodel.train()\nwith torch.no_grad():\n    dummy_imgs = torch.randn(1, 3, 600, 600)\n    dummy_tgts = [{'boxes':  torch.tensor([[50.,50.,250.,250.],[100.,100.,400.,400.]]),\n                   'labels': torch.tensor([3, 7])}]\n    losses_check = model(dummy_imgs, dummy_tgts)\nprint(\"Loss keys:\", list(losses_check.keys()))\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal     = sum(p.numel() for p in model.parameters())\nprint(f\"Parameters: {total/1e6:.1f}M total  |  {trainable/1e6:.1f}M trainable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a4b2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 6. Training demo (5 gradient steps with AMP) ─────────────────────────────\n",
    "\n",
    "model     = FasterRCNN(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "optimizer = torch.optim.SGD(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr=0.005, momentum=0.9, weight_decay=1e-4)\n",
    "scaler    = GradScaler()\n",
    "\n",
    "TRAIN_STEPS = 5\n",
    "train_ds = COCOStreamDataset(split='train', max_samples=TRAIN_STEPS)\n",
    "train_dl = DataLoader(train_ds, batch_size=1, collate_fn=frcnn_collate_fn)\n",
    "\n",
    "model.train()\n",
    "history = []\n",
    "\n",
    "for step, (images, targets) in enumerate(train_dl):\n",
    "    images  = images.to(DEVICE)\n",
    "    targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    with autocast():\n",
    "        losses = model(images, targets)\n",
    "        total  = sum(losses.values())\n",
    "\n",
    "    scaler.scale(total).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "    nn.utils.clip_grad_norm_([p for p in model.parameters() if p.requires_grad],\n",
    "                              max_norm=10.0)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    info = {k: f\"{v.item():.4f}\" for k, v in losses.items()}\n",
    "    info['total'] = f\"{total.item():.4f}\"\n",
    "    history.append({k: float(v.item()) for k, v in {**losses, 'total': total}.items()})\n",
    "    print(f\"Step {step+1}/{TRAIN_STEPS}  {info}\")\n",
    "\n",
    "print(\"\\nTraining demo complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e72f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 7. Loss curves ────────────────────────────────────────────────────────────\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "for k in [kk for kk in history[0] if kk != 'total']:\n",
    "    ax.plot([h[k] for h in history], label=k, marker='o')\n",
    "ax.set_xlabel('Step'); ax.set_ylabel('Loss')\n",
    "ax.set_title('Individual Loss Components (5 steps)'); ax.legend()\n",
    "\n",
    "axes[1].plot([h['total'] for h in history], 'r-o', linewidth=2)\n",
    "axes[1].set_xlabel('Step'); axes[1].set_ylabel('Total loss')\n",
    "axes[1].set_title('Total Loss (5 steps)')\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs('images', exist_ok=True)\n",
    "plt.savefig('images/loss_curves.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f0c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 8. Save checkpoint ────────────────────────────────────────────────────────\n",
    "\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "ckpt_path = 'checkpoints/faster_rcnn_demo.pth'\n",
    "torch.save({\n",
    "    'model_state_dict':     model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'steps_trained':        TRAIN_STEPS,\n",
    "    'num_classes':          NUM_CLASSES,\n",
    "    'final_losses':         history[-1],\n",
    "}, ckpt_path)\n",
    "size_mb = os.path.getsize(ckpt_path) / 1024**2\n",
    "print(f\"Checkpoint saved → {ckpt_path}  ({size_mb:.1f} MB)\")\n",
    "print(f\"Final losses: { {k: f'{v:.4f}' for k,v in history[-1].items()} }\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
