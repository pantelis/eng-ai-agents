{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1d9335",
   "metadata": {
    "papermill": {
     "duration": 0.003752,
     "end_time": "2026-02-15T16:30:54.508860",
     "exception": false,
     "start_time": "2026-02-15T16:30:54.505108",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference, NMS, and COCO Evaluation\n",
    "\n",
    "**Notebook 5 of 5 in the YOLOv11 From-Scratch Series**\n",
    "\n",
    "Now that we have a trained model (Notebook 4), we need to turn its raw predictions into usable detections. This notebook covers the complete inference and evaluation pipeline:\n",
    "\n",
    "1. **Prediction decoding** --- converting raw network outputs (class logits and DFL-encoded offsets) into bounding boxes in image coordinates.\n",
    "2. **Non-Maximum Suppression (NMS)** --- a greedy filtering algorithm that removes redundant overlapping detections, keeping only the most confident prediction for each object.\n",
    "3. **COCO mAP evaluation** --- the standardized evaluation protocol that computes Average Precision across multiple IoU thresholds (0.5 to 0.95), providing a single number that captures both localization accuracy and classification performance.\n",
    "4. **Grad-CAM visualization** --- gradient-weighted class activation mapping reveals which spatial regions in the image drive the model's predictions, offering interpretability into the detection process.\n",
    "\n",
    "By the end of this notebook, you will have a complete, end-to-end object detection system: from a raw image tensor to evaluated, visualized detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Colab Environment Setup ---\n",
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    %pip install -q matplotlib seaborn scikit-learn scipy tqdm datasets opencv-python\n",
    "    print(\"Colab dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d92283",
   "metadata": {
    "papermill": {
     "duration": 0.002516,
     "end_time": "2026-02-15T16:30:54.515490",
     "exception": false,
     "start_time": "2026-02-15T16:30:54.512974",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352862db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:54.521595Z",
     "iopub.status.busy": "2026-02-15T16:30:54.521430Z",
     "iopub.status.idle": "2026-02-15T16:30:56.336402Z",
     "shell.execute_reply": "2026-02-15T16:30:56.335724Z"
    },
    "papermill": {
     "duration": 1.819195,
     "end_time": "2026-02-15T16:30:56.337233",
     "exception": false,
     "start_time": "2026-02-15T16:30:54.518038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import json, os, time, math\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "from datasets import load_dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59603d0b",
   "metadata": {
    "papermill": {
     "duration": 0.002845,
     "end_time": "2026-02-15T16:30:56.345249",
     "exception": false,
     "start_time": "2026-02-15T16:30:56.342404",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model components (from Notebooks 2--3)\n",
    "\n",
    "The following cells re-define the complete YOLOv11 model architecture introduced in Notebooks 2 and 3. They are reproduced here in compact form so that this notebook is fully self-contained. Refer to those notebooks for detailed explanations of each module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68054412",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:56.352120Z",
     "iopub.status.busy": "2026-02-15T16:30:56.351858Z",
     "iopub.status.idle": "2026-02-15T16:30:56.369569Z",
     "shell.execute_reply": "2026-02-15T16:30:56.368874Z"
    },
    "papermill": {
     "duration": 0.021961,
     "end_time": "2026-02-15T16:30:56.370081",
     "exception": false,
     "start_time": "2026-02-15T16:30:56.348120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvBNSiLU(nn.Module):\n",
    "    def __init__(self, in_c, out_c, k=1, s=1, p=None, g=1):\n",
    "        super().__init__()\n",
    "        if p is None:\n",
    "            p = k // 2\n",
    "        self.conv = nn.Conv2d(in_c, out_c, k, s, p, groups=g, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_c)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_c, out_c, shortcut=True, e=0.5):\n",
    "        super().__init__()\n",
    "        mid = int(out_c * e)\n",
    "        self.cv1 = ConvBNSiLU(in_c, mid, 3)\n",
    "        self.cv2 = ConvBNSiLU(mid, out_c, 3)\n",
    "        self.add = shortcut and in_c == out_c\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.cv2(self.cv1(x))\n",
    "        return x + y if self.add else y\n",
    "\n",
    "\n",
    "class C3k2(nn.Module):\n",
    "    def __init__(self, in_c, out_c, n=2, shortcut=True, e=0.5):\n",
    "        super().__init__()\n",
    "        self.c = int(out_c * e)\n",
    "        self.cv1 = ConvBNSiLU(in_c, 2 * self.c, 1)\n",
    "        self.cv2 = ConvBNSiLU((2 + n) * self.c, out_c, 1)\n",
    "        self.bottlenecks = nn.ModuleList(\n",
    "            Bottleneck(self.c, self.c, shortcut) for _ in range(n)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = list(self.cv1(x).chunk(2, dim=1))\n",
    "        for bn in self.bottlenecks:\n",
    "            y.append(bn(y[-1]))\n",
    "        return self.cv2(torch.cat(y, dim=1))\n",
    "\n",
    "\n",
    "class SPPF(nn.Module):\n",
    "    def __init__(self, in_c, out_c, k=5):\n",
    "        super().__init__()\n",
    "        mid = in_c // 2\n",
    "        self.cv1 = ConvBNSiLU(in_c, mid, 1)\n",
    "        self.cv2 = ConvBNSiLU(mid * 4, out_c, 1)\n",
    "        self.pool = nn.MaxPool2d(k, stride=1, padding=k // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cv1(x)\n",
    "        y1 = self.pool(x)\n",
    "        y2 = self.pool(y1)\n",
    "        y3 = self.pool(y2)\n",
    "        return self.cv2(torch.cat([x, y1, y2, y3], dim=1))\n",
    "\n",
    "\n",
    "class YOLOv11Backbone(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_channels=64):\n",
    "        super().__init__()\n",
    "        c1, c2, c3, c4, c5 = (\n",
    "            base_channels, base_channels * 2, base_channels * 4,\n",
    "            base_channels * 8, base_channels * 16,\n",
    "        )\n",
    "        self.stem = ConvBNSiLU(in_channels, c1, 3, 2)\n",
    "        self.stage1 = nn.Sequential(ConvBNSiLU(c1, c2, 3, 2), C3k2(c2, c2, n=2))\n",
    "        self.stage2 = nn.Sequential(ConvBNSiLU(c2, c3, 3, 2), C3k2(c3, c3, n=2))\n",
    "        self.stage3 = nn.Sequential(ConvBNSiLU(c3, c4, 3, 2), C3k2(c4, c4, n=2))\n",
    "        self.stage4 = nn.Sequential(ConvBNSiLU(c4, c5, 3, 2), C3k2(c5, c5, n=2), SPPF(c5, c5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        p3 = self.stage2(x)\n",
    "        p4 = self.stage3(p3)\n",
    "        p5 = self.stage4(p4)\n",
    "        return p3, p4, p5\n",
    "\n",
    "\n",
    "class FPN(nn.Module):\n",
    "    def __init__(self, c3=256, c4=512, c5=1024):\n",
    "        super().__init__()\n",
    "        self.lateral_p5 = ConvBNSiLU(c5, c4, 1)\n",
    "        self.lateral_p4 = ConvBNSiLU(c4, c3, 1)\n",
    "        self.fpn_p4 = C3k2(c4 + c4, c4, n=2, shortcut=False)\n",
    "        self.fpn_p3 = C3k2(c3 + c3, c3, n=2, shortcut=False)\n",
    "\n",
    "    def forward(self, p3, p4, p5):\n",
    "        p5_up = F.interpolate(self.lateral_p5(p5), size=p4.shape[2:], mode='nearest')\n",
    "        fpn_p4 = self.fpn_p4(torch.cat([p5_up, p4], dim=1))\n",
    "        p4_up = F.interpolate(self.lateral_p4(fpn_p4), size=p3.shape[2:], mode='nearest')\n",
    "        fpn_p3 = self.fpn_p3(torch.cat([p4_up, p3], dim=1))\n",
    "        return fpn_p3, fpn_p4, p5\n",
    "\n",
    "\n",
    "class PAN(nn.Module):\n",
    "    def __init__(self, c3=256, c4=512, c5=1024):\n",
    "        super().__init__()\n",
    "        self.down_p3 = ConvBNSiLU(c3, c3, 3, 2)\n",
    "        self.down_p4 = ConvBNSiLU(c4, c4, 3, 2)\n",
    "        self.pan_p4 = C3k2(c3 + c4, c4, n=2, shortcut=False)\n",
    "        self.pan_p5 = C3k2(c4 + c5, c5, n=2, shortcut=False)\n",
    "\n",
    "    def forward(self, fpn_p3, fpn_p4, p5):\n",
    "        p3_down = self.down_p3(fpn_p3)\n",
    "        pan_p4 = self.pan_p4(torch.cat([p3_down, fpn_p4], dim=1))\n",
    "        p4_down = self.down_p4(pan_p4)\n",
    "        pan_p5 = self.pan_p5(torch.cat([p4_down, p5], dim=1))\n",
    "        return fpn_p3, pan_p4, pan_p5\n",
    "\n",
    "\n",
    "class C2PSA(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n=1):\n",
    "        super().__init__()\n",
    "        self.c = in_channels // 2\n",
    "        self.cv1 = ConvBNSiLU(in_channels, 2 * self.c, 1)\n",
    "        self.cv2 = ConvBNSiLU(2 * self.c, out_channels, 1)\n",
    "        self.attention = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "                nn.Linear(self.c, self.c // 4), nn.SiLU(inplace=True),\n",
    "                nn.Linear(self.c // 4, self.c), nn.Sigmoid()\n",
    "            ) for _ in range(n)\n",
    "        ])\n",
    "        self.bottlenecks = nn.ModuleList(\n",
    "            [Bottleneck(self.c, self.c, shortcut=True) for _ in range(n)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = list(self.cv1(x).chunk(2, dim=1))\n",
    "        for attn, bn in zip(self.attention, self.bottlenecks):\n",
    "            feat = bn(y[-1])\n",
    "            att_weights = attn(feat).unsqueeze(-1).unsqueeze(-1)\n",
    "            feat = feat * att_weights\n",
    "            y.append(feat)\n",
    "        return self.cv2(torch.cat([y[0], y[-1]], dim=1))\n",
    "\n",
    "\n",
    "class DFLHead(nn.Module):\n",
    "    def __init__(self, reg_max=16):\n",
    "        super().__init__()\n",
    "        self.reg_max = reg_max\n",
    "        self.register_buffer('project', torch.arange(reg_max, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, _, h, w = x.shape\n",
    "        x = x.view(b, 4, self.reg_max, h, w)\n",
    "        x = F.softmax(x, dim=2)\n",
    "        x = (x * self.project.view(1, 1, -1, 1, 1)).sum(dim=2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DetectionHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes=80, reg_max=16):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.reg_max = reg_max\n",
    "        self.cls_branch = nn.Sequential(\n",
    "            ConvBNSiLU(in_channels, in_channels, 3),\n",
    "            ConvBNSiLU(in_channels, in_channels, 3),\n",
    "            nn.Conv2d(in_channels, num_classes, 1)\n",
    "        )\n",
    "        self.reg_branch = nn.Sequential(\n",
    "            ConvBNSiLU(in_channels, in_channels, 3),\n",
    "            ConvBNSiLU(in_channels, in_channels, 3),\n",
    "            nn.Conv2d(in_channels, 4 * reg_max, 1)\n",
    "        )\n",
    "        self.dfl = DFLHead(reg_max)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cls_pred = self.cls_branch(x)\n",
    "        box_raw = self.reg_branch(x)\n",
    "        box_pred = self.dfl(box_raw)\n",
    "        return cls_pred, box_pred, box_raw\n",
    "\n",
    "\n",
    "class YOLOv11(nn.Module):\n",
    "    def __init__(self, num_classes=80, reg_max=16, base_channels=64):\n",
    "        super().__init__()\n",
    "        c3, c4, c5 = base_channels * 4, base_channels * 8, base_channels * 16\n",
    "        self.backbone = YOLOv11Backbone(base_channels=base_channels)\n",
    "        self.fpn = FPN(c3, c4, c5)\n",
    "        self.pan = PAN(c3, c4, c5)\n",
    "        self.c2psa = C2PSA(c5, c5, n=1)\n",
    "        self.head_p3 = DetectionHead(c3, num_classes, reg_max)\n",
    "        self.head_p4 = DetectionHead(c4, num_classes, reg_max)\n",
    "        self.head_p5 = DetectionHead(c5, num_classes, reg_max)\n",
    "        self.strides = [8, 16, 32]\n",
    "        self.num_classes = num_classes\n",
    "        self.reg_max = reg_max\n",
    "\n",
    "    def forward(self, x):\n",
    "        p3, p4, p5 = self.backbone(x)\n",
    "        fpn_p3, fpn_p4, fpn_p5 = self.fpn(p3, p4, p5)\n",
    "        pan_p3, pan_p4, pan_p5 = self.pan(fpn_p3, fpn_p4, fpn_p5)\n",
    "        pan_p5 = self.c2psa(pan_p5)\n",
    "        pred_p3 = self.head_p3(pan_p3)\n",
    "        pred_p4 = self.head_p4(pan_p4)\n",
    "        pred_p5 = self.head_p5(pan_p5)\n",
    "        return [pred_p3, pred_p4, pred_p5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335164d0",
   "metadata": {
    "papermill": {
     "duration": 0.003365,
     "end_time": "2026-02-15T16:30:56.376205",
     "exception": false,
     "start_time": "2026-02-15T16:30:56.372840",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prediction Decoding\n",
    "\n",
    "The model outputs raw class logits and DFL-decoded LTRB (left, top, right, bottom) offsets at each feature map location. To obtain actual bounding boxes in image coordinates, we need to:\n",
    "\n",
    "1. **Generate anchor points** --- for each cell in each feature map, compute its center in pixel coordinates. A cell at grid position $(i, j)$ in a feature map with stride $s$ corresponds to pixel center $((i + 0.5) \\times s, \\; (j + 0.5) \\times s)$.\n",
    "\n",
    "2. **Apply stride-aware decoding** --- the LTRB offsets predicted by the network are in feature-map units. Multiplying by the stride converts them to pixel distances. The final box coordinates are:\n",
    "\n",
    "$$x_1 = c_x - l \\times s, \\quad y_1 = c_y - t \\times s, \\quad x_2 = c_x + r \\times s, \\quad y_2 = c_y + b \\times s$$\n",
    "\n",
    "where $(c_x, c_y)$ is the anchor center, $(l, t, r, b)$ are the predicted offsets, and $s$ is the stride.\n",
    "\n",
    "3. **Filter by confidence** --- discard predictions with class probability below a threshold (e.g., 0.25) to reduce the number of candidates before NMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd96407",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:56.382863Z",
     "iopub.status.busy": "2026-02-15T16:30:56.382698Z",
     "iopub.status.idle": "2026-02-15T16:30:56.388791Z",
     "shell.execute_reply": "2026-02-15T16:30:56.388137Z"
    },
    "papermill": {
     "duration": 0.010381,
     "end_time": "2026-02-15T16:30:56.389230",
     "exception": false,
     "start_time": "2026-02-15T16:30:56.378849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_anchor_points(feat_sizes, strides, device='cpu'):\n",
    "    \"\"\"Generate anchor center points for all feature levels.\n",
    "    \n",
    "    For each feature map cell, computes the center in pixel coordinates.\n",
    "    A cell at position (i, j) with stride s maps to pixel center\n",
    "    ((i + 0.5) * s, (j + 0.5) * s).\n",
    "    \n",
    "    Args:\n",
    "        feat_sizes: list of (H, W) for each scale\n",
    "        strides: list of stride values [8, 16, 32]\n",
    "        device: target device\n",
    "    Returns:\n",
    "        anchor_points: (total_anchors, 2) pixel centers [x, y]\n",
    "        anchor_strides: (total_anchors,) stride per anchor\n",
    "    \"\"\"\n",
    "    all_points = []\n",
    "    all_strides_tensor = []\n",
    "    for (h, w), stride in zip(feat_sizes, strides):\n",
    "        sy, sx = torch.meshgrid(torch.arange(h, device=device),\n",
    "                                torch.arange(w, device=device), indexing='ij')\n",
    "        points = torch.stack([sx.flatten(), sy.flatten()], dim=-1).float()\n",
    "        points = (points + 0.5) * stride\n",
    "        all_points.append(points)\n",
    "        all_strides_tensor.append(torch.full((h * w,), stride, dtype=torch.float32, device=device))\n",
    "    return torch.cat(all_points), torch.cat(all_strides_tensor)\n",
    "\n",
    "\n",
    "def decode_predictions(predictions, strides=[8, 16, 32], conf_thresh=0.25, img_size=640):\n",
    "    \"\"\"Decode raw model outputs into boxes with scores.\n",
    "    \n",
    "    Args:\n",
    "        predictions: list of (cls_pred, box_pred, box_raw) per scale\n",
    "        strides: feature strides\n",
    "        conf_thresh: confidence threshold for filtering\n",
    "        img_size: input image size for clamping\n",
    "    Returns:\n",
    "        batch_results: list of (boxes, scores, class_ids) per image\n",
    "            boxes: (N, 4) decoded boxes [x1, y1, x2, y2]\n",
    "            scores: (N,) confidence scores\n",
    "            class_ids: (N,) predicted class indices\n",
    "    \"\"\"\n",
    "    device = predictions[0][0].device\n",
    "    feat_sizes = [(p[0].shape[2], p[0].shape[3]) for p in predictions]\n",
    "    anchor_points, anchor_strides = make_anchor_points(feat_sizes, strides, device)\n",
    "    \n",
    "    # Concatenate across scales\n",
    "    all_cls = torch.cat([p[0].flatten(2).permute(0, 2, 1) for p in predictions], dim=1)\n",
    "    all_box = torch.cat([p[1].flatten(2).permute(0, 2, 1) for p in predictions], dim=1)\n",
    "    \n",
    "    batch_results = []\n",
    "    batch_size = all_cls.shape[0]\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        cls_scores = all_cls[b].sigmoid()  # (num_anchors, num_classes)\n",
    "        box_pred = all_box[b]              # (num_anchors, 4) LTRB\n",
    "        \n",
    "        # Decode LTRB to xyxy\n",
    "        lt = box_pred[:, :2] * anchor_strides.unsqueeze(-1)\n",
    "        rb = box_pred[:, 2:] * anchor_strides.unsqueeze(-1)\n",
    "        x1y1 = anchor_points - lt\n",
    "        x2y2 = anchor_points + rb\n",
    "        boxes = torch.cat([x1y1, x2y2], dim=-1)\n",
    "        \n",
    "        # Get max class score per anchor\n",
    "        max_scores, class_ids = cls_scores.max(dim=1)\n",
    "        \n",
    "        # Filter by confidence\n",
    "        mask = max_scores > conf_thresh\n",
    "        boxes = boxes[mask]\n",
    "        scores = max_scores[mask]\n",
    "        class_ids = class_ids[mask]\n",
    "        \n",
    "        # Clip to image bounds\n",
    "        boxes = boxes.clamp(0, img_size)\n",
    "        \n",
    "        batch_results.append((boxes, scores, class_ids))\n",
    "    \n",
    "    return batch_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef9157",
   "metadata": {
    "papermill": {
     "duration": 0.002465,
     "end_time": "2026-02-15T16:30:56.394245",
     "exception": false,
     "start_time": "2026-02-15T16:30:56.391780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Non-Maximum Suppression\n",
    "\n",
    "After decoding and confidence filtering, we typically have many overlapping detections for the same object. **Non-Maximum Suppression (NMS)** is a greedy post-processing algorithm that keeps only the most confident detection for each object instance:\n",
    "\n",
    "1. **Sort** all detections by confidence score (descending).\n",
    "2. **Select** the highest-scoring detection and add it to the output.\n",
    "3. **Compute IoU** between the selected detection and all remaining candidates.\n",
    "4. **Remove** any candidate whose IoU with the selected detection exceeds a threshold (e.g., 0.45)---these are considered redundant detections of the same object.\n",
    "5. **Repeat** from step 2 until no candidates remain.\n",
    "\n",
    "For multi-class detection, we apply NMS independently per class (\"batched NMS\") to avoid suppressing detections of different object categories that happen to overlap spatially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a572c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:56.400194Z",
     "iopub.status.busy": "2026-02-15T16:30:56.400013Z",
     "iopub.status.idle": "2026-02-15T16:30:56.406231Z",
     "shell.execute_reply": "2026-02-15T16:30:56.405463Z"
    },
    "papermill": {
     "duration": 0.010291,
     "end_time": "2026-02-15T16:30:56.406959",
     "exception": false,
     "start_time": "2026-02-15T16:30:56.396668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nms(boxes, scores, iou_threshold=0.45):\n",
    "    \"\"\"Non-Maximum Suppression from scratch.\n",
    "    \n",
    "    Args:\n",
    "        boxes: (N, 4) [x1, y1, x2, y2]\n",
    "        scores: (N,) confidence scores\n",
    "        iou_threshold: IoU threshold for suppression\n",
    "    Returns:\n",
    "        keep: indices of kept boxes\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return torch.tensor([], dtype=torch.long)\n",
    "    \n",
    "    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
    "    areas = (x2 - x1) * (y2 - y1)\n",
    "    \n",
    "    # Sort by score (descending)\n",
    "    order = scores.argsort(descending=True)\n",
    "    \n",
    "    keep = []\n",
    "    while len(order) > 0:\n",
    "        i = order[0].item()\n",
    "        keep.append(i)\n",
    "        \n",
    "        if len(order) == 1:\n",
    "            break\n",
    "        \n",
    "        # Compute IoU with remaining boxes\n",
    "        remaining = order[1:]\n",
    "        inter_x1 = torch.max(x1[i], x1[remaining])\n",
    "        inter_y1 = torch.max(y1[i], y1[remaining])\n",
    "        inter_x2 = torch.min(x2[i], x2[remaining])\n",
    "        inter_y2 = torch.min(y2[i], y2[remaining])\n",
    "        \n",
    "        inter = (inter_x2 - inter_x1).clamp(0) * (inter_y2 - inter_y1).clamp(0)\n",
    "        union = areas[i] + areas[remaining] - inter\n",
    "        iou = inter / (union + 1e-7)\n",
    "        \n",
    "        # Keep boxes with IoU below threshold\n",
    "        mask = iou <= iou_threshold\n",
    "        order = remaining[mask]\n",
    "    \n",
    "    return torch.tensor(keep, dtype=torch.long)\n",
    "\n",
    "\n",
    "def batched_nms(boxes, scores, class_ids, iou_threshold=0.45):\n",
    "    \"\"\"Per-class NMS.\n",
    "    \n",
    "    Applies NMS independently for each class to avoid suppressing\n",
    "    detections of different categories that overlap spatially.\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return boxes, scores, class_ids\n",
    "    \n",
    "    keep_all = []\n",
    "    for cls in class_ids.unique():\n",
    "        cls_mask = class_ids == cls\n",
    "        cls_boxes = boxes[cls_mask]\n",
    "        cls_scores = scores[cls_mask]\n",
    "        cls_indices = torch.where(cls_mask)[0]\n",
    "        \n",
    "        keep = nms(cls_boxes, cls_scores, iou_threshold)\n",
    "        keep_all.append(cls_indices[keep])\n",
    "    \n",
    "    if keep_all:\n",
    "        keep_all = torch.cat(keep_all)\n",
    "        # Sort by score\n",
    "        sorted_idx = scores[keep_all].argsort(descending=True)\n",
    "        keep_all = keep_all[sorted_idx]\n",
    "    else:\n",
    "        keep_all = torch.tensor([], dtype=torch.long)\n",
    "    \n",
    "    return boxes[keep_all], scores[keep_all], class_ids[keep_all]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef72b683",
   "metadata": {
    "papermill": {
     "duration": 0.002377,
     "end_time": "2026-02-15T16:30:56.411849",
     "exception": false,
     "start_time": "2026-02-15T16:30:56.409472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## End-to-End Inference Pipeline\n",
    "\n",
    "The `detect` function chains the complete inference pipeline: forward pass through the model, prediction decoding, and per-class NMS. The result is a clean set of bounding boxes, confidence scores, and class labels ready for visualization or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e4d3d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:56.417411Z",
     "iopub.status.busy": "2026-02-15T16:30:56.417264Z",
     "iopub.status.idle": "2026-02-15T16:30:56.420205Z",
     "shell.execute_reply": "2026-02-15T16:30:56.419853Z"
    },
    "papermill": {
     "duration": 0.006488,
     "end_time": "2026-02-15T16:30:56.420694",
     "exception": false,
     "start_time": "2026-02-15T16:30:56.414206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def detect(model, image_tensor, conf_thresh=0.25, iou_thresh=0.45):\n",
    "    \"\"\"Full inference pipeline: model -> decode -> NMS -> results.\n",
    "    \n",
    "    Args:\n",
    "        model: YOLOv11 model\n",
    "        image_tensor: (1, 3, 640, 640) normalized\n",
    "        conf_thresh: confidence threshold\n",
    "        iou_thresh: NMS IoU threshold\n",
    "    Returns:\n",
    "        boxes: (N, 4) [x1, y1, x2, y2]\n",
    "        scores: (N,) confidences\n",
    "        class_ids: (N,) class indices\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = model(image_tensor)\n",
    "    batch_results = decode_predictions(predictions, conf_thresh=conf_thresh)\n",
    "    boxes, scores, class_ids = batch_results[0]\n",
    "    boxes, scores, class_ids = batched_nms(boxes, scores, class_ids, iou_thresh)\n",
    "    return boxes, scores, class_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc61c63",
   "metadata": {
    "papermill": {
     "duration": 0.002323,
     "end_time": "2026-02-15T16:30:56.425452",
     "exception": false,
     "start_time": "2026-02-15T16:30:56.423129",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523784cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:56.431158Z",
     "iopub.status.busy": "2026-02-15T16:30:56.431019Z",
     "iopub.status.idle": "2026-02-15T16:30:56.436322Z",
     "shell.execute_reply": "2026-02-15T16:30:56.435728Z"
    },
    "papermill": {
     "duration": 0.00885,
     "end_time": "2026-02-15T16:30:56.436755",
     "exception": false,
     "start_time": "2026-02-15T16:30:56.427905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "COCO_NAMES = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck',\n",
    "              'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench',\n",
    "              'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra',\n",
    "              'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "              'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "              'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "              'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n",
    "              'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "              'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',\n",
    "              'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
    "              'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "              'hair drier', 'toothbrush']\n",
    "\n",
    "\n",
    "def visualize_detections(image_tensor, boxes, scores, class_ids, class_names=None, max_det=50):\n",
    "    \"\"\"Draw detection results on image.\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "    \n",
    "    img = image_tensor[0].permute(1, 2, 0).cpu().numpy()\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, 80))\n",
    "    \n",
    "    for i in range(min(len(boxes), max_det)):\n",
    "        x1, y1, x2, y2 = boxes[i].cpu().numpy()\n",
    "        score = scores[i].cpu().item()\n",
    "        cls = class_ids[i].cpu().item()\n",
    "        \n",
    "        color = colors[cls % len(colors)]\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2,\n",
    "                                  edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        name = class_names[cls] if class_names else str(cls)\n",
    "        label = f'{name}: {score:.2f}'\n",
    "        ax.text(x1, y1 - 5, label, color='white', fontsize=9,\n",
    "                bbox=dict(boxstyle='round,pad=0.2', facecolor=color, alpha=0.8))\n",
    "    \n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Detections: {len(boxes)} objects')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31933d2b",
   "metadata": {
    "papermill": {
     "duration": 0.002379,
     "end_time": "2026-02-15T16:30:56.441626",
     "exception": false,
     "start_time": "2026-02-15T16:30:56.439247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Demo inference on a real COCO image\n",
    "\n",
    "We run the full detection pipeline on a real COCO validation image streamed from Hugging Face. With random (untrained) weights, we expect few or no detections above the confidence threshold, but the image itself will be real.\n",
    "\n",
    "> **Data source**: Images streamed from [detection-datasets/coco](https://huggingface.co/datasets/detection-datasets/coco). See our [HF COCO streaming tutorial](/blog/tutorials/hf-coco-streaming) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d206c36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:56.447888Z",
     "iopub.status.busy": "2026-02-15T16:30:56.447731Z",
     "iopub.status.idle": "2026-02-15T16:31:14.858302Z",
     "shell.execute_reply": "2026-02-15T16:31:14.857809Z"
    },
    "papermill": {
     "duration": 18.420114,
     "end_time": "2026-02-15T16:31:14.864278",
     "exception": false,
     "start_time": "2026-02-15T16:30:56.444164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load a real COCO validation image for demo inference\n",
    "model = YOLOv11(num_classes=80)\n",
    "model.eval()\n",
    "\n",
    "# Stream a single COCO validation image\n",
    "print(\"Loading real COCO image from Hugging Face...\")\n",
    "ds = load_dataset('detection-datasets/coco', split='val', streaming=True)\n",
    "example = next(iter(ds))\n",
    "\n",
    "img_pil = example['image'].convert('RGB')\n",
    "img_np = np.array(img_pil)\n",
    "\n",
    "# Letterbox resize to 640x640\n",
    "h_orig, w_orig = img_np.shape[:2]\n",
    "scale = 640 / max(h_orig, w_orig)\n",
    "new_w, new_h = int(w_orig * scale), int(h_orig * scale)\n",
    "resized = np.array(img_pil.resize((new_w, new_h)))\n",
    "\n",
    "padded = np.full((640, 640, 3), 114, dtype=np.uint8)\n",
    "pad_w = (640 - new_w) // 2\n",
    "pad_h = (640 - new_h) // 2\n",
    "padded[pad_h:pad_h + new_h, pad_w:pad_w + new_w] = resized\n",
    "\n",
    "x = torch.from_numpy(padded).permute(2, 0, 1).float().unsqueeze(0) / 255.0\n",
    "\n",
    "# Run inference\n",
    "start = time.time()\n",
    "boxes, scores, class_ids = detect(model, x, conf_thresh=0.1, iou_thresh=0.45)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Inference time: {elapsed*1000:.1f} ms\")\n",
    "print(f\"Detections after NMS: {len(boxes)}\")\n",
    "\n",
    "if len(boxes) > 0:\n",
    "    print(f\"Top detection: class={COCO_NAMES[class_ids[0]]}, score={scores[0]:.3f}\")\n",
    "    visualize_detections(x, boxes, scores, class_ids, COCO_NAMES)\n",
    "else:\n",
    "    print(\"No detections above threshold (expected with random weights)\")\n",
    "    # Show the input image\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    ax.imshow(x[0].permute(1, 2, 0).numpy())\n",
    "    ax.set_title('Input COCO Image (no detections with random weights)')\n",
    "    ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c25ae3",
   "metadata": {
    "papermill": {
     "duration": 0.013155,
     "end_time": "2026-02-15T16:31:14.890594",
     "exception": false,
     "start_time": "2026-02-15T16:31:14.877439",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## COCO mAP Evaluation\n",
    "\n",
    "The **COCO evaluation protocol** is the standard benchmark for object detection. Unlike simple accuracy metrics, it captures both localization precision and classification performance in a single number:\n",
    "\n",
    "- **AP (Average Precision)** is computed per class as the area under the precision-recall curve, using 101-point interpolation.\n",
    "- **AP@IoU** evaluates at a specific IoU threshold. AP@0.50 is lenient (50% overlap required), while AP@0.75 is strict.\n",
    "- **mAP@0.5:0.95** averages AP across 10 IoU thresholds from 0.50 to 0.95 in steps of 0.05, then averages across all classes. This is the primary COCO metric.\n",
    "\n",
    "The evaluation process:\n",
    "1. For each class, sort predictions by confidence score (descending).\n",
    "2. Match each prediction to ground-truth boxes using IoU. A prediction is a true positive (TP) if its IoU with an unmatched ground-truth box exceeds the threshold; otherwise it is a false positive (FP).\n",
    "3. Compute precision and recall at each confidence level.\n",
    "4. Interpolate the precision-recall curve and compute the area under it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b155eb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:31:14.917324Z",
     "iopub.status.busy": "2026-02-15T16:31:14.916972Z",
     "iopub.status.idle": "2026-02-15T16:31:14.933385Z",
     "shell.execute_reply": "2026-02-15T16:31:14.932753Z"
    },
    "papermill": {
     "duration": 0.031338,
     "end_time": "2026-02-15T16:31:14.934056",
     "exception": false,
     "start_time": "2026-02-15T16:31:14.902718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class COCOEvaluator:\n",
    "    \"\"\"Simplified COCO-style mAP evaluator.\n",
    "    \n",
    "    Computes Average Precision at IoU thresholds from 0.5 to 0.95.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, iou_thresholds=None):\n",
    "        if iou_thresholds is None:\n",
    "            self.iou_thresholds = np.arange(0.5, 1.0, 0.05)\n",
    "        else:\n",
    "            self.iou_thresholds = np.array(iou_thresholds)\n",
    "        self.predictions = []  # list of dicts\n",
    "        self.ground_truths = []  # list of dicts\n",
    "    \n",
    "    def add_predictions(self, image_id: int, boxes: np.ndarray, scores: np.ndarray, class_ids: np.ndarray):\n",
    "        \"\"\"Add predicted detections for one image.\"\"\"\n",
    "        for i in range(len(boxes)):\n",
    "            self.predictions.append({\n",
    "                'image_id': image_id,\n",
    "                'bbox': boxes[i],     # [x1, y1, x2, y2]\n",
    "                'score': float(scores[i]),\n",
    "                'class_id': int(class_ids[i])\n",
    "            })\n",
    "    \n",
    "    def add_ground_truths(self, image_id: int, boxes: np.ndarray, class_ids: np.ndarray):\n",
    "        \"\"\"Add ground-truth annotations for one image.\"\"\"\n",
    "        for i in range(len(boxes)):\n",
    "            self.ground_truths.append({\n",
    "                'image_id': image_id,\n",
    "                'bbox': boxes[i],\n",
    "                'class_id': int(class_ids[i]),\n",
    "                'matched': set()\n",
    "            })\n",
    "    \n",
    "    def _compute_iou_matrix(self, pred_boxes, gt_boxes):\n",
    "        \"\"\"Compute pairwise IoU between prediction and GT boxes.\"\"\"\n",
    "        x1 = np.maximum(pred_boxes[:, None, 0], gt_boxes[None, :, 0])\n",
    "        y1 = np.maximum(pred_boxes[:, None, 1], gt_boxes[None, :, 1])\n",
    "        x2 = np.minimum(pred_boxes[:, None, 2], gt_boxes[None, :, 2])\n",
    "        y2 = np.minimum(pred_boxes[:, None, 3], gt_boxes[None, :, 3])\n",
    "        \n",
    "        inter = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n",
    "        area_pred = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])\n",
    "        area_gt = (gt_boxes[:, 2] - gt_boxes[:, 0]) * (gt_boxes[:, 3] - gt_boxes[:, 1])\n",
    "        union = area_pred[:, None] + area_gt[None, :] - inter\n",
    "        \n",
    "        return inter / (union + 1e-7)\n",
    "    \n",
    "    def compute_ap(self, precisions, recalls):\n",
    "        \"\"\"Compute AP using 101-point interpolation (COCO style).\"\"\"\n",
    "        recall_points = np.linspace(0, 1, 101)\n",
    "        interpolated = np.zeros_like(recall_points)\n",
    "        \n",
    "        for i, r in enumerate(recall_points):\n",
    "            precs_above = precisions[recalls >= r]\n",
    "            interpolated[i] = precs_above.max() if len(precs_above) > 0 else 0\n",
    "        \n",
    "        return interpolated.mean()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Run full COCO-style evaluation.\n",
    "        \n",
    "        Returns:\n",
    "            results: dict with mAP@0.5:0.95, mAP@0.5, mAP@0.75, and per-threshold APs\n",
    "        \"\"\"\n",
    "        all_classes = set(p['class_id'] for p in self.predictions) | \\\n",
    "                      set(g['class_id'] for g in self.ground_truths)\n",
    "        \n",
    "        aps_per_threshold = {t: [] for t in self.iou_thresholds}\n",
    "        \n",
    "        for cls in sorted(all_classes):\n",
    "            cls_preds = [p for p in self.predictions if p['class_id'] == cls]\n",
    "            cls_gts = [g for g in self.ground_truths if g['class_id'] == cls]\n",
    "            \n",
    "            if not cls_gts:\n",
    "                continue\n",
    "            \n",
    "            # Sort predictions by score\n",
    "            cls_preds.sort(key=lambda x: x['score'], reverse=True)\n",
    "            \n",
    "            for iou_thresh in self.iou_thresholds:\n",
    "                tp = np.zeros(len(cls_preds))\n",
    "                fp = np.zeros(len(cls_preds))\n",
    "                matched_gt = set()\n",
    "                \n",
    "                pred_boxes = np.array([p['bbox'] for p in cls_preds])\n",
    "                gt_boxes = np.array([g['bbox'] for g in cls_gts])\n",
    "                \n",
    "                if len(pred_boxes) == 0:\n",
    "                    aps_per_threshold[iou_thresh].append(0.0)\n",
    "                    continue\n",
    "                \n",
    "                iou_matrix = self._compute_iou_matrix(pred_boxes, gt_boxes)\n",
    "                \n",
    "                for i in range(len(cls_preds)):\n",
    "                    img_id = cls_preds[i]['image_id']\n",
    "                    # Find matching GTs from same image\n",
    "                    gt_indices = [j for j, g in enumerate(cls_gts) if g['image_id'] == img_id]\n",
    "                    \n",
    "                    best_iou = 0\n",
    "                    best_gt = -1\n",
    "                    for j in gt_indices:\n",
    "                        if j not in matched_gt and iou_matrix[i, j] > best_iou:\n",
    "                            best_iou = iou_matrix[i, j]\n",
    "                            best_gt = j\n",
    "                    \n",
    "                    if best_iou >= iou_thresh and best_gt >= 0:\n",
    "                        tp[i] = 1\n",
    "                        matched_gt.add(best_gt)\n",
    "                    else:\n",
    "                        fp[i] = 1\n",
    "                \n",
    "                cum_tp = np.cumsum(tp)\n",
    "                cum_fp = np.cumsum(fp)\n",
    "                recalls = cum_tp / len(cls_gts)\n",
    "                precisions = cum_tp / (cum_tp + cum_fp + 1e-7)\n",
    "                \n",
    "                ap = self.compute_ap(precisions, recalls)\n",
    "                aps_per_threshold[iou_thresh].append(ap)\n",
    "        \n",
    "        results = {}\n",
    "        for t in self.iou_thresholds:\n",
    "            if aps_per_threshold[t]:\n",
    "                results[f'AP@{t:.2f}'] = np.mean(aps_per_threshold[t])\n",
    "        \n",
    "        # mAP across all thresholds\n",
    "        all_aps = [v for v in results.values()]\n",
    "        results['mAP@0.5:0.95'] = np.mean(all_aps) if all_aps else 0.0\n",
    "        results['mAP@0.5'] = results.get('AP@0.50', 0.0)\n",
    "        results['mAP@0.75'] = results.get('AP@0.75', 0.0)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c790c3",
   "metadata": {
    "papermill": {
     "duration": 0.012047,
     "end_time": "2026-02-15T16:31:14.958293",
     "exception": false,
     "start_time": "2026-02-15T16:31:14.946246",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluation on real COCO images\n",
    "\n",
    "We evaluate the model on 10 real COCO validation images streamed from Hugging Face. Ground-truth annotations from the dataset serve as reference. With random (untrained) weights, mAP will be near zero â€” this demonstrates the evaluation pipeline rather than model quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4260689a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:31:14.983771Z",
     "iopub.status.busy": "2026-02-15T16:31:14.983619Z",
     "iopub.status.idle": "2026-02-15T16:31:29.582803Z",
     "shell.execute_reply": "2026-02-15T16:31:29.582145Z"
    },
    "papermill": {
     "duration": 14.612803,
     "end_time": "2026-02-15T16:31:29.583277",
     "exception": false,
     "start_time": "2026-02-15T16:31:14.970474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluation with real COCO validation images\n",
    "evaluator = COCOEvaluator()\n",
    "\n",
    "print(\"Streaming 10 COCO validation images for evaluation demo...\")\n",
    "ds = load_dataset('detection-datasets/coco', split='val', streaming=True)\n",
    "\n",
    "model = YOLOv11(num_classes=80)\n",
    "model.eval()\n",
    "\n",
    "for img_id, example in enumerate(ds):\n",
    "    if img_id >= 10:\n",
    "        break\n",
    "\n",
    "    img_pil = example['image'].convert('RGB')\n",
    "    img_np = np.array(img_pil)\n",
    "    h_orig, w_orig = img_np.shape[:2]\n",
    "\n",
    "    # Prepare ground truth (convert COCO [x,y,w,h] to [x1,y1,x2,y2])\n",
    "    bboxes = example['objects']['bbox']\n",
    "    cats = example['objects']['category']\n",
    "\n",
    "    gt_boxes = []\n",
    "    gt_classes = []\n",
    "    for bbox, cat_id in zip(bboxes, cats):\n",
    "        bx, by, bw, bh = bbox\n",
    "        if bw <= 0 or bh <= 0:\n",
    "            continue\n",
    "        gt_boxes.append([bx, by, bx + bw, by + bh])\n",
    "        gt_classes.append(int(cat_id))\n",
    "\n",
    "    if len(gt_boxes) == 0:\n",
    "        continue\n",
    "\n",
    "    gt_boxes = np.array(gt_boxes, dtype=np.float32)\n",
    "    gt_classes = np.array(gt_classes, dtype=np.int64)\n",
    "    evaluator.add_ground_truths(img_id, gt_boxes, gt_classes)\n",
    "\n",
    "    # Letterbox resize and run inference\n",
    "    scale = 640 / max(h_orig, w_orig)\n",
    "    new_w, new_h = int(w_orig * scale), int(h_orig * scale)\n",
    "    resized = np.array(img_pil.resize((new_w, new_h)))\n",
    "    padded = np.full((640, 640, 3), 114, dtype=np.uint8)\n",
    "    pad_w = (640 - new_w) // 2\n",
    "    pad_h = (640 - new_h) // 2\n",
    "    padded[pad_h:pad_h + new_h, pad_w:pad_w + new_w] = resized\n",
    "\n",
    "    img_tensor = torch.from_numpy(padded).permute(2, 0, 1).float().unsqueeze(0) / 255.0\n",
    "    boxes, scores, class_ids = detect(model, img_tensor, conf_thresh=0.1, iou_thresh=0.45)\n",
    "\n",
    "    if len(boxes) > 0:\n",
    "        # Scale boxes back to original image coordinates\n",
    "        pred_boxes = boxes.cpu().numpy()\n",
    "        pred_boxes[:, [0, 2]] = (pred_boxes[:, [0, 2]] - pad_w) / scale\n",
    "        pred_boxes[:, [1, 3]] = (pred_boxes[:, [1, 3]] - pad_h) / scale\n",
    "        evaluator.add_predictions(img_id, pred_boxes, scores.cpu().numpy(),\n",
    "                                  class_ids.cpu().numpy())\n",
    "\n",
    "results = evaluator.evaluate()\n",
    "print(\"\\n=== COCO-Style Evaluation Results (random weights) ===\")\n",
    "for k, v in sorted(results.items()):\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "print(\"\\nNote: Near-zero mAP is expected with random (untrained) weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27abc9f",
   "metadata": {
    "papermill": {
     "duration": 0.013108,
     "end_time": "2026-02-15T16:31:29.609088",
     "exception": false,
     "start_time": "2026-02-15T16:31:29.595980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Grad-CAM: Visualizing Model Attention\n",
    "\n",
    "**Gradient-weighted Class Activation Mapping (Grad-CAM)** provides visual explanations for model predictions by highlighting which spatial regions in the input image contribute most to a particular class prediction.\n",
    "\n",
    "The algorithm:\n",
    "1. Perform a forward pass and record activations at a target convolutional layer (typically the last layer in the backbone).\n",
    "2. Compute the gradient of the target class score with respect to those activations.\n",
    "3. Global-average-pool the gradients to obtain per-channel importance weights.\n",
    "4. Compute a weighted sum of the activation channels, followed by ReLU to keep only positive contributions.\n",
    "5. Upsample the resulting heatmap to the input image size.\n",
    "\n",
    "For object detection, Grad-CAM reveals whether the model is attending to the correct object regions or relying on spurious contextual cues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf3eb44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:31:29.635292Z",
     "iopub.status.busy": "2026-02-15T16:31:29.635121Z",
     "iopub.status.idle": "2026-02-15T16:31:29.645485Z",
     "shell.execute_reply": "2026-02-15T16:31:29.644786Z"
    },
    "papermill": {
     "duration": 0.024336,
     "end_time": "2026-02-15T16:31:29.646040",
     "exception": false,
     "start_time": "2026-02-15T16:31:29.621704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    \"\"\"Gradient-weighted Class Activation Mapping for object detection.\n",
    "    \n",
    "    Visualizes which spatial regions in the image contribute most\n",
    "    to the model's classification decisions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Register hooks\n",
    "        target_layer.register_forward_hook(self._save_activation)\n",
    "        target_layer.register_full_backward_hook(self._save_gradient)\n",
    "    \n",
    "    def _save_activation(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "    \n",
    "    def _save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "    \n",
    "    def generate(self, input_tensor, class_idx=None):\n",
    "        \"\"\"Generate Grad-CAM heatmap.\n",
    "        \n",
    "        Args:\n",
    "            input_tensor: (1, 3, H, W)\n",
    "            class_idx: target class (None = use predicted class)\n",
    "        Returns:\n",
    "            heatmap: (H, W) normalized [0, 1]\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        output = self.model(input_tensor)\n",
    "        \n",
    "        # Use P3 predictions (highest resolution)\n",
    "        cls_pred = output[0][0]  # (1, num_classes, H, W)\n",
    "        \n",
    "        if class_idx is None:\n",
    "            # Use the max activation class\n",
    "            class_idx = cls_pred.sum(dim=(0, 2, 3)).argmax().item()\n",
    "        \n",
    "        # Backward for target class\n",
    "        self.model.zero_grad()\n",
    "        target = cls_pred[0, class_idx].sum()\n",
    "        target.backward(retain_graph=True)\n",
    "        \n",
    "        if self.gradients is None:\n",
    "            print(\"Warning: No gradients captured\")\n",
    "            return np.zeros((input_tensor.shape[2], input_tensor.shape[3]))\n",
    "        \n",
    "        # Weight activations by gradients\n",
    "        weights = self.gradients.mean(dim=(2, 3), keepdim=True)\n",
    "        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
    "        cam = F.relu(cam)\n",
    "        \n",
    "        # Upsample to input size\n",
    "        cam = F.interpolate(cam, size=input_tensor.shape[2:], mode='bilinear', align_corners=False)\n",
    "        cam = cam.squeeze().cpu().numpy()\n",
    "        \n",
    "        # Normalize\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "        return cam\n",
    "\n",
    "\n",
    "def visualize_gradcam(image_tensor, heatmap, alpha=0.5):\n",
    "    \"\"\"Overlay Grad-CAM heatmap on image.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    img = image_tensor[0].permute(1, 2, 0).cpu().numpy()\n",
    "    img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "    \n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title('Input Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(heatmap, cmap='jet')\n",
    "    axes[1].set_title('Grad-CAM Heatmap')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    heatmap_colored = plt.cm.jet(heatmap)[:, :, :3]\n",
    "    overlay = img * (1 - alpha) + heatmap_colored * alpha\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    axes[2].imshow(overlay)\n",
    "    axes[2].set_title('Overlay')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.suptitle('Grad-CAM Visualization', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f217871",
   "metadata": {
    "papermill": {
     "duration": 0.012593,
     "end_time": "2026-02-15T16:31:29.671333",
     "exception": false,
     "start_time": "2026-02-15T16:31:29.658740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Grad-CAM Demo\n",
    "\n",
    "We apply Grad-CAM to the backbone's SPPF output layer to visualize which spatial regions influence the model's class predictions. With random weights, the heatmap will appear noisy; with a trained model, it would highlight object-relevant regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c293df5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:31:29.697905Z",
     "iopub.status.busy": "2026-02-15T16:31:29.697713Z",
     "iopub.status.idle": "2026-02-15T16:31:40.728729Z",
     "shell.execute_reply": "2026-02-15T16:31:40.727901Z"
    },
    "papermill": {
     "duration": 11.046953,
     "end_time": "2026-02-15T16:31:40.731008",
     "exception": false,
     "start_time": "2026-02-15T16:31:29.684055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run Grad-CAM on a real COCO image\n",
    "model = YOLOv11(num_classes=80)\n",
    "cam = GradCAM(model, model.backbone.stage4[-1].cv2)\n",
    "\n",
    "# Stream a real COCO image for Grad-CAM visualization\n",
    "print(\"Loading real COCO image for Grad-CAM...\")\n",
    "ds = load_dataset('detection-datasets/coco', split='val', streaming=True)\n",
    "example = next(iter(ds))\n",
    "img_pil = example['image'].convert('RGB')\n",
    "img_np = np.array(img_pil)\n",
    "h_orig, w_orig = img_np.shape[:2]\n",
    "\n",
    "# Letterbox resize\n",
    "scale = 640 / max(h_orig, w_orig)\n",
    "new_w, new_h = int(w_orig * scale), int(h_orig * scale)\n",
    "resized = np.array(img_pil.resize((new_w, new_h)))\n",
    "padded = np.full((640, 640, 3), 114, dtype=np.uint8)\n",
    "pad_w = (640 - new_w) // 2\n",
    "pad_h = (640 - new_h) // 2\n",
    "padded[pad_h:pad_h + new_h, pad_w:pad_w + new_w] = resized\n",
    "\n",
    "x = torch.from_numpy(padded).permute(2, 0, 1).float().unsqueeze(0) / 255.0\n",
    "x.requires_grad_(True)\n",
    "\n",
    "heatmap = cam.generate(x)\n",
    "visualize_gradcam(x.detach(), heatmap)\n",
    "print(\"Grad-CAM shows which regions influence classification on a real COCO image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36ec1b2",
   "metadata": {
    "papermill": {
     "duration": 0.018764,
     "end_time": "2026-02-15T16:31:40.768381",
     "exception": false,
     "start_time": "2026-02-15T16:31:40.749617",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Performance Benchmarking\n",
    "\n",
    "We measure the end-to-end latency of the inference pipeline (forward pass + decoding + NMS) to establish a CPU baseline. GPU inference would be significantly faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c200fa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:31:40.805517Z",
     "iopub.status.busy": "2026-02-15T16:31:40.805354Z",
     "iopub.status.idle": "2026-02-15T16:31:58.713104Z",
     "shell.execute_reply": "2026-02-15T16:31:58.712284Z"
    },
    "papermill": {
     "duration": 17.927755,
     "end_time": "2026-02-15T16:31:58.713666",
     "exception": false,
     "start_time": "2026-02-15T16:31:40.785911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = YOLOv11(num_classes=80)\n",
    "model.eval()\n",
    "\n",
    "# Use a real COCO image for benchmarking\n",
    "print(\"Loading real COCO image for benchmark...\")\n",
    "ds = load_dataset('detection-datasets/coco', split='val', streaming=True)\n",
    "example = next(iter(ds))\n",
    "img_pil = example['image'].convert('RGB')\n",
    "img_np = np.array(img_pil)\n",
    "h_orig, w_orig = img_np.shape[:2]\n",
    "\n",
    "scale = 640 / max(h_orig, w_orig)\n",
    "new_w, new_h = int(w_orig * scale), int(h_orig * scale)\n",
    "resized = np.array(img_pil.resize((new_w, new_h)))\n",
    "padded = np.full((640, 640, 3), 114, dtype=np.uint8)\n",
    "pad_w = (640 - new_w) // 2\n",
    "pad_h = (640 - new_h) // 2\n",
    "padded[pad_h:pad_h + new_h, pad_w:pad_w + new_w] = resized\n",
    "\n",
    "x = torch.from_numpy(padded).permute(2, 0, 1).float().unsqueeze(0) / 255.0\n",
    "\n",
    "# Warm up\n",
    "for _ in range(3):\n",
    "    with torch.no_grad():\n",
    "        _ = model(x)\n",
    "\n",
    "# Benchmark\n",
    "times = []\n",
    "for _ in range(10):\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(x)\n",
    "        boxes, scores, cids = decode_predictions(predictions, conf_thresh=0.25)[0]\n",
    "        if len(boxes) > 0:\n",
    "            boxes, scores, cids = batched_nms(boxes, scores, cids, 0.45)\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "avg_time = np.mean(times) * 1000\n",
    "print(f\"Average inference time (CPU): {avg_time:.1f} ms\")\n",
    "print(f\"FPS (CPU): {1000/avg_time:.1f}\")\n",
    "print(f\"Note: GPU inference would be significantly faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb84014",
   "metadata": {
    "papermill": {
     "duration": 0.018259,
     "end_time": "2026-02-15T16:31:58.754560",
     "exception": false,
     "start_time": "2026-02-15T16:31:58.736301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook completed the inference and evaluation pipeline for our from-scratch YOLOv11 implementation:\n",
    "\n",
    "- **Prediction decoding** converts raw network outputs (class logits and DFL-encoded LTRB offsets) into bounding boxes in image coordinates through stride-aware anchor point generation.\n",
    "- **Non-Maximum Suppression (NMS)** removes redundant overlapping detections using a greedy algorithm that keeps the highest-confidence prediction for each object, applied independently per class.\n",
    "- **COCO mAP evaluation** provides standardized metrics by computing Average Precision across multiple IoU thresholds (0.50 to 0.95), capturing both localization accuracy and classification performance.\n",
    "- **Grad-CAM visualization** offers model interpretability by highlighting which spatial regions drive the network's predictions.\n",
    "\n",
    "### Series recap\n",
    "\n",
    "This concludes the 5-notebook series building YOLOv11 from scratch:\n",
    "\n",
    "1. **Notebook 1** --- COCO data loading and augmentation pipeline.\n",
    "2. **Notebook 2** --- Backbone architecture: ConvBNSiLU, C3k2 (CSP bottleneck), SPPF for multi-scale feature extraction.\n",
    "3. **Notebook 3** --- Neck and head: FPN (top-down) + PAN (bottom-up) feature fusion, C2PSA attention, and DFL-based decoupled detection heads.\n",
    "4. **Notebook 4** --- Loss functions and training: Task-Aligned Label Assignment, BCE classification loss, CIoU + DFL regression loss, and the training loop.\n",
    "5. **Notebook 5** --- Inference, NMS, COCO evaluation, and Grad-CAM (this notebook).\n",
    "\n",
    "**Key architectural takeaways**:\n",
    "- Anchor-free detection eliminates the need for hand-designed anchor priors.\n",
    "- C3k2 blocks with cross-stage partial connections enable efficient feature reuse.\n",
    "- SPPF provides multi-scale receptive fields with minimal computational overhead.\n",
    "- Bidirectional FPN+PAN neck ensures both semantic and localization information flows across all scales.\n",
    "- DFL (Distribution Focal Loss) models boundary uncertainty through discrete distributions, improving localization precision.\n",
    "- Task-Aligned Assignment dynamically matches predictions to ground truths based on both classification and localization quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 65.794619,
   "end_time": "2026-02-15T16:31:59.591000",
   "environment_variables": {},
   "exception": null,
   "input_path": "notebooks/scene-understanding/object-detection/yolo/pytorch/05_inference_and_evaluation.ipynb",
   "output_path": "notebooks/scene-understanding/object-detection/yolo/pytorch/05_inference_and_evaluation-executed.ipynb",
   "parameters": {},
   "start_time": "2026-02-15T16:30:53.796381",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
