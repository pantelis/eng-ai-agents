{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66e9557",
   "metadata": {
    "papermill": {
     "duration": 0.007836,
     "end_time": "2026-02-14T07:59:56.042801",
     "exception": false,
     "start_time": "2026-02-14T07:59:56.034965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Building the YOLOv11 Backbone\n",
    "\n",
    "*Notebook 2 of 5 in the YOLOv11 from-scratch series*\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The backbone is the feature extraction engine of any object detection model. In YOLOv11, the backbone extracts **hierarchical features at multiple spatial scales**, enabling the detector to find objects ranging from small pedestrians to large vehicles in a single forward pass.\n",
    "\n",
    "### Key innovations in the YOLOv11 backbone\n",
    "\n",
    "1. **C3k2 block** - A Cross Stage Partial (CSP) bottleneck that uses 2 convolutions instead of 3. It splits the input channels, processes one branch through a series of bottleneck blocks, collects intermediate outputs, concatenates everything, and projects back. This is more parameter-efficient than the older C3 block while achieving similar representational power.\n",
    "\n",
    "2. **SPPF (Spatial Pyramid Pooling - Fast)** - Applies three sequential 5x5 max-pooling operations (equivalent to 5x5, 9x9, and 13x13 receptive fields) to capture multi-scale contextual information without increasing computational cost significantly.\n",
    "\n",
    "3. **Multi-scale outputs** - The backbone produces three feature maps at different resolutions:\n",
    "   - **P3**: stride 8 (80x80 for 640x640 input) - fine-grained features for small objects\n",
    "   - **P4**: stride 16 (40x40) - mid-level features for medium objects\n",
    "   - **P5**: stride 32 (20x20) - coarse features with large receptive field for large objects\n",
    "\n",
    "By the end of this notebook, you will have a fully functional YOLOv11 backbone implemented from scratch in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Colab Environment Setup ---\n",
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    %pip install -q matplotlib seaborn scikit-learn scipy tqdm\n",
    "    print(\"Colab dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a23a2c",
   "metadata": {
    "papermill": {
     "duration": 0.003886,
     "end_time": "2026-02-14T07:59:56.052926",
     "exception": false,
     "start_time": "2026-02-14T07:59:56.049040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ceede",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T07:59:56.058320Z",
     "iopub.status.busy": "2026-02-14T07:59:56.058157Z",
     "iopub.status.idle": "2026-02-14T07:59:57.350956Z",
     "shell.execute_reply": "2026-02-14T07:59:57.350267Z"
    },
    "papermill": {
     "duration": 1.296338,
     "end_time": "2026-02-14T07:59:57.351657",
     "exception": false,
     "start_time": "2026-02-14T07:59:56.055319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f833ece2",
   "metadata": {
    "papermill": {
     "duration": 0.002112,
     "end_time": "2026-02-14T07:59:57.356101",
     "exception": false,
     "start_time": "2026-02-14T07:59:57.353989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Building blocks: Conv-BN-SiLU\n",
    "\n",
    "Every convolutional layer in modern YOLO architectures follows the same pattern:\n",
    "\n",
    "1. **Convolution** (`nn.Conv2d`) - the learnable spatial filter, with `bias=False` since batch normalization handles the bias term.\n",
    "2. **Batch Normalization** (`nn.BatchNorm2d`) - normalizes activations across the batch, stabilizing training and allowing higher learning rates.\n",
    "3. **SiLU activation** (also known as Swish: $f(x) = x \\cdot \\sigma(x)$) - a smooth, non-monotonic activation that consistently outperforms ReLU in detection tasks.\n",
    "\n",
    "This pattern is so pervasive that we encapsulate it in a single `ConvBNSiLU` module. The `padding` parameter defaults to `kernel_size // 2`, which preserves spatial dimensions for odd kernel sizes (the standard choice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5992b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T07:59:57.360412Z",
     "iopub.status.busy": "2026-02-14T07:59:57.360201Z",
     "iopub.status.idle": "2026-02-14T07:59:57.363959Z",
     "shell.execute_reply": "2026-02-14T07:59:57.363323Z"
    },
    "papermill": {
     "duration": 0.006498,
     "end_time": "2026-02-14T07:59:57.364314",
     "exception": false,
     "start_time": "2026-02-14T07:59:57.357816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvBNSiLU(nn.Module):\n",
    "    \"\"\"Standard Conv + BatchNorm + SiLU (Swish) activation block.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 1,\n",
    "                 stride: int = 1, padding: int = None, groups: int = 1):\n",
    "        super().__init__()\n",
    "        if padding is None:\n",
    "            padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride,\n",
    "                              padding, groups=groups, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15c2aaa",
   "metadata": {
    "papermill": {
     "duration": 0.001822,
     "end_time": "2026-02-14T07:59:57.368167",
     "exception": false,
     "start_time": "2026-02-14T07:59:57.366345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bottleneck block\n",
    "\n",
    "The `Bottleneck` is the fundamental processing unit inside CSP blocks. It consists of two convolutions:\n",
    "\n",
    "1. A **squeeze** convolution that reduces channels by the `expansion` factor (default 0.5).\n",
    "2. An **expand** convolution that restores the channel count.\n",
    "\n",
    "When `shortcut=True` and the input/output channel counts match, a **residual connection** adds the input directly to the output. This identity shortcut helps gradients flow through deep networks and has been a cornerstone of modern architectures since ResNet.\n",
    "\n",
    "The `kernel_size` parameter accepts a tuple `(k1, k2)` to independently set the kernel size for each convolution. YOLOv11's C3k2 block uses `(3, 3)` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34620c94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T07:59:57.372691Z",
     "iopub.status.busy": "2026-02-14T07:59:57.372536Z",
     "iopub.status.idle": "2026-02-14T07:59:57.376225Z",
     "shell.execute_reply": "2026-02-14T07:59:57.375730Z"
    },
    "papermill": {
     "duration": 0.006844,
     "end_time": "2026-02-14T07:59:57.376812",
     "exception": false,
     "start_time": "2026-02-14T07:59:57.369968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"Standard bottleneck with optional residual connection.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, shortcut: bool = True,\n",
    "                 kernel_size: Tuple[int, int] = (3, 3), expansion: float = 0.5):\n",
    "        super().__init__()\n",
    "        hidden = int(out_channels * expansion)\n",
    "        self.cv1 = ConvBNSiLU(in_channels, hidden, kernel_size[0])\n",
    "        self.cv2 = ConvBNSiLU(hidden, out_channels, kernel_size[1])\n",
    "        self.add = shortcut and in_channels == out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f1531e",
   "metadata": {
    "papermill": {
     "duration": 0.001626,
     "end_time": "2026-02-14T07:59:57.380838",
     "exception": false,
     "start_time": "2026-02-14T07:59:57.379212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## C3k2 block: CSP with 2 convolutions\n",
    "\n",
    "The **C3k2** (Cross Stage Partial with 2 convolutions) block is a key architectural element of YOLOv11. It improves upon earlier CSP designs (C3, C2f) by being more parameter-efficient.\n",
    "\n",
    "### How CSP works\n",
    "\n",
    "The Cross Stage Partial (CSP) design philosophy is:\n",
    "\n",
    "1. **Split**: A 1x1 convolution (`cv1`) projects the input into `2 * hidden_channels`, then the output is split (chunked) into two equal halves along the channel dimension.\n",
    "2. **Transform**: One half passes through a series of `n` bottleneck blocks. Crucially, **each bottleneck's output is collected** (not just the final one), creating a dense connection pattern.\n",
    "3. **Concatenate**: The original split half, plus all `n` bottleneck outputs (total of `2 + n` feature groups), are concatenated along the channel dimension.\n",
    "4. **Project**: A final 1x1 convolution (`cv2`) fuses the concatenated features back to the desired output channel count.\n",
    "\n",
    "The \"2 convolutions\" in C3k2 refers to the two projection convolutions (`cv1` and `cv2`), distinguishing it from C3 which uses three. The default kernel size pair `(3, 3)` in each bottleneck gives the block its name suffix \"k2\" (2 kernels of size 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd71fcf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T07:59:57.385218Z",
     "iopub.status.busy": "2026-02-14T07:59:57.385059Z",
     "iopub.status.idle": "2026-02-14T07:59:57.389041Z",
     "shell.execute_reply": "2026-02-14T07:59:57.388588Z"
    },
    "papermill": {
     "duration": 0.007009,
     "end_time": "2026-02-14T07:59:57.389479",
     "exception": false,
     "start_time": "2026-02-14T07:59:57.382470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class C3k2(nn.Module):\n",
    "    \"\"\"CSP Bottleneck with 2 convolutions (YOLOv11 variant).\n",
    "\n",
    "    Splits input channels, processes one part through bottleneck blocks,\n",
    "    concatenates, and projects back. More efficient than C3 with similar performance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, n: int = 1,\n",
    "                 shortcut: bool = True, expansion: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.c = int(out_channels * expansion)  # hidden channels\n",
    "        self.cv1 = ConvBNSiLU(in_channels, 2 * self.c, 1)\n",
    "        self.cv2 = ConvBNSiLU((2 + n) * self.c, out_channels, 1)\n",
    "        self.bottlenecks = nn.ModuleList(\n",
    "            Bottleneck(self.c, self.c, shortcut, kernel_size=(3, 3), expansion=1.0)\n",
    "            for _ in range(n)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split into two branches\n",
    "        y = list(self.cv1(x).chunk(2, dim=1))\n",
    "        # Pass through sequential bottlenecks, collecting outputs\n",
    "        for bn in self.bottlenecks:\n",
    "            y.append(bn(y[-1]))\n",
    "        return self.cv2(torch.cat(y, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7438dfd",
   "metadata": {
    "papermill": {
     "duration": 0.001622,
     "end_time": "2026-02-14T07:59:57.392787",
     "exception": false,
     "start_time": "2026-02-14T07:59:57.391165",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SPPF: Spatial Pyramid Pooling - Fast\n",
    "\n",
    "The **SPPF** (Spatial Pyramid Pooling - Fast) module addresses a fundamental challenge: how to capture context at multiple spatial scales without drastically increasing computation.\n",
    "\n",
    "### Design\n",
    "\n",
    "The original SPP module applied max-pooling with three different kernel sizes (5, 9, 13) in parallel. SPPF achieves the **same effective receptive fields** by applying a single 5x5 max-pool operation **three times sequentially**:\n",
    "\n",
    "- After 1 pool: effective receptive field of 5x5\n",
    "- After 2 pools: effective receptive field of 9x9\n",
    "- After 3 pools: effective receptive field of 13x13\n",
    "\n",
    "The four feature maps (original + 3 pooled versions) are concatenated and projected through a 1x1 convolution. Using `stride=1` and `padding=k//2` preserves the spatial dimensions throughout.\n",
    "\n",
    "This sequential design is faster than parallel pooling because it reuses intermediate results, and it is applied only at the deepest stage of the backbone where feature maps are smallest (20x20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93fa7f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T07:59:57.396772Z",
     "iopub.status.busy": "2026-02-14T07:59:57.396640Z",
     "iopub.status.idle": "2026-02-14T07:59:57.400185Z",
     "shell.execute_reply": "2026-02-14T07:59:57.399374Z"
    },
    "papermill": {
     "duration": 0.006278,
     "end_time": "2026-02-14T07:59:57.400640",
     "exception": false,
     "start_time": "2026-02-14T07:59:57.394362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SPPF(nn.Module):\n",
    "    \"\"\"Spatial Pyramid Pooling - Fast (SPPF).\n",
    "\n",
    "    Three sequential 5x5 max-pool operations (equivalent to 5x5, 9x9, 13x13 pooling)\n",
    "    capture multi-scale context efficiently.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, k: int = 5):\n",
    "        super().__init__()\n",
    "        hidden = in_channels // 2\n",
    "        self.cv1 = ConvBNSiLU(in_channels, hidden, 1)\n",
    "        self.cv2 = ConvBNSiLU(hidden * 4, out_channels, 1)\n",
    "        self.pool = nn.MaxPool2d(k, stride=1, padding=k // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cv1(x)\n",
    "        y1 = self.pool(x)\n",
    "        y2 = self.pool(y1)\n",
    "        y3 = self.pool(y2)\n",
    "        return self.cv2(torch.cat([x, y1, y2, y3], dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc73fe7",
   "metadata": {
    "papermill": {
     "duration": 0.001645,
     "end_time": "2026-02-14T07:59:57.403949",
     "exception": false,
     "start_time": "2026-02-14T07:59:57.402304",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Full backbone assembly\n",
    "\n",
    "Now we assemble the complete YOLOv11 backbone by stacking the building blocks we have defined. The backbone is organized into a **stem** followed by **four stages**, each performing spatial downsampling (stride 2) and feature refinement:\n",
    "\n",
    "| Component | Operation | Output Shape | Notes |\n",
    "|-----------|-----------|-------------|-------|\n",
    "| **Stem** | Conv 3x3, s=2 | 64 x 320 x 320 | Initial feature extraction |\n",
    "| **Stage 1** | Conv 3x3, s=2 + C3k2(n=2) | 128 x 160 x 160 | Low-level features |\n",
    "| **Stage 2** | Conv 3x3, s=2 + C3k2(n=2) | 256 x 80 x 80 | **P3 output** (stride 8) |\n",
    "| **Stage 3** | Conv 3x3, s=2 + C3k2(n=2) | 512 x 40 x 40 | **P4 output** (stride 16) |\n",
    "| **Stage 4** | Conv 3x3, s=2 + C3k2(n=2) + SPPF | 1024 x 20 x 20 | **P5 output** (stride 32) |\n",
    "\n",
    "The three outputs (P3, P4, P5) form a **feature pyramid** that will be further refined by the neck (FPN/PAN) in the next notebook. Small objects are detected at P3 (high resolution, low-level features), while large objects are detected at P5 (low resolution, high-level semantic features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494ac61d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T07:59:57.408083Z",
     "iopub.status.busy": "2026-02-14T07:59:57.407961Z",
     "iopub.status.idle": "2026-02-14T07:59:57.412952Z",
     "shell.execute_reply": "2026-02-14T07:59:57.412514Z"
    },
    "papermill": {
     "duration": 0.007713,
     "end_time": "2026-02-14T07:59:57.413300",
     "exception": false,
     "start_time": "2026-02-14T07:59:57.405587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class YOLOv11Backbone(nn.Module):\n",
    "    \"\"\"YOLOv11 backbone producing P3, P4, P5 feature maps.\n",
    "\n",
    "    Architecture:\n",
    "        Stem (3->64) -> Stage1 (64->128) -> Stage2 (128->256, P3)\n",
    "        -> Stage3 (256->512, P4) -> Stage4 (512->1024) -> SPPF (P5)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int = 3, base_channels: int = 64):\n",
    "        super().__init__()\n",
    "        c1 = base_channels       # 64\n",
    "        c2 = c1 * 2              # 128\n",
    "        c3 = c2 * 2              # 256\n",
    "        c4 = c3 * 2              # 512\n",
    "        c5 = c4 * 2              # 1024\n",
    "\n",
    "        # Stem\n",
    "        self.stem = ConvBNSiLU(in_channels, c1, 3, stride=2)\n",
    "\n",
    "        # Stage 1: downsample + C3k2\n",
    "        self.stage1_down = ConvBNSiLU(c1, c2, 3, stride=2)\n",
    "        self.stage1_c3k2 = C3k2(c2, c2, n=2, shortcut=True)\n",
    "\n",
    "        # Stage 2: downsample + C3k2 -> P3 output\n",
    "        self.stage2_down = ConvBNSiLU(c2, c3, 3, stride=2)\n",
    "        self.stage2_c3k2 = C3k2(c3, c3, n=2, shortcut=True)\n",
    "\n",
    "        # Stage 3: downsample + C3k2 -> P4 output\n",
    "        self.stage3_down = ConvBNSiLU(c3, c4, 3, stride=2)\n",
    "        self.stage3_c3k2 = C3k2(c4, c4, n=2, shortcut=True)\n",
    "\n",
    "        # Stage 4: downsample + C3k2 + SPPF -> P5 output\n",
    "        self.stage4_down = ConvBNSiLU(c4, c5, 3, stride=2)\n",
    "        self.stage4_c3k2 = C3k2(c5, c5, n=2, shortcut=True)\n",
    "        self.sppf = SPPF(c5, c5)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass returning multi-scale features.\n",
    "\n",
    "        Args:\n",
    "            x: (B, 3, 640, 640) input images\n",
    "        Returns:\n",
    "            p3: (B, 256, 80, 80) - stride 8\n",
    "            p4: (B, 512, 40, 40) - stride 16\n",
    "            p5: (B, 1024, 20, 20) - stride 32\n",
    "        \"\"\"\n",
    "        # Stem: 640 -> 320\n",
    "        x = self.stem(x)\n",
    "\n",
    "        # Stage 1: 320 -> 160\n",
    "        x = self.stage1_c3k2(self.stage1_down(x))\n",
    "\n",
    "        # Stage 2: 160 -> 80 (P3)\n",
    "        x = self.stage2_c3k2(self.stage2_down(x))\n",
    "        p3 = x  # 256 channels, 80x80\n",
    "\n",
    "        # Stage 3: 80 -> 40 (P4)\n",
    "        x = self.stage3_c3k2(self.stage3_down(x))\n",
    "        p4 = x  # 512 channels, 40x40\n",
    "\n",
    "        # Stage 4: 40 -> 20 (P5)\n",
    "        x = self.stage4_c3k2(self.stage4_down(x))\n",
    "        p5 = self.sppf(x)  # 1024 channels, 20x20\n",
    "\n",
    "        return p3, p4, p5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70a6389",
   "metadata": {
    "papermill": {
     "duration": 0.001692,
     "end_time": "2026-02-14T07:59:57.416748",
     "exception": false,
     "start_time": "2026-02-14T07:59:57.415056",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Shape verification\n",
    "\n",
    "Let us instantiate the backbone and verify that the output feature maps have the expected shapes. This is a critical sanity check: if the shapes are wrong, the downstream neck and head will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee60b42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T07:59:57.421156Z",
     "iopub.status.busy": "2026-02-14T07:59:57.420990Z",
     "iopub.status.idle": "2026-02-14T07:59:57.749411Z",
     "shell.execute_reply": "2026-02-14T07:59:57.748770Z"
    },
    "papermill": {
     "duration": 0.331587,
     "end_time": "2026-02-14T07:59:57.749993",
     "exception": false,
     "start_time": "2026-02-14T07:59:57.418406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify output shapes\n",
    "backbone = YOLOv11Backbone()\n",
    "dummy_input = torch.randn(1, 3, 640, 640)\n",
    "\n",
    "with torch.no_grad():\n",
    "    p3, p4, p5 = backbone(dummy_input)\n",
    "\n",
    "print(\"Input shape:\", dummy_input.shape)\n",
    "print(f\"P3 shape: {p3.shape}  (stride 8,  {p3.shape[1]} channels)\")\n",
    "print(f\"P4 shape: {p4.shape}  (stride 16, {p4.shape[1]} channels)\")\n",
    "print(f\"P5 shape: {p5.shape}  (stride 32, {p5.shape[1]} channels)\")\n",
    "\n",
    "# Verify spatial dimensions\n",
    "assert p3.shape == (1, 256, 80, 80), f\"P3 expected (1, 256, 80, 80), got {p3.shape}\"\n",
    "assert p4.shape == (1, 512, 40, 40), f\"P4 expected (1, 512, 40, 40), got {p4.shape}\"\n",
    "assert p5.shape == (1, 1024, 20, 20), f\"P5 expected (1, 1024, 20, 20), got {p5.shape}\"\n",
    "print(\"\\nAll shape checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959a1ff6",
   "metadata": {
    "papermill": {
     "duration": 0.002461,
     "end_time": "2026-02-14T07:59:57.755583",
     "exception": false,
     "start_time": "2026-02-14T07:59:57.753122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Parameter count\n",
    "\n",
    "Understanding the parameter distribution across stages helps with model analysis and debugging. The later stages have exponentially more parameters due to the doubling of channel widths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4429e739",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T07:59:57.760572Z",
     "iopub.status.busy": "2026-02-14T07:59:57.760350Z",
     "iopub.status.idle": "2026-02-14T07:59:57.767900Z",
     "shell.execute_reply": "2026-02-14T07:59:57.767261Z"
    },
    "papermill": {
     "duration": 0.010918,
     "end_time": "2026-02-14T07:59:57.768466",
     "exception": false,
     "start_time": "2026-02-14T07:59:57.757548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable and total parameters.\"\"\"\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total:,}\")\n",
    "    print(f\"Trainable parameters: {trainable:,}\")\n",
    "    print(f\"Size (MB): {total * 4 / 1024 / 1024:.1f}\")\n",
    "    return total\n",
    "\n",
    "# Per-stage breakdown\n",
    "print(\"=== Parameter Breakdown ===\")\n",
    "for name, module in backbone.named_children():\n",
    "    params = sum(p.numel() for p in module.parameters())\n",
    "    print(f\"  {name}: {params:,}\")\n",
    "print()\n",
    "count_parameters(backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7fe6fb",
   "metadata": {
    "papermill": {
     "duration": 0.002281,
     "end_time": "2026-02-14T07:59:57.773035",
     "exception": false,
     "start_time": "2026-02-14T07:59:57.770754",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature map visualization\n",
    "\n",
    "Visualizing the feature maps at each scale gives intuition for what the backbone learns. Even with random weights, we can observe that:\n",
    "\n",
    "- **P3** (80x80) retains fine spatial detail\n",
    "- **P4** (40x40) captures medium-scale structure\n",
    "- **P5** (20x20) shows coarse, high-level patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2084bb54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T07:59:57.778378Z",
     "iopub.status.busy": "2026-02-14T07:59:57.778206Z",
     "iopub.status.idle": "2026-02-14T07:59:58.384056Z",
     "shell.execute_reply": "2026-02-14T07:59:58.383540Z"
    },
    "papermill": {
     "duration": 0.613987,
     "end_time": "2026-02-14T07:59:58.389106",
     "exception": false,
     "start_time": "2026-02-14T07:59:57.775119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_feature_maps(features, names, num_channels=8):\n",
    "    \"\"\"Visualize first few channels of each feature map.\"\"\"\n",
    "    fig, axes = plt.subplots(len(features), num_channels, figsize=(20, 3 * len(features)))\n",
    "\n",
    "    for i, (feat, name) in enumerate(zip(features, names)):\n",
    "        feat_np = feat[0].detach().numpy()  # Remove batch dim\n",
    "        for j in range(min(num_channels, feat_np.shape[0])):\n",
    "            ax = axes[i, j] if len(features) > 1 else axes[j]\n",
    "            ax.imshow(feat_np[j], cmap='viridis')\n",
    "            ax.axis('off')\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(name, fontsize=12, rotation=0, labelpad=60)\n",
    "\n",
    "    plt.suptitle('Feature Map Activations (first 8 channels per scale)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate with random input for visualization\n",
    "with torch.no_grad():\n",
    "    # Use structured input so feature maps are more interesting\n",
    "    x = torch.randn(1, 3, 640, 640)\n",
    "    p3, p4, p5 = backbone(x)\n",
    "\n",
    "visualize_feature_maps([p3, p4, p5], ['P3 (80x80)', 'P4 (40x40)', 'P5 (20x20)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69344a1",
   "metadata": {
    "papermill": {
     "duration": 0.00959,
     "end_time": "2026-02-14T07:59:58.408265",
     "exception": false,
     "start_time": "2026-02-14T07:59:58.398675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Architecture diagram\n",
    "\n",
    "The following diagram summarizes the complete backbone data flow:\n",
    "\n",
    "```\n",
    "Input (3x640x640)\n",
    "      |\n",
    "   +------+\n",
    "   | Stem |  Conv 3x3, s=2\n",
    "   +------+  -> 64x320x320\n",
    "      |\n",
    "   +------+\n",
    "   |  S1  |  Conv 3x3, s=2 -> C3k2(n=2)\n",
    "   +------+  -> 128x160x160\n",
    "      |\n",
    "   +------+\n",
    "   |  S2  |  Conv 3x3, s=2 -> C3k2(n=2)\n",
    "   +------+  -> 256x80x80 --------> P3\n",
    "      |\n",
    "   +------+\n",
    "   |  S3  |  Conv 3x3, s=2 -> C3k2(n=2)\n",
    "   +------+  -> 512x40x40 --------> P4\n",
    "      |\n",
    "   +------+\n",
    "   |  S4  |  Conv 3x3, s=2 -> C3k2(n=2) -> SPPF\n",
    "   +------+  -> 1024x20x20 -------> P5\n",
    "```\n",
    "\n",
    "Each stage doubles the channel count while halving the spatial resolution. The SPPF module is applied only at the deepest level where the computational overhead is minimal but the benefit of multi-scale pooling is greatest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7bd03c",
   "metadata": {
    "papermill": {
     "duration": 0.00969,
     "end_time": "2026-02-14T07:59:58.429158",
     "exception": false,
     "start_time": "2026-02-14T07:59:58.419468",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we built the complete YOLOv11 backbone from scratch. Here is a recap of the key design choices:\n",
    "\n",
    "1. **ConvBNSiLU** provides a clean, reusable primitive that appears throughout the architecture. Disabling the convolution bias (since batch normalization subsumes it) saves parameters.\n",
    "\n",
    "2. **Bottleneck** blocks with residual connections enable deeper networks without vanishing gradients. The expansion factor controls the compute/accuracy tradeoff.\n",
    "\n",
    "3. **C3k2** (CSP with 2 convolutions) is more parameter-efficient than C3 while maintaining strong feature extraction. The dense connections (collecting all bottleneck outputs) improve gradient flow and feature reuse.\n",
    "\n",
    "4. **SPPF** captures multi-scale context through sequential max-pooling, enriching the deepest feature map with information from multiple receptive field sizes.\n",
    "\n",
    "5. The **multi-scale output** design (P3, P4, P5) is essential for detecting objects of varying sizes. This feature pyramid will be further refined in the next notebook.\n",
    "\n",
    "### Next steps\n",
    "\n",
    "In **Notebook 3**, we will build the **FPN/PAN neck** that fuses these multi-scale features bidirectionally, and the **detection head** that produces bounding box predictions and class scores at each scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.753338,
   "end_time": "2026-02-14T07:59:59.058770",
   "environment_variables": {},
   "exception": null,
   "input_path": "notebooks/scene-understanding/object-detection/yolo/pytorch/02_backbone.ipynb",
   "output_path": "notebooks/scene-understanding/object-detection/yolo/pytorch/02_backbone-executed.ipynb",
   "parameters": {},
   "start_time": "2026-02-14T07:59:55.305432",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
