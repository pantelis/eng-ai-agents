{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a840bf50",
   "metadata": {
    "papermill": {
     "duration": 0.007906,
     "end_time": "2026-02-14T08:02:14.880180",
     "exception": false,
     "start_time": "2026-02-14T08:02:14.872274",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Aggregation and Anchor-Free Detection Head\n",
    "\n",
    "**Notebook 3 of 5 in the YOLOv11 From-Scratch Series**\n",
    "\n",
    "In this notebook we build the **neck** and **detection head** of YOLOv11. The backbone (Notebook 2) produces multi-scale feature maps P3, P4, and P5, but these features are not yet ready for detection:\n",
    "\n",
    "- **Deep features (P5)** have strong semantics but poor spatial resolution.\n",
    "- **Shallow features (P3)** have fine spatial detail but weak semantics.\n",
    "\n",
    "The neck bridges this gap through bidirectional feature fusion:\n",
    "\n",
    "1. **FPN (Feature Pyramid Network)** --- top-down pathway that propagates high-level semantic information to lower-level features.\n",
    "2. **PAN (Path Aggregation Network)** --- bottom-up pathway that propagates strong localization signals back up.\n",
    "3. **C2PSA (Channel Attention)** --- lightweight partial self-attention for feature refinement.\n",
    "\n",
    "On top of the fused features, a **decoupled anchor-free detection head** independently predicts:\n",
    "\n",
    "- **Classification logits** --- probability distribution over object classes.\n",
    "- **Box regression offsets** --- encoded via **Distribution Focal Loss (DFL)** for precise localization.\n",
    "\n",
    "By the end of this notebook, you will have a complete, forward-passable YOLOv11 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8580e3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Colab Environment Setup ---\n",
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    %pip install -q matplotlib seaborn scikit-learn scipy tqdm\n",
    "    print(\"Colab dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84521742",
   "metadata": {
    "papermill": {
     "duration": 0.003879,
     "end_time": "2026-02-14T08:02:14.889666",
     "exception": false,
     "start_time": "2026-02-14T08:02:14.885787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcea485",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T08:02:14.894726Z",
     "iopub.status.busy": "2026-02-14T08:02:14.894390Z",
     "iopub.status.idle": "2026-02-14T08:02:16.258228Z",
     "shell.execute_reply": "2026-02-14T08:02:16.257707Z"
    },
    "papermill": {
     "duration": 1.367446,
     "end_time": "2026-02-14T08:02:16.259003",
     "exception": false,
     "start_time": "2026-02-14T08:02:14.891557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da19992",
   "metadata": {
    "papermill": {
     "duration": 0.002005,
     "end_time": "2026-02-14T08:02:16.263542",
     "exception": false,
     "start_time": "2026-02-14T08:02:16.261537",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Backbone building blocks (from Notebook 2)\n",
    "\n",
    "The following cells re-define the backbone building blocks introduced in Notebook 2. They are reproduced here in compact form so that this notebook is fully self-contained. Refer to Notebook 2 for detailed explanations of each module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0217353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T08:02:16.268350Z",
     "iopub.status.busy": "2026-02-14T08:02:16.268127Z",
     "iopub.status.idle": "2026-02-14T08:02:16.276991Z",
     "shell.execute_reply": "2026-02-14T08:02:16.276522Z"
    },
    "papermill": {
     "duration": 0.012399,
     "end_time": "2026-02-14T08:02:16.277696",
     "exception": false,
     "start_time": "2026-02-14T08:02:16.265297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvBNSiLU(nn.Module):\n",
    "    def __init__(self, in_c, out_c, k=1, s=1, p=None, g=1):\n",
    "        super().__init__()\n",
    "        if p is None:\n",
    "            p = k // 2\n",
    "        self.conv = nn.Conv2d(in_c, out_c, k, s, p, groups=g, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_c)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_c, out_c, shortcut=True, e=0.5):\n",
    "        super().__init__()\n",
    "        mid = int(out_c * e)\n",
    "        self.cv1 = ConvBNSiLU(in_c, mid, 3)\n",
    "        self.cv2 = ConvBNSiLU(mid, out_c, 3)\n",
    "        self.add = shortcut and in_c == out_c\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.cv2(self.cv1(x))\n",
    "        return x + y if self.add else y\n",
    "\n",
    "\n",
    "class C3k2(nn.Module):\n",
    "    def __init__(self, in_c, out_c, n=2, shortcut=True, e=0.5):\n",
    "        super().__init__()\n",
    "        self.c = int(out_c * e)\n",
    "        self.cv1 = ConvBNSiLU(in_c, 2 * self.c, 1)\n",
    "        self.cv2 = ConvBNSiLU((2 + n) * self.c, out_c, 1)\n",
    "        self.bottlenecks = nn.ModuleList(\n",
    "            Bottleneck(self.c, self.c, shortcut) for _ in range(n)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = list(self.cv1(x).chunk(2, dim=1))\n",
    "        for bn in self.bottlenecks:\n",
    "            y.append(bn(y[-1]))\n",
    "        return self.cv2(torch.cat(y, dim=1))\n",
    "\n",
    "\n",
    "class SPPF(nn.Module):\n",
    "    def __init__(self, in_c, out_c, k=5):\n",
    "        super().__init__()\n",
    "        mid = in_c // 2\n",
    "        self.cv1 = ConvBNSiLU(in_c, mid, 1)\n",
    "        self.cv2 = ConvBNSiLU(mid * 4, out_c, 1)\n",
    "        self.pool = nn.MaxPool2d(k, stride=1, padding=k // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cv1(x)\n",
    "        y1 = self.pool(x)\n",
    "        y2 = self.pool(y1)\n",
    "        y3 = self.pool(y2)\n",
    "        return self.cv2(torch.cat([x, y1, y2, y3], dim=1))\n",
    "\n",
    "\n",
    "class YOLOv11Backbone(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_channels=64):\n",
    "        super().__init__()\n",
    "        c1, c2, c3, c4, c5 = (\n",
    "            base_channels, base_channels * 2, base_channels * 4,\n",
    "            base_channels * 8, base_channels * 16,\n",
    "        )\n",
    "        self.stem = ConvBNSiLU(in_channels, c1, 3, 2)\n",
    "        self.stage1 = nn.Sequential(ConvBNSiLU(c1, c2, 3, 2), C3k2(c2, c2, n=2))\n",
    "        self.stage2 = nn.Sequential(ConvBNSiLU(c2, c3, 3, 2), C3k2(c3, c3, n=2))\n",
    "        self.stage3 = nn.Sequential(ConvBNSiLU(c3, c4, 3, 2), C3k2(c4, c4, n=2))\n",
    "        self.stage4 = nn.Sequential(ConvBNSiLU(c4, c5, 3, 2), C3k2(c5, c5, n=2), SPPF(c5, c5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        p3 = self.stage2(x)\n",
    "        p4 = self.stage3(p3)\n",
    "        p5 = self.stage4(p4)\n",
    "        return p3, p4, p5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2701a411",
   "metadata": {
    "papermill": {
     "duration": 0.001979,
     "end_time": "2026-02-14T08:02:16.281717",
     "exception": false,
     "start_time": "2026-02-14T08:02:16.279738",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## FPN: Top-Down Path\n",
    "\n",
    "The **Feature Pyramid Network (FPN)** implements a top-down pathway that enriches lower-resolution, semantically strong features with higher-resolution spatial information. The process works as follows:\n",
    "\n",
    "1. **P5 is upsampled** (nearest-neighbor interpolation) and concatenated with P4. A C3k2 block fuses the concatenated features.\n",
    "2. **The fused P4 is upsampled** and concatenated with P3. Another C3k2 block produces the final FPN P3 output.\n",
    "\n",
    "After the FPN, every level in the pyramid carries both high-level semantics from deeper layers and fine-grained spatial detail from shallower layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7558764",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T08:02:16.287467Z",
     "iopub.status.busy": "2026-02-14T08:02:16.287298Z",
     "iopub.status.idle": "2026-02-14T08:02:16.291005Z",
     "shell.execute_reply": "2026-02-14T08:02:16.290573Z"
    },
    "papermill": {
     "duration": 0.007911,
     "end_time": "2026-02-14T08:02:16.291580",
     "exception": false,
     "start_time": "2026-02-14T08:02:16.283669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FPN(nn.Module):\n",
    "    \"\"\"Feature Pyramid Network - top-down pathway.\n",
    "\n",
    "    Fuses high-level semantic features (P5) with lower-level features (P4, P3)\n",
    "    through upsampling and concatenation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, c3: int = 256, c4: int = 512, c5: int = 1024):\n",
    "        super().__init__()\n",
    "        # Lateral convolutions to reduce channel dims\n",
    "        self.lateral_p5 = ConvBNSiLU(c5, c4, 1)\n",
    "        self.lateral_p4 = ConvBNSiLU(c4, c3, 1)\n",
    "\n",
    "        # C3k2 blocks after concatenation\n",
    "        self.fpn_p4 = C3k2(c4 + c4, c4, n=2, shortcut=False)\n",
    "        self.fpn_p3 = C3k2(c3 + c3, c3, n=2, shortcut=False)\n",
    "\n",
    "    def forward(self, p3, p4, p5):\n",
    "        # Top-down: P5 -> P4\n",
    "        p5_up = F.interpolate(self.lateral_p5(p5), size=p4.shape[2:], mode='nearest')\n",
    "        fpn_p4 = self.fpn_p4(torch.cat([p5_up, p4], dim=1))\n",
    "\n",
    "        # Top-down: P4 -> P3\n",
    "        p4_up = F.interpolate(self.lateral_p4(fpn_p4), size=p3.shape[2:], mode='nearest')\n",
    "        fpn_p3 = self.fpn_p3(torch.cat([p4_up, p3], dim=1))\n",
    "\n",
    "        return fpn_p3, fpn_p4, p5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4789c46a",
   "metadata": {
    "papermill": {
     "duration": 0.002056,
     "end_time": "2026-02-14T08:02:16.295471",
     "exception": false,
     "start_time": "2026-02-14T08:02:16.293415",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PAN: Bottom-Up Path\n",
    "\n",
    "The **Path Aggregation Network (PAN)** complements the FPN with a bottom-up pathway. While FPN carries semantic information downward, PAN carries strong localization features back upward:\n",
    "\n",
    "1. **FPN P3 is downsampled** (stride-2 convolution) and concatenated with FPN P4. A C3k2 block fuses them.\n",
    "2. **The fused P4 is downsampled** and concatenated with P5. Another C3k2 block produces the final PAN P5 output.\n",
    "\n",
    "The combination of FPN + PAN creates a bidirectional feature fusion pathway. Every scale level now benefits from both high-level category semantics and low-level localization accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495378f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T08:02:16.300099Z",
     "iopub.status.busy": "2026-02-14T08:02:16.299963Z",
     "iopub.status.idle": "2026-02-14T08:02:16.303514Z",
     "shell.execute_reply": "2026-02-14T08:02:16.303085Z"
    },
    "papermill": {
     "duration": 0.006575,
     "end_time": "2026-02-14T08:02:16.303958",
     "exception": false,
     "start_time": "2026-02-14T08:02:16.297383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PAN(nn.Module):\n",
    "    \"\"\"Path Aggregation Network - bottom-up pathway.\n",
    "\n",
    "    Fuses strong localization features from lower levels back up,\n",
    "    complementing FPN's semantic fusion.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, c3: int = 256, c4: int = 512, c5: int = 1024):\n",
    "        super().__init__()\n",
    "        # Downsample convolutions (k=3, s=2)\n",
    "        self.down_p3 = ConvBNSiLU(c3, c3, 3, 2)\n",
    "        self.down_p4 = ConvBNSiLU(c4, c4, 3, 2)\n",
    "\n",
    "        # C3k2 blocks after concatenation\n",
    "        self.pan_p4 = C3k2(c3 + c4, c4, n=2, shortcut=False)\n",
    "        self.pan_p5 = C3k2(c4 + c5, c5, n=2, shortcut=False)\n",
    "\n",
    "    def forward(self, fpn_p3, fpn_p4, p5):\n",
    "        # Bottom-up: P3 -> P4\n",
    "        p3_down = self.down_p3(fpn_p3)\n",
    "        pan_p4 = self.pan_p4(torch.cat([p3_down, fpn_p4], dim=1))\n",
    "\n",
    "        # Bottom-up: P4 -> P5\n",
    "        p4_down = self.down_p4(pan_p4)\n",
    "        pan_p5 = self.pan_p5(torch.cat([p4_down, p5], dim=1))\n",
    "\n",
    "        return fpn_p3, pan_p4, pan_p5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d1feb4",
   "metadata": {
    "papermill": {
     "duration": 0.001758,
     "end_time": "2026-02-14T08:02:16.307534",
     "exception": false,
     "start_time": "2026-02-14T08:02:16.305776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## C2PSA: Channel Attention Block\n",
    "\n",
    "**Partial Self-Attention (PSA)** is a lightweight attention mechanism introduced in YOLOv11 that selectively emphasizes the most informative channels in a feature map while suppressing noise.\n",
    "\n",
    "The C2PSA block follows the CSP (Cross Stage Partial) pattern:\n",
    "\n",
    "1. Split the input channels into two halves.\n",
    "2. Process one half through bottleneck layers with channel attention (squeeze-excitation style).\n",
    "3. Concatenate the unprocessed half with the attended output.\n",
    "\n",
    "This design keeps computational cost low while still providing the network with a learned mechanism for feature selection. In YOLOv11, C2PSA is applied to the deepest feature map (P5) where the receptive field is largest and attention is most beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ebb67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T08:02:16.312190Z",
     "iopub.status.busy": "2026-02-14T08:02:16.312051Z",
     "iopub.status.idle": "2026-02-14T08:02:16.316289Z",
     "shell.execute_reply": "2026-02-14T08:02:16.315903Z"
    },
    "papermill": {
     "duration": 0.007686,
     "end_time": "2026-02-14T08:02:16.316973",
     "exception": false,
     "start_time": "2026-02-14T08:02:16.309287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class C2PSA(nn.Module):\n",
    "    \"\"\"CSP block with Partial Self-Attention for feature refinement.\n",
    "\n",
    "    Applies channel attention to selectively emphasize informative features\n",
    "    while suppressing noise, improving detection at all scales.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, n: int = 1):\n",
    "        super().__init__()\n",
    "        self.c = in_channels // 2\n",
    "        self.cv1 = ConvBNSiLU(in_channels, 2 * self.c, 1)\n",
    "        self.cv2 = ConvBNSiLU(2 * self.c, out_channels, 1)\n",
    "\n",
    "        # Attention blocks\n",
    "        self.attention = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(self.c, self.c // 4),\n",
    "                nn.SiLU(inplace=True),\n",
    "                nn.Linear(self.c // 4, self.c),\n",
    "                nn.Sigmoid()\n",
    "            ) for _ in range(n)\n",
    "        ])\n",
    "        self.bottlenecks = nn.ModuleList([\n",
    "            Bottleneck(self.c, self.c, shortcut=True) for _ in range(n)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = list(self.cv1(x).chunk(2, dim=1))\n",
    "        for attn, bn in zip(self.attention, self.bottlenecks):\n",
    "            feat = bn(y[-1])\n",
    "            # Channel attention\n",
    "            att_weights = attn(feat).unsqueeze(-1).unsqueeze(-1)\n",
    "            feat = feat * att_weights\n",
    "            y.append(feat)\n",
    "        # Use only first split and last bottleneck output\n",
    "        return self.cv2(torch.cat([y[0], y[-1]], dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6674d7a",
   "metadata": {
    "papermill": {
     "duration": 0.003231,
     "end_time": "2026-02-14T08:02:16.323466",
     "exception": false,
     "start_time": "2026-02-14T08:02:16.320235",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DFL: Distribution Focal Loss Head\n",
    "\n",
    "Traditional object detectors regress four continuous values (e.g., center offsets and width/height) for each bounding box. **Distribution Focal Loss (DFL)** takes a fundamentally different approach: instead of predicting a single scalar per box boundary, the network predicts a **discrete probability distribution** over a set of `reg_max` bins.\n",
    "\n",
    "### Why distributions instead of scalars?\n",
    "\n",
    "- **Ambiguity modeling**: Object boundaries are often ambiguous (occlusion, blur). A distribution naturally represents this uncertainty.\n",
    "- **Better optimization**: The softmax-based formulation provides smoother gradients than direct regression.\n",
    "- **Improved small-object accuracy**: The expected-value computation gives sub-bin precision.\n",
    "\n",
    "### How it works\n",
    "\n",
    "For each of the 4 box boundaries (left, top, right, bottom):\n",
    "\n",
    "1. The network outputs `reg_max` logits.\n",
    "2. A softmax converts them to a probability distribution.\n",
    "3. The final offset is the expected value: $\\sum_{i=0}^{\\text{reg\\_max}-1} i \\cdot P(i)$.\n",
    "\n",
    "This gives the model the expressiveness of a full distribution while producing a single precise offset for each boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5016e05",
   "metadata": {
    "papermill": {
     "duration": 0.002256,
     "end_time": "2026-02-14T08:02:16.328898",
     "exception": false,
     "start_time": "2026-02-14T08:02:16.326642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Decoupled Detection Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd0e285",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T08:02:16.335451Z",
     "iopub.status.busy": "2026-02-14T08:02:16.335310Z",
     "iopub.status.idle": "2026-02-14T08:02:16.341929Z",
     "shell.execute_reply": "2026-02-14T08:02:16.341128Z"
    },
    "papermill": {
     "duration": 0.011247,
     "end_time": "2026-02-14T08:02:16.343019",
     "exception": false,
     "start_time": "2026-02-14T08:02:16.331772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DFLHead(nn.Module):\n",
    "    \"\"\"Distribution Focal Loss module for precise box regression.\n",
    "\n",
    "    Predicts a discrete probability distribution over reg_max bins\n",
    "    for each of the 4 box boundaries (left, top, right, bottom).\n",
    "    The expected value gives the final regression offset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reg_max: int = 16):\n",
    "        super().__init__()\n",
    "        self.reg_max = reg_max\n",
    "        # Project: weight vector [0, 1, ..., reg_max-1]\n",
    "        self.register_buffer('project', torch.arange(reg_max, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x: (B, 4 * reg_max, H, W) -> (B, 4, H, W) box offsets.\"\"\"\n",
    "        b, _, h, w = x.shape\n",
    "        # Reshape to (B, 4, reg_max, H, W)\n",
    "        x = x.view(b, 4, self.reg_max, h, w)\n",
    "        # Softmax over reg_max dimension\n",
    "        x = F.softmax(x, dim=2)\n",
    "        # Expected value: weighted sum with [0, 1, ..., reg_max-1]\n",
    "        x = (x * self.project.view(1, 1, -1, 1, 1)).sum(dim=2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DetectionHead(nn.Module):\n",
    "    \"\"\"Decoupled anchor-free detection head for one scale level.\n",
    "\n",
    "    Separate classification and regression branches with DFL\n",
    "    for precise localization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, num_classes: int = 80, reg_max: int = 16):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.reg_max = reg_max\n",
    "\n",
    "        # Classification branch\n",
    "        self.cls_branch = nn.Sequential(\n",
    "            ConvBNSiLU(in_channels, in_channels, 3),\n",
    "            ConvBNSiLU(in_channels, in_channels, 3),\n",
    "            nn.Conv2d(in_channels, num_classes, 1)\n",
    "        )\n",
    "\n",
    "        # Regression branch (DFL)\n",
    "        self.reg_branch = nn.Sequential(\n",
    "            ConvBNSiLU(in_channels, in_channels, 3),\n",
    "            ConvBNSiLU(in_channels, in_channels, 3),\n",
    "            nn.Conv2d(in_channels, 4 * reg_max, 1)\n",
    "        )\n",
    "\n",
    "        self.dfl = DFLHead(reg_max)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, C, H, W) feature map from neck\n",
    "        Returns:\n",
    "            cls_pred: (B, num_classes, H, W) class logits\n",
    "            box_pred: (B, 4, H, W) decoded box offsets (ltrb)\n",
    "            box_raw: (B, 4*reg_max, H, W) raw DFL logits (for loss)\n",
    "        \"\"\"\n",
    "        cls_pred = self.cls_branch(x)\n",
    "        box_raw = self.reg_branch(x)\n",
    "        box_pred = self.dfl(box_raw)\n",
    "        return cls_pred, box_pred, box_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebf3e46",
   "metadata": {
    "papermill": {
     "duration": 0.003063,
     "end_time": "2026-02-14T08:02:16.350466",
     "exception": false,
     "start_time": "2026-02-14T08:02:16.347403",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Full YOLOv11 Model Assembly\n",
    "\n",
    "We now assemble the complete YOLOv11 model by combining backbone, FPN + PAN neck, C2PSA attention, and decoupled detection heads into a single end-to-end module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41364c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T08:02:16.355819Z",
     "iopub.status.busy": "2026-02-14T08:02:16.355649Z",
     "iopub.status.idle": "2026-02-14T08:02:16.361209Z",
     "shell.execute_reply": "2026-02-14T08:02:16.360221Z"
    },
    "papermill": {
     "duration": 0.009102,
     "end_time": "2026-02-14T08:02:16.361957",
     "exception": false,
     "start_time": "2026-02-14T08:02:16.352855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class YOLOv11(nn.Module):\n",
    "    \"\"\"Complete YOLOv11 object detection model.\n",
    "\n",
    "    Assembles backbone, FPN+PAN neck, and decoupled detection heads\n",
    "    into a single end-to-end model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int = 80, reg_max: int = 16, base_channels: int = 64):\n",
    "        super().__init__()\n",
    "        c3, c4, c5 = base_channels * 4, base_channels * 8, base_channels * 16\n",
    "\n",
    "        self.backbone = YOLOv11Backbone(base_channels=base_channels)\n",
    "        self.fpn = FPN(c3, c4, c5)\n",
    "        self.pan = PAN(c3, c4, c5)\n",
    "\n",
    "        # Optional C2PSA attention on P5\n",
    "        self.c2psa = C2PSA(c5, c5, n=1)\n",
    "\n",
    "        # Detection heads - one per scale\n",
    "        self.head_p3 = DetectionHead(c3, num_classes, reg_max)\n",
    "        self.head_p4 = DetectionHead(c4, num_classes, reg_max)\n",
    "        self.head_p5 = DetectionHead(c5, num_classes, reg_max)\n",
    "\n",
    "        self.strides = [8, 16, 32]\n",
    "        self.num_classes = num_classes\n",
    "        self.reg_max = reg_max\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, 3, 640, 640) input images\n",
    "        Returns:\n",
    "            predictions: list of (cls_pred, box_pred, box_raw) per scale\n",
    "        \"\"\"\n",
    "        # Backbone\n",
    "        p3, p4, p5 = self.backbone(x)\n",
    "\n",
    "        # Neck: FPN + PAN\n",
    "        fpn_p3, fpn_p4, fpn_p5 = self.fpn(p3, p4, p5)\n",
    "        pan_p3, pan_p4, pan_p5 = self.pan(fpn_p3, fpn_p4, fpn_p5)\n",
    "\n",
    "        # Attention on P5\n",
    "        pan_p5 = self.c2psa(pan_p5)\n",
    "\n",
    "        # Detection heads\n",
    "        pred_p3 = self.head_p3(pan_p3)\n",
    "        pred_p4 = self.head_p4(pan_p4)\n",
    "        pred_p5 = self.head_p5(pan_p5)\n",
    "\n",
    "        return [pred_p3, pred_p4, pred_p5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5784c3a",
   "metadata": {
    "papermill": {
     "duration": 0.003797,
     "end_time": "2026-02-14T08:02:16.369717",
     "exception": false,
     "start_time": "2026-02-14T08:02:16.365920",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Shape Verification\n",
    "\n",
    "Let us verify that the full model produces outputs of the expected shapes at each scale level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2de0905",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T08:02:16.374738Z",
     "iopub.status.busy": "2026-02-14T08:02:16.374591Z",
     "iopub.status.idle": "2026-02-14T08:02:17.575328Z",
     "shell.execute_reply": "2026-02-14T08:02:17.574142Z"
    },
    "papermill": {
     "duration": 1.204141,
     "end_time": "2026-02-14T08:02:17.576192",
     "exception": false,
     "start_time": "2026-02-14T08:02:16.372051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = YOLOv11(num_classes=80)\n",
    "x = torch.randn(2, 3, 640, 640)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print()\n",
    "for i, (cls_pred, box_pred, box_raw) in enumerate(predictions):\n",
    "    stride = model.strides[i]\n",
    "    print(f\"P{i+3} (stride {stride}):\")\n",
    "    print(f\"  cls_pred: {cls_pred.shape}  (B, {model.num_classes} classes, H, W)\")\n",
    "    print(f\"  box_pred: {box_pred.shape}  (B, 4 ltrb offsets, H, W)\")\n",
    "    print(f\"  box_raw:  {box_raw.shape}  (B, 4x{model.reg_max} DFL bins, H, W)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbfbeda",
   "metadata": {
    "papermill": {
     "duration": 0.003518,
     "end_time": "2026-02-14T08:02:17.583473",
     "exception": false,
     "start_time": "2026-02-14T08:02:17.579955",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Parameter Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73c8418",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T08:02:17.591877Z",
     "iopub.status.busy": "2026-02-14T08:02:17.591568Z",
     "iopub.status.idle": "2026-02-14T08:02:17.599964Z",
     "shell.execute_reply": "2026-02-14T08:02:17.599434Z"
    },
    "papermill": {
     "duration": 0.013888,
     "end_time": "2026-02-14T08:02:17.600729",
     "exception": false,
     "start_time": "2026-02-14T08:02:17.586841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_params(model, name=\"Model\"):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\n{name}: {total:,} parameters ({total * 4 / 1024**2:.1f} MB)\")\n",
    "    return total\n",
    "\n",
    "print(\"=== YOLOv11 Parameter Breakdown ===\")\n",
    "for name, module in model.named_children():\n",
    "    count_params(module, name)\n",
    "count_params(model, \"Total YOLOv11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb69d17a",
   "metadata": {
    "papermill": {
     "duration": 0.00245,
     "end_time": "2026-02-14T08:02:17.606547",
     "exception": false,
     "start_time": "2026-02-14T08:02:17.604097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Architecture Visualization\n",
    "\n",
    "The following diagram illustrates the complete information flow through the YOLOv11 architecture: backbone feature extraction, FPN top-down fusion, PAN bottom-up fusion, and the per-scale detection heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2970063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T08:02:17.612135Z",
     "iopub.status.busy": "2026-02-14T08:02:17.611889Z",
     "iopub.status.idle": "2026-02-14T08:02:17.708981Z",
     "shell.execute_reply": "2026-02-14T08:02:17.708412Z"
    },
    "papermill": {
     "duration": 0.100829,
     "end_time": "2026-02-14T08:02:17.709634",
     "exception": false,
     "start_time": "2026-02-14T08:02:17.608805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_architecture():\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    ax.set_xlim(0, 14)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Draw backbone\n",
    "    backbone_boxes = [\n",
    "        (1, 8.5, 'Stem\\n3->64', 'lightblue'),\n",
    "        (1, 7.0, 'Stage1\\n64->128', 'lightblue'),\n",
    "        (1, 5.5, 'Stage2\\n128->256', 'lightblue'),\n",
    "        (1, 4.0, 'Stage3\\n256->512', 'lightblue'),\n",
    "        (1, 2.5, 'Stage4+SPPF\\n512->1024', 'lightblue'),\n",
    "    ]\n",
    "\n",
    "    for bx, by, text, color in backbone_boxes:\n",
    "        rect = plt.Rectangle((bx - 0.7, by - 0.5), 1.4, 0.8,\n",
    "                             facecolor=color, edgecolor='black', linewidth=1.5)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(bx, by, text, ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "    # FPN arrows and boxes\n",
    "    fpn_x = 5\n",
    "    ax.text(fpn_x, 9.2, 'FPN (Top-Down)', ha='center', fontsize=11,\n",
    "            fontweight='bold', color='green')\n",
    "\n",
    "    fpn_boxes = [\n",
    "        (fpn_x, 5.5, 'FPN P3\\n256', 'lightgreen'),\n",
    "        (fpn_x, 4.0, 'FPN P4\\n512', 'lightgreen'),\n",
    "        (fpn_x, 2.5, 'P5\\n1024', 'lightgreen'),\n",
    "    ]\n",
    "    for bx, by, text, color in fpn_boxes:\n",
    "        rect = plt.Rectangle((bx - 0.7, by - 0.5), 1.4, 0.8,\n",
    "                             facecolor=color, edgecolor='black', linewidth=1.5)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(bx, by, text, ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "    # PAN boxes\n",
    "    pan_x = 9\n",
    "    ax.text(pan_x, 9.2, 'PAN (Bottom-Up)', ha='center', fontsize=11,\n",
    "            fontweight='bold', color='orange')\n",
    "\n",
    "    pan_boxes = [\n",
    "        (pan_x, 5.5, 'PAN P3\\n256', 'moccasin'),\n",
    "        (pan_x, 4.0, 'PAN P4\\n512', 'moccasin'),\n",
    "        (pan_x, 2.5, 'PAN P5\\n1024', 'moccasin'),\n",
    "    ]\n",
    "    for bx, by, text, color in pan_boxes:\n",
    "        rect = plt.Rectangle((bx - 0.7, by - 0.5), 1.4, 0.8,\n",
    "                             facecolor=color, edgecolor='black', linewidth=1.5)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(bx, by, text, ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "    # Heads\n",
    "    head_x = 12.5\n",
    "    ax.text(head_x, 9.2, 'Detection Heads', ha='center', fontsize=11,\n",
    "            fontweight='bold', color='red')\n",
    "\n",
    "    head_boxes = [\n",
    "        (head_x, 5.5, 'Head P3\\ncls+reg', 'lightyellow'),\n",
    "        (head_x, 4.0, 'Head P4\\ncls+reg', 'lightyellow'),\n",
    "        (head_x, 2.5, 'Head P5\\ncls+reg', 'lightyellow'),\n",
    "    ]\n",
    "    for bx, by, text, color in head_boxes:\n",
    "        rect = plt.Rectangle((bx - 0.8, by - 0.5), 1.6, 0.8,\n",
    "                             facecolor=color, edgecolor='black', linewidth=1.5)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(bx, by, text, ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "    ax.set_title('YOLOv11 Architecture: Backbone -> FPN -> PAN -> Heads',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850810a9",
   "metadata": {
    "papermill": {
     "duration": 0.003959,
     "end_time": "2026-02-14T08:02:17.718709",
     "exception": false,
     "start_time": "2026-02-14T08:02:17.714750",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we built the complete YOLOv11 architecture on top of the backbone from Notebook 2:\n",
    "\n",
    "- **FPN (top-down)** propagates high-level semantics from P5 down to P3, ensuring that every scale level understands *what* objects are present.\n",
    "- **PAN (bottom-up)** propagates strong localization features from P3 back up to P5, ensuring that every scale level knows *where* objects are.\n",
    "- **C2PSA** applies lightweight channel attention to P5, allowing the network to selectively emphasize the most informative features.\n",
    "- **DFL (Distribution Focal Loss)** replaces direct box regression with a discrete distribution over offsets, enabling more precise localization---especially for small objects.\n",
    "- **Decoupled detection heads** separate classification and regression into independent branches, allowing each task to specialize without interfering with the other.\n",
    "\n",
    "The model produces predictions at three scales (P3/P4/P5 with strides 8/16/32), covering objects from small to large.\n",
    "\n",
    "**Next**: Notebook 4 covers the loss functions used to train this model, including the task-aligned assigner, classification loss (BCE), box regression loss (CIoU + DFL), and the overall multi-task training objective."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.199861,
   "end_time": "2026-02-14T08:02:18.340942",
   "environment_variables": {},
   "exception": null,
   "input_path": "notebooks/scene-understanding/object-detection/yolo/pytorch/03_neck_and_head.ipynb",
   "output_path": "notebooks/scene-understanding/object-detection/yolo/pytorch/03_neck_and_head-executed.ipynb",
   "parameters": {},
   "start_time": "2026-02-14T08:02:14.141081",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
