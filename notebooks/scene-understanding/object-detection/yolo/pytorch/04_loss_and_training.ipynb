{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06ab0a1c",
   "metadata": {
    "papermill": {
     "duration": 0.005587,
     "end_time": "2026-02-15T16:30:51.568216",
     "exception": false,
     "start_time": "2026-02-15T16:30:51.562629",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loss Functions, Label Assignment, and Training\n",
    "\n",
    "*Notebook 4 of 5 in the YOLOv11 from-scratch series*\n",
    "\n",
    "## Introduction\n",
    "\n",
    "With our model architecture complete (backbone, neck, and head from Notebooks 2-3), we now face three critical challenges that determine whether the detector actually learns to find objects:\n",
    "\n",
    "1. **IoU computation** - How do we measure the geometric overlap between predicted and ground-truth boxes? The choice of IoU variant directly affects gradient quality and convergence speed.\n",
    "\n",
    "2. **Label assignment strategy** - Given thousands of anchor points but only a handful of ground-truth boxes per image, which anchors should be responsible for predicting each object? This is the assignment problem.\n",
    "\n",
    "3. **Loss function design** - How do we combine classification, localization, and distribution regression objectives into a single scalar loss that balances all three tasks?\n",
    "\n",
    "YOLOv11 addresses these with:\n",
    "- **CIoU** (Complete IoU) for box regression, which captures overlap, center distance, and aspect ratio in a single differentiable metric\n",
    "- **Task-Aligned Learning (TAL)** for label assignment, which selects anchors based on both classification confidence and localization quality\n",
    "- A **composite loss** combining BCE classification, CIoU box regression, and Distribution Focal Loss (DFL)\n",
    "\n",
    "We will build each component from scratch and run a small training loop on synthetic data to verify that the entire pipeline works end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Colab Environment Setup ---\n",
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    %pip install -q matplotlib seaborn scikit-learn scipy tqdm datasets\n",
    "    print(\"Colab dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14d8cc7",
   "metadata": {
    "papermill": {
     "duration": 0.002399,
     "end_time": "2026-02-15T16:30:51.574191",
     "exception": false,
     "start_time": "2026-02-15T16:30:51.571792",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91671ef9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:51.579466Z",
     "iopub.status.busy": "2026-02-15T16:30:51.579307Z",
     "iopub.status.idle": "2026-02-15T16:30:53.423505Z",
     "shell.execute_reply": "2026-02-15T16:30:53.423030Z"
    },
    "papermill": {
     "duration": 1.847749,
     "end_time": "2026-02-15T16:30:53.424086",
     "exception": false,
     "start_time": "2026-02-15T16:30:51.576337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "from datasets import load_dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6d688c",
   "metadata": {
    "papermill": {
     "duration": 0.002455,
     "end_time": "2026-02-15T16:30:53.429213",
     "exception": false,
     "start_time": "2026-02-15T16:30:53.426758",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model components from Notebooks 2-3\n",
    "\n",
    "To keep this notebook self-contained, we re-define all model components (backbone, neck, head) in a single compact cell. These are identical to the implementations in Notebooks 2 and 3. Refer to those notebooks for detailed explanations of each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc88928b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:53.435386Z",
     "iopub.status.busy": "2026-02-15T16:30:53.435105Z",
     "iopub.status.idle": "2026-02-15T16:30:53.931229Z",
     "shell.execute_reply": "2026-02-15T16:30:53.930372Z"
    },
    "papermill": {
     "duration": 0.500172,
     "end_time": "2026-02-15T16:30:53.932057",
     "exception": false,
     "start_time": "2026-02-15T16:30:53.431885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvBNSiLU(nn.Module):\n",
    "    def __init__(self, c_in, c_out, k=1, s=1, p=None, g=1):\n",
    "        super().__init__()\n",
    "        if p is None: p = k // 2\n",
    "        self.conv = nn.Conv2d(c_in, c_out, k, s, p, groups=g, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c_out)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "    def forward(self, x): return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, c_in, c_out, shortcut=True, k=(3,3), e=0.5):\n",
    "        super().__init__()\n",
    "        c_hid = int(c_out * e)\n",
    "        self.cv1 = ConvBNSiLU(c_in, c_hid, k[0])\n",
    "        self.cv2 = ConvBNSiLU(c_hid, c_out, k[1])\n",
    "        self.add = shortcut and c_in == c_out\n",
    "    def forward(self, x): return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
    "\n",
    "class C3k2(nn.Module):\n",
    "    def __init__(self, c_in, c_out, n=1, shortcut=True, e=0.5):\n",
    "        super().__init__()\n",
    "        self.c = int(c_out * e)\n",
    "        self.cv1 = ConvBNSiLU(c_in, 2 * self.c, 1)\n",
    "        self.cv2 = ConvBNSiLU((2 + n) * self.c, c_out, 1)\n",
    "        self.bottlenecks = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, (3,3), 1.0) for _ in range(n))\n",
    "    def forward(self, x):\n",
    "        y = list(self.cv1(x).chunk(2, dim=1))\n",
    "        for bn in self.bottlenecks: y.append(bn(y[-1]))\n",
    "        return self.cv2(torch.cat(y, dim=1))\n",
    "\n",
    "class SPPF(nn.Module):\n",
    "    def __init__(self, c_in, c_out, k=5):\n",
    "        super().__init__()\n",
    "        c_hid = c_in // 2\n",
    "        self.cv1 = ConvBNSiLU(c_in, c_hid, 1)\n",
    "        self.cv2 = ConvBNSiLU(c_hid * 4, c_out, 1)\n",
    "        self.pool = nn.MaxPool2d(k, stride=1, padding=k // 2)\n",
    "    def forward(self, x):\n",
    "        x = self.cv1(x); y1 = self.pool(x); y2 = self.pool(y1); y3 = self.pool(y2)\n",
    "        return self.cv2(torch.cat([x, y1, y2, y3], dim=1))\n",
    "\n",
    "class YOLOv11Backbone(nn.Module):\n",
    "    def __init__(self, c_in=3, base=64):\n",
    "        super().__init__()\n",
    "        c1, c2, c3, c4, c5 = base, base*2, base*4, base*8, base*16\n",
    "        self.stem = ConvBNSiLU(c_in, c1, 3, s=2)\n",
    "        self.s1_down = ConvBNSiLU(c1, c2, 3, s=2); self.s1_c3k2 = C3k2(c2, c2, n=2)\n",
    "        self.s2_down = ConvBNSiLU(c2, c3, 3, s=2); self.s2_c3k2 = C3k2(c3, c3, n=2)\n",
    "        self.s3_down = ConvBNSiLU(c3, c4, 3, s=2); self.s3_c3k2 = C3k2(c4, c4, n=2)\n",
    "        self.s4_down = ConvBNSiLU(c4, c5, 3, s=2); self.s4_c3k2 = C3k2(c5, c5, n=2)\n",
    "        self.sppf = SPPF(c5, c5)\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.s1_c3k2(self.s1_down(x))\n",
    "        p3 = self.s2_c3k2(self.s2_down(x))\n",
    "        p4 = self.s3_c3k2(self.s3_down(p3))\n",
    "        p5 = self.sppf(self.s4_c3k2(self.s4_down(p4)))\n",
    "        return p3, p4, p5\n",
    "\n",
    "class FPN(nn.Module):\n",
    "    def __init__(self, c3=256, c4=512, c5=1024):\n",
    "        super().__init__()\n",
    "        self.lateral5 = ConvBNSiLU(c5, c4, 1)\n",
    "        self.lateral4 = ConvBNSiLU(c4, c3, 1)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.fuse4 = C3k2(c4 * 2, c4, n=2)\n",
    "        self.fuse3 = C3k2(c3 * 2, c3, n=2)\n",
    "    def forward(self, p3, p4, p5):\n",
    "        p5_up = self.up(self.lateral5(p5))\n",
    "        p4 = self.fuse4(torch.cat([p4, p5_up], dim=1))\n",
    "        p4_up = self.up(self.lateral4(p4))\n",
    "        p3 = self.fuse3(torch.cat([p3, p4_up], dim=1))\n",
    "        return p3, p4, p5\n",
    "\n",
    "class PAN(nn.Module):\n",
    "    def __init__(self, c3=256, c4=512, c5=1024):\n",
    "        super().__init__()\n",
    "        self.down3 = ConvBNSiLU(c3, c3, 3, s=2)\n",
    "        self.fuse4 = C3k2(c3 + c4, c4, n=2)\n",
    "        self.down4 = ConvBNSiLU(c4, c4, 3, s=2)\n",
    "        self.fuse5 = C3k2(c4 + c5, c5, n=2)\n",
    "    def forward(self, p3, p4, p5):\n",
    "        p4 = self.fuse4(torch.cat([self.down3(p3), p4], dim=1))\n",
    "        p5 = self.fuse5(torch.cat([self.down4(p4), p5], dim=1))\n",
    "        return p3, p4, p5\n",
    "\n",
    "class C2PSA(nn.Module):\n",
    "    def __init__(self, c, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.cv1 = ConvBNSiLU(c, c, 1)\n",
    "        self.attn = nn.MultiheadAttention(c, n_heads, batch_first=True)\n",
    "        self.ffn = nn.Sequential(ConvBNSiLU(c, c * 2, 1), ConvBNSiLU(c * 2, c, 1))\n",
    "        self.cv2 = ConvBNSiLU(c, c, 1)\n",
    "    def forward(self, x):\n",
    "        y = self.cv1(x)\n",
    "        B, C, H, W = y.shape\n",
    "        flat = y.flatten(2).permute(0, 2, 1)\n",
    "        flat = flat + self.attn(flat, flat, flat, need_weights=False)[0]\n",
    "        y = flat.permute(0, 2, 1).view(B, C, H, W)\n",
    "        y = y + self.ffn(y)\n",
    "        return self.cv2(y)\n",
    "\n",
    "class DFLHead(nn.Module):\n",
    "    def __init__(self, c_in, num_classes=80, reg_max=16):\n",
    "        super().__init__()\n",
    "        self.reg_max = reg_max\n",
    "        self.num_classes = num_classes\n",
    "        self.cls_convs = nn.Sequential(ConvBNSiLU(c_in, c_in, 3), ConvBNSiLU(c_in, c_in, 3))\n",
    "        self.reg_convs = nn.Sequential(ConvBNSiLU(c_in, c_in, 3), ConvBNSiLU(c_in, c_in, 3))\n",
    "        self.cls_pred = nn.Conv2d(c_in, num_classes, 1)\n",
    "        self.reg_pred = nn.Conv2d(c_in, 4 * reg_max, 1)\n",
    "        self.proj = nn.Parameter(torch.arange(reg_max, dtype=torch.float32), requires_grad=False)\n",
    "    def forward(self, x):\n",
    "        cls_out = self.cls_pred(self.cls_convs(x))\n",
    "        reg_raw = self.reg_pred(self.reg_convs(x))\n",
    "        B, _, H, W = reg_raw.shape\n",
    "        reg_dist = reg_raw.view(B, 4, self.reg_max, H, W)\n",
    "        reg_box = F.softmax(reg_dist, dim=2)\n",
    "        reg_box = (reg_box * self.proj.view(1, 1, -1, 1, 1)).sum(dim=2)\n",
    "        return cls_out, reg_box, reg_raw\n",
    "\n",
    "class DetectionHead(nn.Module):\n",
    "    def __init__(self, channels=[256, 512, 1024], num_classes=80, reg_max=16):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([DFLHead(c, num_classes, reg_max) for c in channels])\n",
    "    def forward(self, features):\n",
    "        return [head(f) for head, f in zip(self.heads, features)]\n",
    "\n",
    "class YOLOv11(nn.Module):\n",
    "    def __init__(self, num_classes=80, reg_max=16):\n",
    "        super().__init__()\n",
    "        self.backbone = YOLOv11Backbone()\n",
    "        self.fpn = FPN()\n",
    "        self.pan = PAN()\n",
    "        self.c2psa = C2PSA(1024)\n",
    "        self.head = DetectionHead(num_classes=num_classes, reg_max=reg_max)\n",
    "    def forward(self, x):\n",
    "        p3, p4, p5 = self.backbone(x)\n",
    "        p5 = self.c2psa(p5)\n",
    "        p3, p4, p5 = self.fpn(p3, p4, p5)\n",
    "        p3, p4, p5 = self.pan(p3, p4, p5)\n",
    "        return self.head([p3, p4, p5])\n",
    "\n",
    "print(\"Model components loaded successfully.\")\n",
    "print(f\"YOLOv11 parameters: {sum(p.numel() for p in YOLOv11(num_classes=80).parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6dd8a0",
   "metadata": {
    "papermill": {
     "duration": 0.003806,
     "end_time": "2026-02-15T16:30:53.940279",
     "exception": false,
     "start_time": "2026-02-15T16:30:53.936473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The evolution of IoU metrics\n",
    "\n",
    "Intersection over Union (IoU) is the foundational metric for measuring bounding box quality. However, the basic IoU has significant limitations that led to a series of improvements:\n",
    "\n",
    "### IoU (Intersection over Union)\n",
    "$$\\text{IoU} = \\frac{|B_p \\cap B_{gt}|}{|B_p \\cup B_{gt}|}$$\n",
    "\n",
    "Simple and intuitive, but has a critical flaw: when two boxes do not overlap, IoU is zero regardless of how far apart they are. This means **zero gradient** for non-overlapping predictions, making it useless as a standalone loss for poorly initialized detectors.\n",
    "\n",
    "### GIoU (Generalized IoU)\n",
    "$$\\text{GIoU} = \\text{IoU} - \\frac{|C \\setminus (B_p \\cup B_{gt})|}{|C|}$$\n",
    "\n",
    "where $C$ is the smallest enclosing box. GIoU adds a penalty for the gap between the predicted and ground-truth boxes. It provides gradients even when boxes do not overlap, but converges slowly because it only penalizes the empty area ratio, not the distance directly.\n",
    "\n",
    "### DIoU (Distance IoU)\n",
    "$$\\text{DIoU} = \\text{IoU} - \\frac{d^2(\\mathbf{b}_p, \\mathbf{b}_{gt})}{c^2}$$\n",
    "\n",
    "where $d$ is the Euclidean distance between box centers and $c$ is the diagonal of the enclosing box. By directly penalizing center-point distance, DIoU converges much faster than GIoU.\n",
    "\n",
    "### CIoU (Complete IoU)\n",
    "$$\\text{CIoU} = \\text{IoU} - \\frac{d^2(\\mathbf{b}_p, \\mathbf{b}_{gt})}{c^2} - \\alpha v$$\n",
    "\n",
    "where $v = \\frac{4}{\\pi^2}\\left(\\arctan\\frac{w_{gt}}{h_{gt}} - \\arctan\\frac{w_p}{h_p}\\right)^2$ measures aspect ratio consistency, and $\\alpha = \\frac{v}{(1 - \\text{IoU}) + v}$ is an adaptive weight. CIoU provides **complete geometric alignment** by considering overlap, center distance, and aspect ratio simultaneously. This is what YOLOv11 uses for box regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec78f50",
   "metadata": {
    "papermill": {
     "duration": 0.00342,
     "end_time": "2026-02-15T16:30:53.947282",
     "exception": false,
     "start_time": "2026-02-15T16:30:53.943862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## IoU implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181d7b2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:53.955156Z",
     "iopub.status.busy": "2026-02-15T16:30:53.954997Z",
     "iopub.status.idle": "2026-02-15T16:30:53.961797Z",
     "shell.execute_reply": "2026-02-15T16:30:53.961076Z"
    },
    "papermill": {
     "duration": 0.012506,
     "end_time": "2026-02-15T16:30:53.962410",
     "exception": false,
     "start_time": "2026-02-15T16:30:53.949904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_iou(box1, box2, mode='ciou', eps=1e-7):\n",
    "    \"\"\"Compute IoU variants between two sets of boxes.\n",
    "    \n",
    "    Args:\n",
    "        box1: (N, 4) in [x1, y1, x2, y2] format\n",
    "        box2: (M, 4) in [x1, y1, x2, y2] format\n",
    "        mode: 'iou', 'giou', 'diou', or 'ciou'\n",
    "    Returns:\n",
    "        iou: (N, M) pairwise IoU values\n",
    "    \"\"\"\n",
    "    # Intersection\n",
    "    inter_x1 = torch.max(box1[:, None, 0], box2[None, :, 0])\n",
    "    inter_y1 = torch.max(box1[:, None, 1], box2[None, :, 1])\n",
    "    inter_x2 = torch.min(box1[:, None, 2], box2[None, :, 2])\n",
    "    inter_y2 = torch.min(box1[:, None, 3], box2[None, :, 3])\n",
    "    inter = (inter_x2 - inter_x1).clamp(0) * (inter_y2 - inter_y1).clamp(0)\n",
    "    \n",
    "    # Union\n",
    "    area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
    "    area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n",
    "    union = area1[:, None] + area2[None, :] - inter\n",
    "    \n",
    "    iou = inter / (union + eps)\n",
    "    \n",
    "    if mode == 'iou':\n",
    "        return iou\n",
    "    \n",
    "    # Enclosing box\n",
    "    enc_x1 = torch.min(box1[:, None, 0], box2[None, :, 0])\n",
    "    enc_y1 = torch.min(box1[:, None, 1], box2[None, :, 1])\n",
    "    enc_x2 = torch.max(box1[:, None, 2], box2[None, :, 2])\n",
    "    enc_y2 = torch.max(box1[:, None, 3], box2[None, :, 3])\n",
    "    enc_area = (enc_x2 - enc_x1) * (enc_y2 - enc_y1)\n",
    "    \n",
    "    if mode == 'giou':\n",
    "        return iou - (enc_area - union) / (enc_area + eps)\n",
    "    \n",
    "    # Center distance\n",
    "    cx1 = (box1[:, 0] + box1[:, 2]) / 2\n",
    "    cy1 = (box1[:, 1] + box1[:, 3]) / 2\n",
    "    cx2 = (box2[:, 0] + box2[:, 2]) / 2\n",
    "    cy2 = (box2[:, 1] + box2[:, 3]) / 2\n",
    "    \n",
    "    center_dist = (cx1[:, None] - cx2[None, :]) ** 2 + (cy1[:, None] - cy2[None, :]) ** 2\n",
    "    diag_dist = (enc_x2 - enc_x1) ** 2 + (enc_y2 - enc_y1) ** 2\n",
    "    \n",
    "    if mode == 'diou':\n",
    "        return iou - center_dist / (diag_dist + eps)\n",
    "    \n",
    "    # CIoU: aspect ratio penalty\n",
    "    w1 = box1[:, 2] - box1[:, 0]\n",
    "    h1 = box1[:, 3] - box1[:, 1]\n",
    "    w2 = box2[:, 2] - box2[:, 0]\n",
    "    h2 = box2[:, 3] - box2[:, 1]\n",
    "    \n",
    "    v = (4 / math.pi ** 2) * (\n",
    "        torch.atan(w2[None, :] / (h2[None, :] + eps)) - \n",
    "        torch.atan(w1[:, None] / (h1[:, None] + eps))\n",
    "    ) ** 2\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        alpha = v / (1 - iou + v + eps)\n",
    "    \n",
    "    return iou - center_dist / (diag_dist + eps) - alpha * v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717cdd8e",
   "metadata": {
    "papermill": {
     "duration": 0.002298,
     "end_time": "2026-02-15T16:30:53.968770",
     "exception": false,
     "start_time": "2026-02-15T16:30:53.966472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Visualizing IoU variants\n",
    "\n",
    "To build intuition for how these metrics differ, we slide a prediction box horizontally away from a fixed ground-truth box and plot each IoU variant. Notice how:\n",
    "\n",
    "- **IoU** drops to zero once boxes separate and stays there (no gradient signal)\n",
    "- **GIoU** continues to decrease below zero but slowly\n",
    "- **DIoU** decreases more steeply due to the direct distance penalty\n",
    "- **CIoU** behaves like DIoU here (same aspect ratio), but would differ for shape changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f0a5dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:53.974324Z",
     "iopub.status.busy": "2026-02-15T16:30:53.974127Z",
     "iopub.status.idle": "2026-02-15T16:30:54.374427Z",
     "shell.execute_reply": "2026-02-15T16:30:54.373924Z"
    },
    "papermill": {
     "duration": 0.404224,
     "end_time": "2026-02-15T16:30:54.375203",
     "exception": false,
     "start_time": "2026-02-15T16:30:53.970979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_iou_variants():\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    # Fixed reference box\n",
    "    ref = torch.tensor([[2.0, 2.0, 5.0, 5.0]])\n",
    "    \n",
    "    # Move a box horizontally\n",
    "    offsets = torch.linspace(0, 6, 50)\n",
    "    modes = ['iou', 'giou', 'diou', 'ciou']\n",
    "    \n",
    "    for ax, mode in zip(axes, modes):\n",
    "        values = []\n",
    "        for dx in offsets:\n",
    "            pred = torch.tensor([[2.0 + dx.item(), 2.0, 5.0 + dx.item(), 5.0]])\n",
    "            val = compute_iou(ref, pred, mode=mode)\n",
    "            values.append(val.item())\n",
    "        ax.plot(offsets.numpy(), values, linewidth=2)\n",
    "        ax.set_title(mode.upper(), fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Horizontal offset')\n",
    "        ax.set_ylabel(f'{mode} value')\n",
    "        ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('IoU Variants: Response to Horizontal Box Translation', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_iou_variants()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415ef833",
   "metadata": {
    "papermill": {
     "duration": 0.004859,
     "end_time": "2026-02-15T16:30:54.385199",
     "exception": false,
     "start_time": "2026-02-15T16:30:54.380340",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Task-Aligned Learning (TAL)\n",
    "\n",
    "Label assignment is the bridge between ground-truth annotations and the thousands of predictions a detector makes. For each ground-truth box, we need to decide which anchor points are \"responsible\" for predicting it.\n",
    "\n",
    "### The problem with simpler strategies\n",
    "\n",
    "- **IoU-based assignment** (used in earlier YOLO versions): assigns anchors based purely on spatial overlap with GT. This ignores whether the model is actually confident about the prediction, leading to misalignment between assignment and model capacity.\n",
    "- **Center-based assignment** (e.g., FCOS): assigns all anchors whose centers fall inside the GT box. Simple but does not consider prediction quality.\n",
    "\n",
    "### Task-Aligned Learning\n",
    "\n",
    "TAL resolves this by computing an **alignment metric** that combines both classification and localization quality:\n",
    "\n",
    "$$t = s^\\alpha \\cdot u^\\beta$$\n",
    "\n",
    "where:\n",
    "- $s$ is the predicted classification score for the GT class\n",
    "- $u$ is the IoU between the predicted box and the GT box\n",
    "- $\\alpha = 1.0$ and $\\beta = 6.0$ control the relative importance (localization is weighted much more heavily)\n",
    "\n",
    "The assignment procedure:\n",
    "1. Filter to anchors whose centers lie inside each GT box\n",
    "2. Compute the alignment metric $t$ for all valid anchor-GT pairs\n",
    "3. Select the **top-k** anchors (default $k=13$) per GT based on $t$\n",
    "4. Resolve conflicts (anchor assigned to multiple GTs) by keeping the highest-alignment GT\n",
    "5. Generate **soft label targets** by normalizing the alignment scores\n",
    "\n",
    "This approach is more effective because it assigns labels to anchors that the model is already doing well on, creating a positive feedback loop that accelerates training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72887d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:54.393590Z",
     "iopub.status.busy": "2026-02-15T16:30:54.393421Z",
     "iopub.status.idle": "2026-02-15T16:30:54.400967Z",
     "shell.execute_reply": "2026-02-15T16:30:54.400454Z"
    },
    "papermill": {
     "duration": 0.011961,
     "end_time": "2026-02-15T16:30:54.401380",
     "exception": false,
     "start_time": "2026-02-15T16:30:54.389419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TaskAlignedAssigner:\n",
    "    \"\"\"Task-Aligned Label Assignment for anchor-free detection.\n",
    "    \n",
    "    Assigns ground truth to predictions using alignment metric\n",
    "    that considers both classification confidence and box IoU.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, topk: int = 13, alpha: float = 1.0, beta: float = 6.0, eps: float = 1e-9):\n",
    "        self.topk = topk\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def assign(self, pred_scores, pred_bboxes, gt_labels, gt_bboxes, anchor_points, stride):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred_scores: (num_anchors, num_classes) predicted class scores (sigmoid)\n",
    "            pred_bboxes: (num_anchors, 4) predicted boxes [x1, y1, x2, y2]\n",
    "            gt_labels: (num_gt,) ground truth class indices\n",
    "            gt_bboxes: (num_gt, 4) ground truth boxes [x1, y1, x2, y2]\n",
    "            anchor_points: (num_anchors, 2) anchor center positions\n",
    "            stride: feature stride for this level\n",
    "        Returns:\n",
    "            assigned_labels: (num_anchors,) -1 for background\n",
    "            assigned_bboxes: (num_anchors, 4) \n",
    "            assigned_scores: (num_anchors, num_classes) soft labels\n",
    "        \"\"\"\n",
    "        device = pred_scores.device\n",
    "        num_anchors = pred_scores.shape[0]\n",
    "        num_gt = gt_bboxes.shape[0]\n",
    "        num_classes = pred_scores.shape[1]\n",
    "        \n",
    "        if num_gt == 0:\n",
    "            return (\n",
    "                torch.full((num_anchors,), -1, dtype=torch.long, device=device),\n",
    "                torch.zeros((num_anchors, 4), device=device),\n",
    "                torch.zeros((num_anchors, num_classes), device=device)\n",
    "            )\n",
    "        \n",
    "        # Check if anchor centers fall inside GT boxes\n",
    "        # anchor_points: (num_anchors, 2) [cx, cy]\n",
    "        lt = anchor_points[:, None, :] - gt_bboxes[None, :, :2]  # (na, ng, 2)\n",
    "        rb = gt_bboxes[None, :, 2:] - anchor_points[:, None, :]  # (na, ng, 2)\n",
    "        in_gt = torch.cat([lt, rb], dim=-1).min(dim=-1).values > 0  # (na, ng)\n",
    "        \n",
    "        # Compute alignment metric\n",
    "        # Get predicted class scores for GT classes\n",
    "        gt_cls_scores = pred_scores[:, gt_labels]  # (na, ng)\n",
    "        \n",
    "        # Compute pairwise IoU\n",
    "        pair_iou = compute_iou(pred_bboxes, gt_bboxes, mode='iou')  # (na, ng)\n",
    "        pair_iou = pair_iou.clamp(0, 1)\n",
    "        \n",
    "        # Alignment metric: score^alpha * iou^beta\n",
    "        alignment = gt_cls_scores.pow(self.alpha) * pair_iou.pow(self.beta)\n",
    "        alignment[~in_gt] = 0  # mask out anchors not inside GT\n",
    "        \n",
    "        # Select top-k anchors per GT\n",
    "        topk_mask = torch.zeros_like(alignment, dtype=torch.bool)\n",
    "        for j in range(num_gt):\n",
    "            vals = alignment[:, j]\n",
    "            k = min(self.topk, (vals > 0).sum().item())\n",
    "            if k > 0:\n",
    "                _, topk_idx = vals.topk(k)\n",
    "                topk_mask[topk_idx, j] = True\n",
    "        \n",
    "        alignment[~topk_mask] = 0\n",
    "        \n",
    "        # Resolve conflicts: each anchor -> highest alignment GT\n",
    "        assigned_gt = alignment.argmax(dim=1)  # (na,)\n",
    "        max_alignment = alignment.max(dim=1).values  # (na,)\n",
    "        \n",
    "        # Background mask\n",
    "        bg_mask = max_alignment < self.eps\n",
    "        assigned_gt[bg_mask] = -1\n",
    "        \n",
    "        # Build outputs\n",
    "        assigned_labels = torch.where(\n",
    "            bg_mask,\n",
    "            torch.tensor(-1, device=device),\n",
    "            gt_labels[assigned_gt.clamp(min=0)]\n",
    "        )\n",
    "        assigned_labels[bg_mask] = -1\n",
    "        assigned_bboxes = torch.zeros((num_anchors, 4), device=device)\n",
    "        assigned_bboxes[~bg_mask] = gt_bboxes[assigned_gt[~bg_mask]]\n",
    "        \n",
    "        # Soft label targets (normalized alignment score)\n",
    "        assigned_scores = torch.zeros((num_anchors, num_classes), device=device)\n",
    "        fg_mask = ~bg_mask\n",
    "        if fg_mask.any():\n",
    "            norm_align = max_alignment[fg_mask] / (max_alignment[fg_mask].max() + self.eps)\n",
    "            assigned_scores[fg_mask, assigned_labels[fg_mask]] = norm_align\n",
    "        \n",
    "        return assigned_labels, assigned_bboxes, assigned_scores\n",
    "\n",
    "print(\"TaskAlignedAssigner ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e164ef4",
   "metadata": {
    "papermill": {
     "duration": 0.002812,
     "end_time": "2026-02-15T16:30:54.407257",
     "exception": false,
     "start_time": "2026-02-15T16:30:54.404445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Composite loss function\n",
    "\n",
    "YOLOv11's loss function combines three complementary objectives:\n",
    "\n",
    "1. **Classification loss** (BCE with soft labels): Binary cross-entropy between predicted class logits and the soft label targets produced by TAL. Soft labels (values between 0 and 1 based on alignment quality) provide richer supervisory signal than hard 0/1 labels.\n",
    "\n",
    "2. **Box regression loss** (CIoU): $\\mathcal{L}_{box} = 1 - \\text{CIoU}(\\hat{b}, b^*)$, applied only to foreground (assigned) anchors. CIoU captures overlap, center distance, and aspect ratio in a single loss term.\n",
    "\n",
    "3. **Distribution Focal Loss** (DFL): Instead of directly regressing LTRB offsets, the DFL head predicts a discrete probability distribution over integer bins $\\{0, 1, \\ldots, \\text{reg\\_max}-1\\}$. The DFL loss is a weighted cross-entropy between adjacent bins:\n",
    "\n",
    "$$\\mathcal{L}_{DFL}(S_i, S_{i+1}, y) = -(1 - (y - i)) \\log(S_i) - (y - i) \\log(S_{i+1})$$\n",
    "\n",
    "where $y$ is the continuous target offset, $i = \\lfloor y \\rfloor$, and $S_i$ is the softmax probability for bin $i$.\n",
    "\n",
    "The total loss is a weighted sum:\n",
    "\n",
    "$$\\mathcal{L} = \\lambda_{cls} \\cdot \\mathcal{L}_{cls} + \\lambda_{box} \\cdot \\mathcal{L}_{box} + \\lambda_{dfl} \\cdot \\mathcal{L}_{dfl}$$\n",
    "\n",
    "with default weights $\\lambda_{cls} = 0.5$, $\\lambda_{box} = 7.5$, $\\lambda_{dfl} = 1.5$. The high box weight reflects the importance of precise localization in object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5f264f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:54.414020Z",
     "iopub.status.busy": "2026-02-15T16:30:54.413872Z",
     "iopub.status.idle": "2026-02-15T16:30:54.425464Z",
     "shell.execute_reply": "2026-02-15T16:30:54.424978Z"
    },
    "papermill": {
     "duration": 0.015932,
     "end_time": "2026-02-15T16:30:54.425932",
     "exception": false,
     "start_time": "2026-02-15T16:30:54.410000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class YOLOv11Loss(nn.Module):\n",
    "    \"\"\"Composite loss for YOLOv11 training.\n",
    "    \n",
    "    Components:\n",
    "        1. Classification: BCE with soft labels from TAL\n",
    "        2. Box regression: CIoU loss\n",
    "        3. Distribution Focal Loss: cross-entropy on DFL bins\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = 80, reg_max: int = 16, strides: List[int] = [8, 16, 32],\n",
    "                 cls_weight: float = 0.5, box_weight: float = 7.5, dfl_weight: float = 1.5):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.reg_max = reg_max\n",
    "        self.strides = strides\n",
    "        self.cls_weight = cls_weight\n",
    "        self.box_weight = box_weight\n",
    "        self.dfl_weight = dfl_weight\n",
    "        self.assigner = TaskAlignedAssigner()\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    \n",
    "    def _make_anchor_points(self, feat_sizes, device):\n",
    "        \"\"\"Generate anchor points for all feature levels.\"\"\"\n",
    "        all_points = []\n",
    "        all_strides = []\n",
    "        for (h, w), stride in zip(feat_sizes, self.strides):\n",
    "            sy, sx = torch.meshgrid(torch.arange(h), torch.arange(w), indexing='ij')\n",
    "            points = torch.stack([sx.flatten(), sy.flatten()], dim=-1).float()\n",
    "            points = (points + 0.5) * stride  # center of each cell in image coords\n",
    "            all_points.append(points)\n",
    "            all_strides.append(torch.full((h * w,), stride, dtype=torch.float32))\n",
    "        return torch.cat(all_points).to(device), torch.cat(all_strides).to(device)\n",
    "    \n",
    "    def _decode_boxes(self, box_pred, anchor_points, strides):\n",
    "        \"\"\"Decode LTRB offsets to x1y1x2y2 boxes.\"\"\"\n",
    "        lt = box_pred[:, :2] * strides.unsqueeze(-1)\n",
    "        rb = box_pred[:, 2:] * strides.unsqueeze(-1)\n",
    "        x1y1 = anchor_points - lt\n",
    "        x2y2 = anchor_points + rb\n",
    "        return torch.cat([x1y1, x2y2], dim=-1)\n",
    "    \n",
    "    def forward(self, predictions, gt_boxes_list, gt_labels_list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: list of (cls_pred, box_pred, box_raw) per scale\n",
    "            gt_boxes_list: list of (num_gt, 4) per image, normalized [cx, cy, w, h]\n",
    "            gt_labels_list: list of (num_gt,) per image\n",
    "        \"\"\"\n",
    "        device = predictions[0][0].device\n",
    "        batch_size = predictions[0][0].shape[0]\n",
    "        \n",
    "        feat_sizes = [(p[0].shape[2], p[0].shape[3]) for p in predictions]\n",
    "        anchor_points, anchor_strides = self._make_anchor_points(feat_sizes, device)\n",
    "        \n",
    "        # Concatenate predictions across scales\n",
    "        all_cls = torch.cat([p[0].flatten(2).permute(0, 2, 1) for p in predictions], dim=1)\n",
    "        all_box = torch.cat([p[1].flatten(2).permute(0, 2, 1) for p in predictions], dim=1)\n",
    "        all_raw = torch.cat([p[2].flatten(2).permute(0, 2, 1) for p in predictions], dim=1)\n",
    "        \n",
    "        total_cls_loss = torch.tensor(0.0, device=device)\n",
    "        total_box_loss = torch.tensor(0.0, device=device)\n",
    "        total_dfl_loss = torch.tensor(0.0, device=device)\n",
    "        num_pos = 0\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            cls_pred = all_cls[b].sigmoid()  # (num_anchors, num_classes)\n",
    "            box_pred = all_box[b]            # (num_anchors, 4) LTRB\n",
    "            raw_pred = all_raw[b]            # (num_anchors, 4*reg_max)\n",
    "            \n",
    "            # Decode predicted boxes\n",
    "            pred_bboxes = self._decode_boxes(box_pred, anchor_points, anchor_strides)\n",
    "            \n",
    "            gt_boxes = gt_boxes_list[b]\n",
    "            gt_labels = gt_labels_list[b]\n",
    "            \n",
    "            if len(gt_boxes) == 0:\n",
    "                total_cls_loss += self.bce(all_cls[b], torch.zeros_like(all_cls[b])).sum()\n",
    "                continue\n",
    "            \n",
    "            # Convert GT from [cx, cy, w, h] normalized to [x1, y1, x2, y2] pixel\n",
    "            gt_xyxy = torch.zeros_like(gt_boxes)\n",
    "            gt_xyxy[:, 0] = (gt_boxes[:, 0] - gt_boxes[:, 2] / 2) * 640\n",
    "            gt_xyxy[:, 1] = (gt_boxes[:, 1] - gt_boxes[:, 3] / 2) * 640\n",
    "            gt_xyxy[:, 2] = (gt_boxes[:, 0] + gt_boxes[:, 2] / 2) * 640\n",
    "            gt_xyxy[:, 3] = (gt_boxes[:, 1] + gt_boxes[:, 3] / 2) * 640\n",
    "            \n",
    "            # Task-aligned assignment\n",
    "            assigned_labels, assigned_bboxes, assigned_scores = self.assigner.assign(\n",
    "                cls_pred, pred_bboxes, gt_labels.long(), gt_xyxy,\n",
    "                anchor_points, anchor_strides\n",
    "            )\n",
    "            \n",
    "            fg_mask = assigned_labels >= 0\n",
    "            num_fg = fg_mask.sum().item()\n",
    "            num_pos += num_fg\n",
    "            \n",
    "            # Classification loss (BCE with soft labels)\n",
    "            cls_targets = assigned_scores.to(device)\n",
    "            total_cls_loss += self.bce(all_cls[b], cls_targets).sum()\n",
    "            \n",
    "            if num_fg > 0:\n",
    "                # Box loss (CIoU)\n",
    "                fg_pred_boxes = pred_bboxes[fg_mask]\n",
    "                fg_gt_boxes = assigned_bboxes[fg_mask].to(device)\n",
    "                ciou = compute_iou(fg_pred_boxes, fg_gt_boxes, mode='ciou')\n",
    "                ciou_diag = torch.diag(ciou)\n",
    "                total_box_loss += (1.0 - ciou_diag).sum()\n",
    "                \n",
    "                # DFL loss\n",
    "                fg_raw = raw_pred[fg_mask]  # (num_fg, 4*reg_max)\n",
    "                fg_raw = fg_raw.view(-1, self.reg_max)  # (num_fg*4, reg_max)\n",
    "                # Target: continuous LTRB offsets\n",
    "                fg_target_ltrb = torch.zeros((num_fg, 4), device=device)\n",
    "                fg_target_ltrb[:, :2] = (anchor_points[fg_mask] - fg_gt_boxes[:, :2]) / anchor_strides[fg_mask].unsqueeze(-1)\n",
    "                fg_target_ltrb[:, 2:] = (fg_gt_boxes[:, 2:] - anchor_points[fg_mask]) / anchor_strides[fg_mask].unsqueeze(-1)\n",
    "                fg_target_ltrb = fg_target_ltrb.clamp(0, self.reg_max - 1 - 0.01)\n",
    "                target_flat = fg_target_ltrb.view(-1)\n",
    "                # DFL: cross-entropy between adjacent integer bins\n",
    "                target_left = target_flat.long()\n",
    "                target_right = target_left + 1\n",
    "                weight_right = target_flat - target_left.float()\n",
    "                weight_left = 1.0 - weight_right\n",
    "                dfl_loss = (\n",
    "                    F.cross_entropy(fg_raw, target_left, reduction='none') * weight_left +\n",
    "                    F.cross_entropy(fg_raw, target_right.clamp(max=self.reg_max - 1), reduction='none') * weight_right\n",
    "                )\n",
    "                total_dfl_loss += dfl_loss.sum()\n",
    "        \n",
    "        num_pos = max(num_pos, 1)\n",
    "        loss_cls = self.cls_weight * total_cls_loss / num_pos\n",
    "        loss_box = self.box_weight * total_box_loss / num_pos\n",
    "        loss_dfl = self.dfl_weight * total_dfl_loss / num_pos\n",
    "        total_loss = loss_cls + loss_box + loss_dfl\n",
    "        \n",
    "        return total_loss, {\n",
    "            'cls_loss': loss_cls.item(),\n",
    "            'box_loss': loss_box.item(),\n",
    "            'dfl_loss': loss_dfl.item(),\n",
    "            'total_loss': total_loss.item(),\n",
    "            'num_pos': num_pos\n",
    "        }\n",
    "\n",
    "print(\"YOLOv11Loss ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5183a1fd",
   "metadata": {
    "papermill": {
     "duration": 0.002899,
     "end_time": "2026-02-15T16:30:54.431839",
     "exception": false,
     "start_time": "2026-02-15T16:30:54.428940",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Real COCO data for training\n",
    "\n",
    "Instead of training on synthetic colored rectangles, we stream real COCO images from [detection-datasets/coco](https://huggingface.co/datasets/detection-datasets/coco) on the Hugging Face Hub. We buffer 32 images in memory for this demo to keep training fast while using real-world data.\n",
    "\n",
    "> **Data source**: Images streamed from [detection-datasets/coco](https://huggingface.co/datasets/detection-datasets/coco). See our [HF COCO streaming tutorial](/blog/tutorials/hf-coco-streaming) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea5a55f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:54.438738Z",
     "iopub.status.busy": "2026-02-15T16:30:54.438584Z",
     "iopub.status.idle": "2026-02-15T16:31:08.168207Z",
     "shell.execute_reply": "2026-02-15T16:31:08.167734Z"
    },
    "papermill": {
     "duration": 13.743161,
     "end_time": "2026-02-15T16:31:08.177879",
     "exception": false,
     "start_time": "2026-02-15T16:30:54.434718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class COCOStreamDetectionDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Buffer real COCO images from HF streaming for detection training.\n",
    "\n",
    "    Pre-fetches max_samples images via streaming and stores them in memory,\n",
    "    providing random access and len() support for the DataLoader.\n",
    "    Annotations are converted to YOLO format [cx, cy, w, h] normalized.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split='train', max_samples=32, img_size=640, num_classes=80):\n",
    "        self.img_size = img_size\n",
    "        self.num_classes = num_classes\n",
    "        self.data = []\n",
    "\n",
    "        print(f\"Streaming {max_samples} COCO images from Hugging Face...\")\n",
    "        ds = load_dataset('detection-datasets/coco', split=split, streaming=True)\n",
    "\n",
    "        for example in ds:\n",
    "            if len(self.data) >= max_samples:\n",
    "                break\n",
    "\n",
    "            img_pil = example['image'].convert('RGB')\n",
    "            img_np = np.array(img_pil)\n",
    "            h, w = img_np.shape[:2]\n",
    "\n",
    "            bboxes = example['objects']['bbox']\n",
    "            cats = example['objects']['category']\n",
    "\n",
    "            boxes = []\n",
    "            labels = []\n",
    "            for bbox, cat_id in zip(bboxes, cats):\n",
    "                bx, by, bw, bh = bbox\n",
    "                if bw <= 0 or bh <= 0:\n",
    "                    continue\n",
    "                cx = (bx + bw / 2) / w\n",
    "                cy = (by + bh / 2) / h\n",
    "                boxes.append([cx, cy, bw / w, bh / h])\n",
    "                labels.append(int(cat_id))\n",
    "\n",
    "            if len(boxes) == 0:\n",
    "                continue\n",
    "\n",
    "            # Resize to model input size\n",
    "            img_resized = np.array(img_pil.resize((self.img_size, self.img_size)))\n",
    "            img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "            boxes_t = torch.tensor(boxes, dtype=torch.float32)\n",
    "            labels_t = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "            self.data.append((img_tensor, boxes_t, labels_t))\n",
    "\n",
    "        print(f\"Buffered {len(self.data)} COCO images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate: images are stacked, boxes/labels stay as lists.\"\"\"\n",
    "    imgs, boxes, labels = zip(*batch)\n",
    "    imgs = torch.stack(imgs, dim=0)\n",
    "    targets = torch.zeros(len(imgs))\n",
    "    return imgs, targets, list(boxes), list(labels)\n",
    "\n",
    "\n",
    "# Create dataset and dataloader with real COCO images\n",
    "dataset = COCOStreamDetectionDataset(max_samples=32, num_classes=80)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Visualize samples\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for i, ax in enumerate(axes):\n",
    "    img, boxes, labels = dataset[i]\n",
    "    ax.imshow(img.permute(1, 2, 0).numpy())\n",
    "    ax.set_title(f'Image {i}: {len(boxes)} objects')\n",
    "    ax.axis('off')\n",
    "    for box in boxes:\n",
    "        cx, cy, w, h = box.numpy() * 640\n",
    "        rect = plt.Rectangle((cx - w/2, cy - h/2), w, h,\n",
    "                             linewidth=2, edgecolor='white', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "plt.suptitle('Real COCO Training Data', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dataset: {len(dataset)} images, DataLoader: {len(loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3134809d",
   "metadata": {
    "papermill": {
     "duration": 0.017214,
     "end_time": "2026-02-15T16:31:08.212640",
     "exception": false,
     "start_time": "2026-02-15T16:31:08.195426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training loop\n",
    "\n",
    "The training loop follows a standard PyTorch pattern with a few detection-specific details:\n",
    "\n",
    "- **Gradient clipping** (`max_norm=10.0`) prevents exploding gradients, which can occur when the loss components have very different magnitudes early in training.\n",
    "- We track individual loss components (classification, box, DFL) to diagnose training behavior.\n",
    "- The number of positive (foreground) assignments per batch is logged to ensure the assigner is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478d58c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:31:08.248534Z",
     "iopub.status.busy": "2026-02-15T16:31:08.248261Z",
     "iopub.status.idle": "2026-02-15T16:31:08.253019Z",
     "shell.execute_reply": "2026-02-15T16:31:08.252302Z"
    },
    "papermill": {
     "duration": 0.023756,
     "end_time": "2026-02-15T16:31:08.253527",
     "exception": false,
     "start_time": "2026-02-15T16:31:08.229771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, device, epoch):\n",
    "    model.train()\n",
    "    epoch_losses = {'cls_loss': 0, 'box_loss': 0, 'dfl_loss': 0, 'total_loss': 0}\n",
    "    \n",
    "    for batch_idx, (imgs, targets, boxes_list, labels_list) in enumerate(dataloader):\n",
    "        imgs = imgs.to(device)\n",
    "        \n",
    "        # Move GT to device\n",
    "        gt_boxes = [b.to(device) for b in boxes_list]\n",
    "        gt_labels = [l.to(device) for l in labels_list]\n",
    "        \n",
    "        # Forward\n",
    "        predictions = model(imgs)\n",
    "        loss, loss_dict = loss_fn(predictions, gt_boxes, gt_labels)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        for k in epoch_losses:\n",
    "            epoch_losses[k] += loss_dict[k]\n",
    "        \n",
    "        if batch_idx % 2 == 0:\n",
    "            print(f\"  Batch {batch_idx}: loss={loss_dict['total_loss']:.4f} \"\n",
    "                  f\"(cls={loss_dict['cls_loss']:.4f}, box={loss_dict['box_loss']:.4f}, \"\n",
    "                  f\"dfl={loss_dict['dfl_loss']:.4f}, pos={loss_dict['num_pos']})\")\n",
    "    \n",
    "    n = len(dataloader)\n",
    "    return {k: v / n for k, v in epoch_losses.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5695e8f7",
   "metadata": {
    "papermill": {
     "duration": 0.017558,
     "end_time": "2026-02-15T16:31:08.288467",
     "exception": false,
     "start_time": "2026-02-15T16:31:08.270909",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Running the training demo\n",
    "\n",
    "We train for 5 epochs on our tiny synthetic dataset. The goal is not to achieve good detection performance (that requires real data and many more epochs), but to verify that:\n",
    "\n",
    "1. The forward pass produces valid predictions at all three scales\n",
    "2. The TAL assigner finds positive anchors for the ground-truth boxes\n",
    "3. All three loss components produce valid gradients\n",
    "4. The total loss decreases over training\n",
    "\n",
    "We use AdamW with cosine annealing, which is standard for YOLO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f2996",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:31:08.324709Z",
     "iopub.status.busy": "2026-02-15T16:31:08.324455Z",
     "iopub.status.idle": "2026-02-15T16:31:22.265351Z",
     "shell.execute_reply": "2026-02-15T16:31:22.264949Z"
    },
    "papermill": {
     "duration": 13.959997,
     "end_time": "2026-02-15T16:31:22.265954",
     "exception": false,
     "start_time": "2026-02-15T16:31:08.305957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Small-scale training demo with real COCO data\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = YOLOv11(num_classes=80).to(device)\n",
    "loss_fn = YOLOv11Loss(num_classes=80)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.05)\n",
    "\n",
    "# Cosine LR scheduler\n",
    "num_epochs = 5\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Training for {num_epochs} epochs on {len(dataset)} real COCO images\\n\")\n",
    "\n",
    "history = []\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs} (lr={scheduler.get_last_lr()[0]:.6f})\")\n",
    "    epoch_loss = train_one_epoch(model, loader, optimizer, loss_fn, device, epoch)\n",
    "    scheduler.step()\n",
    "    history.append(epoch_loss)\n",
    "    print(f\"  -> Avg loss: {epoch_loss['total_loss']:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc8fa4f",
   "metadata": {
    "papermill": {
     "duration": 0.018284,
     "end_time": "2026-02-15T16:31:22.303006",
     "exception": false,
     "start_time": "2026-02-15T16:31:22.284722",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loss curves visualization\n",
    "\n",
    "Plotting the individual loss components over training helps diagnose issues:\n",
    "\n",
    "- **Classification loss** should decrease as the model learns to distinguish object classes from background\n",
    "- **Box (CIoU) loss** should decrease as predicted boxes align better with ground truth\n",
    "- **DFL loss** should decrease as the distribution predictions sharpen around the correct offsets\n",
    "\n",
    "If one component plateaus while others decrease, it may indicate an imbalance in loss weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0de9c35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:31:22.340531Z",
     "iopub.status.busy": "2026-02-15T16:31:22.340183Z",
     "iopub.status.idle": "2026-02-15T16:31:22.595413Z",
     "shell.execute_reply": "2026-02-15T16:31:22.594841Z"
    },
    "papermill": {
     "duration": 0.274739,
     "end_time": "2026-02-15T16:31:22.595871",
     "exception": false,
     "start_time": "2026-02-15T16:31:22.321132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "keys = ['total_loss', 'cls_loss', 'box_loss', 'dfl_loss']\n",
    "titles = ['Total Loss', 'Classification Loss', 'Box (CIoU) Loss', 'DFL Loss']\n",
    "\n",
    "for ax, key, title in zip(axes, keys, titles):\n",
    "    values = [h[key] for h in history]\n",
    "    ax.plot(range(1, len(values)+1), values, 'b-o', linewidth=2)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Training Loss Curves', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46139a03",
   "metadata": {
    "papermill": {
     "duration": 0.019455,
     "end_time": "2026-02-15T16:31:22.634241",
     "exception": false,
     "start_time": "2026-02-15T16:31:22.614786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we built the complete training pipeline for YOLOv11 from scratch. Here is a recap of the key components:\n",
    "\n",
    "1. **CIoU** provides complete geometric alignment by combining overlap area, center-point distance, and aspect ratio consistency into a single differentiable metric. This gives the optimizer rich gradient information for box regression, unlike basic IoU which produces zero gradients for non-overlapping boxes.\n",
    "\n",
    "2. **Task-Aligned Learning (TAL)** assigns ground-truth boxes to anchor points based on both classification confidence and localization quality. The alignment metric $t = s^\\alpha \\cdot u^\\beta$ ensures that labels are assigned to anchors where the model is already performing well, creating a virtuous cycle during training.\n",
    "\n",
    "3. **Distribution Focal Loss (DFL)** enables precise box regression by predicting a probability distribution over discrete offset bins rather than a single scalar. The weighted cross-entropy between adjacent bins preserves the continuous nature of the target.\n",
    "\n",
    "4. The **composite loss** balances classification ($\\lambda = 0.5$), localization ($\\lambda = 7.5$), and distribution quality ($\\lambda = 1.5$). The heavy weight on box regression reflects the critical importance of precise localization in object detection.\n",
    "\n",
    "### Next steps\n",
    "\n",
    "In **Notebook 5**, we will build the inference pipeline: decoding predictions into bounding boxes, applying Non-Maximum Suppression (NMS), and evaluating detection quality using COCO metrics (mAP, AP50, AP75)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 32.844227,
   "end_time": "2026-02-15T16:31:23.671763",
   "environment_variables": {},
   "exception": null,
   "input_path": "notebooks/scene-understanding/object-detection/yolo/pytorch/04_loss_and_training.ipynb",
   "output_path": "notebooks/scene-understanding/object-detection/yolo/pytorch/04_loss_and_training-executed.ipynb",
   "parameters": {},
   "start_time": "2026-02-15T16:30:50.827536",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
