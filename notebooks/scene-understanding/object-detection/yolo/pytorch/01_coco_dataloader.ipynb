{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8823d335",
   "metadata": {
    "papermill": {
     "duration": 0.006583,
     "end_time": "2026-02-15T16:30:14.676775",
     "exception": false,
     "start_time": "2026-02-15T16:30:14.670192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# COCO Data Pipeline for Anchor-Free Detection\n",
    "\n",
    "*Notebook 1 of 5 in the YOLOv11 from-scratch series*\n",
    "\n",
    "---\n",
    "\n",
    "Modern YOLO detectors require a specialized data pipeline that goes well beyond simple image loading. The pipeline must handle several responsibilities:\n",
    "\n",
    "- **Parsing** COCO-format annotations and mapping non-contiguous category IDs to a contiguous range\n",
    "- **Resizing** images via letterboxing to preserve aspect ratio while fitting a fixed input resolution\n",
    "- **Augmenting** training data with techniques like mosaic augmentation to increase object diversity per sample\n",
    "- **Encoding** ground-truth bounding boxes into multi-scale target tensors suitable for anchor-free detection heads\n",
    "\n",
    "In this notebook we build a complete COCO data pipeline for YOLOv11 training. The pipeline produces:\n",
    "\n",
    "| Output | Grid Size | Stride | Object Scale |\n",
    "|--------|-----------|--------|--------------|\n",
    "| P3     | 80 x 80   | 8      | Small        |\n",
    "| P4     | 40 x 40   | 16     | Medium       |\n",
    "| P5     | 20 x 20   | 32     | Large        |\n",
    "\n",
    "All outputs assume a 640 x 640 input resolution. By the end of this notebook you will have a `DataLoader` that yields image tensors paired with multi-scale target grids ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Colab Environment Setup ---\n",
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    %pip install -q matplotlib seaborn scikit-learn scipy tqdm datasets\n",
    "    print(\"Colab dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fd2cfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:14.688035Z",
     "iopub.status.busy": "2026-02-15T16:30:14.687829Z",
     "iopub.status.idle": "2026-02-15T16:30:16.516765Z",
     "shell.execute_reply": "2026-02-15T16:30:16.516027Z"
    },
    "papermill": {
     "duration": 1.83563,
     "end_time": "2026-02-15T16:30:16.517760",
     "exception": false,
     "start_time": "2026-02-15T16:30:14.682130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, json, random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Configuration\n",
    "IMG_SIZE = 640\n",
    "NUM_CLASSES = 80\n",
    "STRIDES = [8, 16, 32]  # P3, P4, P5\n",
    "GRID_SIZES = [IMG_SIZE // s for s in STRIDES]  # 80, 40, 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea03f12",
   "metadata": {
    "papermill": {
     "duration": 0.002203,
     "end_time": "2026-02-15T16:30:16.522590",
     "exception": false,
     "start_time": "2026-02-15T16:30:16.520387",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## COCO annotation format\n",
    "\n",
    "The COCO (Common Objects in Context) dataset uses a JSON annotation format with three top-level keys:\n",
    "\n",
    "- **`images`** -- a list of image metadata entries, each containing an `id`, `file_name`, `width`, and `height`.\n",
    "- **`annotations`** -- a list of object annotations. Each annotation links to an image via `image_id` and contains a `bbox` in **top-left `[x, y, width, height]`** format, a `category_id`, and an `iscrowd` flag.\n",
    "- **`categories`** -- a list of category definitions mapping `id` to `name`.\n",
    "\n",
    "One important detail: COCO category IDs are **not contiguous**. For example, category IDs might jump from 1 to 16. We need to build a mapping from the original IDs to a contiguous `0..N-1` range for use in classification targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef256fd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:16.528225Z",
     "iopub.status.busy": "2026-02-15T16:30:16.527963Z",
     "iopub.status.idle": "2026-02-15T16:30:16.533199Z",
     "shell.execute_reply": "2026-02-15T16:30:16.532500Z"
    },
    "papermill": {
     "duration": 0.009096,
     "end_time": "2026-02-15T16:30:16.533777",
     "exception": false,
     "start_time": "2026-02-15T16:30:16.524681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class COCOParser:\n",
    "    \"\"\"Parse COCO-format annotations.\"\"\"\n",
    "\n",
    "    def __init__(self, annotation_file: str, image_dir: str):\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            coco = json.load(f)\n",
    "\n",
    "        self.image_dir = image_dir\n",
    "        self.images = {img['id']: img for img in coco['images']}\n",
    "\n",
    "        # Build category mapping (COCO IDs are not contiguous)\n",
    "        cat_ids = sorted([c['id'] for c in coco['categories']])\n",
    "        self.cat_id_to_continuous = {cid: i for i, cid in enumerate(cat_ids)}\n",
    "        self.categories = {c['id']: c['name'] for c in coco['categories']}\n",
    "\n",
    "        # Group annotations by image\n",
    "        self.img_annotations = {}\n",
    "        for ann in coco['annotations']:\n",
    "            if ann.get('iscrowd', 0):\n",
    "                continue\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.img_annotations:\n",
    "                self.img_annotations[img_id] = []\n",
    "            self.img_annotations[img_id].append(ann)\n",
    "\n",
    "        # Only keep images that have annotations\n",
    "        self.img_ids = [iid for iid in self.images if iid in self.img_annotations]\n",
    "        print(f\"Loaded {len(self.img_ids)} images with \"\n",
    "              f\"{sum(len(v) for v in self.img_annotations.values())} annotations\")\n",
    "\n",
    "    def get_image_path(self, img_id: int) -> str:\n",
    "        return os.path.join(self.image_dir, self.images[img_id]['file_name'])\n",
    "\n",
    "    def get_annotations(self, img_id: int) -> List[Dict]:\n",
    "        return self.img_annotations.get(img_id, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cc7f24",
   "metadata": {
    "papermill": {
     "duration": 0.002164,
     "end_time": "2026-02-15T16:30:16.538382",
     "exception": false,
     "start_time": "2026-02-15T16:30:16.536218",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Letterbox resizing\n",
    "\n",
    "YOLO models expect a fixed square input (640 x 640). Naively resizing images to this shape would distort their aspect ratio, which can hurt detection accuracy -- especially for objects with extreme aspect ratios.\n",
    "\n",
    "**Letterboxing** solves this by:\n",
    "\n",
    "1. Scaling the image so its longest side matches the target size.\n",
    "2. Padding the shorter side symmetrically with a neutral gray value (114) to form a square.\n",
    "\n",
    "This preserves the original aspect ratio while fitting the model's input dimensions. The bounding box coordinates must be adjusted to account for both the scale factor and the padding offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64166e40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:16.543607Z",
     "iopub.status.busy": "2026-02-15T16:30:16.543477Z",
     "iopub.status.idle": "2026-02-15T16:30:16.547742Z",
     "shell.execute_reply": "2026-02-15T16:30:16.547222Z"
    },
    "papermill": {
     "duration": 0.007636,
     "end_time": "2026-02-15T16:30:16.548209",
     "exception": false,
     "start_time": "2026-02-15T16:30:16.540573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def letterbox_resize(image: np.ndarray, target_size: int = 640\n",
    "                     ) -> Tuple[np.ndarray, float, Tuple[int, int]]:\n",
    "    \"\"\"Resize image with letterboxing (preserve aspect ratio, pad to square).\n",
    "\n",
    "    Returns:\n",
    "        resized_image: (target_size, target_size, 3) uint8 array\n",
    "        scale: resize scale factor\n",
    "        pad: (pad_w, pad_h) padding applied\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    scale = target_size / max(h, w)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "\n",
    "    resized = np.array(Image.fromarray(image).resize((new_w, new_h), Image.BILINEAR))\n",
    "\n",
    "    # Create padded image (gray padding = 114)\n",
    "    padded = np.full((target_size, target_size, 3), 114, dtype=np.uint8)\n",
    "    pad_w = (target_size - new_w) // 2\n",
    "    pad_h = (target_size - new_h) // 2\n",
    "    padded[pad_h:pad_h + new_h, pad_w:pad_w + new_w] = resized\n",
    "\n",
    "    return padded, scale, (pad_w, pad_h)\n",
    "\n",
    "\n",
    "def adjust_boxes_for_letterbox(boxes: np.ndarray, scale: float,\n",
    "                                pad: Tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"Adjust bounding boxes after letterbox resize.\n",
    "\n",
    "    Args:\n",
    "        boxes: (N, 4) in [x_center, y_center, w, h] format (original pixel coords)\n",
    "        scale: letterbox scale factor\n",
    "        pad: (pad_w, pad_h)\n",
    "    Returns:\n",
    "        adjusted: (N, 4) in [x_center, y_center, w, h] in letterboxed image coords\n",
    "    \"\"\"\n",
    "    adjusted = boxes.copy().astype(np.float32)\n",
    "    adjusted[:, 0] = boxes[:, 0] * scale + pad[0]  # x_center\n",
    "    adjusted[:, 1] = boxes[:, 1] * scale + pad[1]  # y_center\n",
    "    adjusted[:, 2] = boxes[:, 2] * scale            # width\n",
    "    adjusted[:, 3] = boxes[:, 3] * scale            # height\n",
    "    return adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df1beeb",
   "metadata": {
    "papermill": {
     "duration": 0.00294,
     "end_time": "2026-02-15T16:30:16.554190",
     "exception": false,
     "start_time": "2026-02-15T16:30:16.551250",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Mosaic augmentation\n",
    "\n",
    "Mosaic augmentation was introduced in YOLOv4 and remains a staple in modern YOLO training. The idea is simple but powerful: combine four randomly selected training images into a single composite image by placing each in one quadrant.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- **More objects per sample** -- the model sees objects from four images in a single forward pass, which improves gradient quality.\n",
    "- **Context diversity** -- objects appear against varied backgrounds and alongside different neighbors.\n",
    "- **Reduced batch size dependence** -- because each sample is richer, you can train effectively with smaller batches.\n",
    "- **Scale variation** -- objects end up at a wider range of scales than they would in isolated images.\n",
    "\n",
    "The mosaic center is randomized to prevent the model from learning a fixed spatial prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38c156",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:16.559249Z",
     "iopub.status.busy": "2026-02-15T16:30:16.559121Z",
     "iopub.status.idle": "2026-02-15T16:30:16.569801Z",
     "shell.execute_reply": "2026-02-15T16:30:16.569320Z"
    },
    "papermill": {
     "duration": 0.014045,
     "end_time": "2026-02-15T16:30:16.570320",
     "exception": false,
     "start_time": "2026-02-15T16:30:16.556275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mosaic_augmentation(dataset, indices: List[int],\n",
    "                        img_size: int = 640\n",
    "                        ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Create a mosaic from 4 images.\n",
    "\n",
    "    Returns:\n",
    "        mosaic_img: (img_size, img_size, 3)\n",
    "        mosaic_boxes: (N, 4) [x_center, y_center, w, h] normalized to [0,1]\n",
    "        mosaic_labels: (N,) class indices\n",
    "    \"\"\"\n",
    "    cx, cy = img_size // 2, img_size // 2  # mosaic center\n",
    "    # Add random offset for variety\n",
    "    cx += random.randint(-img_size // 4, img_size // 4)\n",
    "    cy += random.randint(-img_size // 4, img_size // 4)\n",
    "\n",
    "    mosaic_img = np.full((img_size, img_size, 3), 114, dtype=np.uint8)\n",
    "    all_boxes = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        img, boxes, labels = dataset.load_raw(idx)\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        # Determine placement in mosaic quadrant\n",
    "        if i == 0:    # top-left\n",
    "            x1, y1, x2, y2 = max(cx - w, 0), max(cy - h, 0), cx, cy\n",
    "            crop_x1, crop_y1 = w - (x2 - x1), h - (y2 - y1)\n",
    "            crop_x2, crop_y2 = w, h\n",
    "        elif i == 1:  # top-right\n",
    "            x1, y1, x2, y2 = cx, max(cy - h, 0), min(cx + w, img_size), cy\n",
    "            crop_x1, crop_y1 = 0, h - (y2 - y1)\n",
    "            crop_x2, crop_y2 = x2 - x1, h\n",
    "        elif i == 2:  # bottom-left\n",
    "            x1, y1, x2, y2 = max(cx - w, 0), cy, cx, min(cy + h, img_size)\n",
    "            crop_x1, crop_y1 = w - (x2 - x1), 0\n",
    "            crop_x2, crop_y2 = w, y2 - y1\n",
    "        else:         # bottom-right\n",
    "            x1, y1, x2, y2 = cx, cy, min(cx + w, img_size), min(cy + h, img_size)\n",
    "            crop_x1, crop_y1 = 0, 0\n",
    "            crop_x2, crop_y2 = x2 - x1, y2 - y1\n",
    "\n",
    "        mosaic_img[y1:y2, x1:x2] = img[crop_y1:crop_y2, crop_x1:crop_x2]\n",
    "\n",
    "        # Adjust boxes: convert from normalized [0,1] to pixel coords in mosaic\n",
    "        if len(boxes) > 0:\n",
    "            pixel_boxes = boxes.copy()\n",
    "            pixel_boxes[:, 0] = boxes[:, 0] * w - crop_x1 + x1  # x_center\n",
    "            pixel_boxes[:, 1] = boxes[:, 1] * h - crop_y1 + y1  # y_center\n",
    "            pixel_boxes[:, 2] = boxes[:, 2] * w                   # width\n",
    "            pixel_boxes[:, 3] = boxes[:, 3] * h                   # height\n",
    "            all_boxes.append(pixel_boxes)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    if all_boxes:\n",
    "        all_boxes = np.concatenate(all_boxes, axis=0)\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "        # Clip to mosaic bounds and filter invalid\n",
    "        x1 = all_boxes[:, 0] - all_boxes[:, 2] / 2\n",
    "        y1 = all_boxes[:, 1] - all_boxes[:, 3] / 2\n",
    "        x2 = all_boxes[:, 0] + all_boxes[:, 2] / 2\n",
    "        y2 = all_boxes[:, 1] + all_boxes[:, 3] / 2\n",
    "        x1 = np.clip(x1, 0, img_size)\n",
    "        y1 = np.clip(y1, 0, img_size)\n",
    "        x2 = np.clip(x2, 0, img_size)\n",
    "        y2 = np.clip(y2, 0, img_size)\n",
    "\n",
    "        all_boxes[:, 2] = x2 - x1\n",
    "        all_boxes[:, 3] = y2 - y1\n",
    "        all_boxes[:, 0] = (x1 + x2) / 2\n",
    "        all_boxes[:, 1] = (y1 + y2) / 2\n",
    "\n",
    "        # Filter out tiny boxes\n",
    "        valid = (all_boxes[:, 2] > 2) & (all_boxes[:, 3] > 2)\n",
    "        all_boxes = all_boxes[valid]\n",
    "        all_labels = all_labels[valid]\n",
    "\n",
    "        # Normalize to [0, 1]\n",
    "        all_boxes[:, [0, 2]] /= img_size\n",
    "        all_boxes[:, [1, 3]] /= img_size\n",
    "    else:\n",
    "        all_boxes = np.zeros((0, 4), dtype=np.float32)\n",
    "        all_labels = np.zeros((0,), dtype=np.int64)\n",
    "\n",
    "    return mosaic_img, all_boxes, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841add1d",
   "metadata": {
    "papermill": {
     "duration": 0.003474,
     "end_time": "2026-02-15T16:30:16.576953",
     "exception": false,
     "start_time": "2026-02-15T16:30:16.573479",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Multi-scale target encoding\n",
    "\n",
    "YOLOv11 uses an **anchor-free** detection paradigm. Instead of pre-defined anchor boxes, each grid cell directly predicts whether it contains an object center and, if so, the bounding box parameters.\n",
    "\n",
    "The target encoding works as follows:\n",
    "\n",
    "1. **Scale assignment** -- each ground-truth box is assigned to the feature pyramid level (P3, P4, or P5) whose receptive field best matches the box size. Small objects (up to 64 px) go to P3, medium objects (65-128 px) to P4, and large objects (129+ px) to P5.\n",
    "\n",
    "2. **Grid cell assignment** -- within the chosen scale, the grid cell that contains the box center is designated as the positive sample.\n",
    "\n",
    "3. **Target encoding** -- at the assigned grid cell, we store:\n",
    "   - **Objectness** = 1.0 (binary indicator that this cell is responsible for an object)\n",
    "   - **Center offsets** (cx_offset, cy_offset) -- the fractional position of the box center within the grid cell, both in [0, 1]\n",
    "   - **Box dimensions** (w, h) -- normalized by the image size\n",
    "   - **Class label** -- one-hot encoded across the number of classes\n",
    "\n",
    "The resulting target tensor at each scale has shape `(grid_h, grid_w, 5 + num_classes)` where the first 5 channels are `[objectness, cx_offset, cy_offset, w, h]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db4292d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:16.583194Z",
     "iopub.status.busy": "2026-02-15T16:30:16.583040Z",
     "iopub.status.idle": "2026-02-15T16:30:16.588261Z",
     "shell.execute_reply": "2026-02-15T16:30:16.587396Z"
    },
    "papermill": {
     "duration": 0.009143,
     "end_time": "2026-02-15T16:30:16.588974",
     "exception": false,
     "start_time": "2026-02-15T16:30:16.579831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_targets(boxes: np.ndarray, labels: np.ndarray,\n",
    "                   img_size: int = 640, num_classes: int = 80,\n",
    "                   strides: List[int] = [8, 16, 32]) -> List[np.ndarray]:\n",
    "    \"\"\"Encode ground-truth boxes into multi-scale target tensors for anchor-free detection.\n",
    "\n",
    "    Args:\n",
    "        boxes: (N, 4) normalized [cx, cy, w, h] in [0, 1]\n",
    "        labels: (N,) class indices\n",
    "        strides: feature map strides\n",
    "\n",
    "    Returns:\n",
    "        targets: list of arrays, one per scale level\n",
    "            Each has shape (grid_h, grid_w, 5 + num_classes)\n",
    "            Channel layout: [obj, cx_offset, cy_offset, w, h, one_hot_classes...]\n",
    "    \"\"\"\n",
    "    targets = []\n",
    "    for stride in strides:\n",
    "        grid_size = img_size // stride\n",
    "        # obj(1) + box(4) + classes\n",
    "        target = np.zeros((grid_size, grid_size, 5 + num_classes), dtype=np.float32)\n",
    "        targets.append(target)\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        cx, cy, w, h = boxes[i]\n",
    "        cls = int(labels[i])\n",
    "\n",
    "        # Convert to pixel coords\n",
    "        cx_px = cx * img_size\n",
    "        cy_px = cy * img_size\n",
    "        w_px = w * img_size\n",
    "        h_px = h * img_size\n",
    "\n",
    "        # Assign to stride level based on box size\n",
    "        box_size = max(w_px, h_px)\n",
    "        if box_size <= 64:\n",
    "            level = 0   # P3, stride 8\n",
    "        elif box_size <= 128:\n",
    "            level = 1   # P4, stride 16\n",
    "        else:\n",
    "            level = 2   # P5, stride 32\n",
    "\n",
    "        stride = strides[level]\n",
    "        grid_size = img_size // stride\n",
    "\n",
    "        # Grid cell containing the box center\n",
    "        gx = int(cx_px / stride)\n",
    "        gy = int(cy_px / stride)\n",
    "        gx = min(gx, grid_size - 1)\n",
    "        gy = min(gy, grid_size - 1)\n",
    "\n",
    "        # Encode: objectness, center offset within cell, box size\n",
    "        targets[level][gy, gx, 0] = 1.0                # objectness\n",
    "        targets[level][gy, gx, 1] = cx_px / stride - gx  # cx offset [0,1]\n",
    "        targets[level][gy, gx, 2] = cy_px / stride - gy  # cy offset [0,1]\n",
    "        targets[level][gy, gx, 3] = w_px / img_size     # normalized width\n",
    "        targets[level][gy, gx, 4] = h_px / img_size     # normalized height\n",
    "        targets[level][gy, gx, 5 + cls] = 1.0           # one-hot class\n",
    "\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ac2da5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:16.595748Z",
     "iopub.status.busy": "2026-02-15T16:30:16.595530Z",
     "iopub.status.idle": "2026-02-15T16:30:16.604631Z",
     "shell.execute_reply": "2026-02-15T16:30:16.604153Z"
    },
    "papermill": {
     "duration": 0.013739,
     "end_time": "2026-02-15T16:30:16.605203",
     "exception": false,
     "start_time": "2026-02-15T16:30:16.591464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class YOLODataset(Dataset):\n",
    "    \"\"\"COCO-format dataset for YOLOv11 training with mosaic augmentation.\"\"\"\n",
    "\n",
    "    def __init__(self, annotation_file: str, image_dir: str, img_size: int = 640,\n",
    "                 num_classes: int = 80, augment: bool = True, mosaic_prob: float = 0.5):\n",
    "        self.parser = COCOParser(annotation_file, image_dir)\n",
    "        self.img_size = img_size\n",
    "        self.num_classes = num_classes\n",
    "        self.augment = augment\n",
    "        self.mosaic_prob = mosaic_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.parser.img_ids)\n",
    "\n",
    "    def load_raw(self, idx: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Load image and annotations without augmentation.\"\"\"\n",
    "        img_id = self.parser.img_ids[idx]\n",
    "        img = np.array(Image.open(self.parser.get_image_path(img_id)).convert('RGB'))\n",
    "        anns = self.parser.get_annotations(img_id)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        h, w = img.shape[:2]\n",
    "        for ann in anns:\n",
    "            x, y, bw, bh = ann['bbox']  # COCO format: top-left x, y, w, h\n",
    "            # Convert to center format and normalize\n",
    "            cx = (x + bw / 2) / w\n",
    "            cy = (y + bh / 2) / h\n",
    "            bw = bw / w\n",
    "            bh = bh / h\n",
    "            if bw > 0 and bh > 0:\n",
    "                boxes.append([cx, cy, bw, bh])\n",
    "                labels.append(self.parser.cat_id_to_continuous[ann['category_id']])\n",
    "\n",
    "        boxes = np.array(boxes, dtype=np.float32) if boxes else np.zeros((0, 4), dtype=np.float32)\n",
    "        labels = np.array(labels, dtype=np.int64) if labels else np.zeros((0,), dtype=np.int64)\n",
    "        return img, boxes, labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.augment and random.random() < self.mosaic_prob:\n",
    "            indices = [idx] + [random.randint(0, len(self) - 1) for _ in range(3)]\n",
    "            img, boxes, labels = mosaic_augmentation(self, indices, self.img_size)\n",
    "        else:\n",
    "            img, boxes, labels = self.load_raw(idx)\n",
    "            img, scale, pad = letterbox_resize(img, self.img_size)\n",
    "            if len(boxes) > 0:\n",
    "                # Convert normalized boxes to pixel, adjust for letterbox, re-normalize\n",
    "                orig_img = np.array(Image.open(\n",
    "                    self.parser.get_image_path(self.parser.img_ids[idx])).convert('RGB'))\n",
    "                oh, ow = orig_img.shape[:2]\n",
    "                pixel_boxes = boxes.copy()\n",
    "                pixel_boxes[:, 0] *= ow\n",
    "                pixel_boxes[:, 1] *= oh\n",
    "                pixel_boxes[:, 2] *= ow\n",
    "                pixel_boxes[:, 3] *= oh\n",
    "                boxes = adjust_boxes_for_letterbox(pixel_boxes, scale, pad)\n",
    "                boxes[:, [0, 2]] /= self.img_size\n",
    "                boxes[:, [1, 3]] /= self.img_size\n",
    "\n",
    "        # Encode multi-scale targets\n",
    "        targets = encode_targets(boxes, labels, self.img_size, self.num_classes)\n",
    "\n",
    "        # To tensor\n",
    "        img_tensor = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "        target_tensors = [torch.from_numpy(t) for t in targets]\n",
    "\n",
    "        return img_tensor, target_tensors, torch.from_numpy(boxes), torch.from_numpy(labels)\n",
    "\n",
    "\n",
    "def yolo_collate_fn(batch):\n",
    "    \"\"\"Custom collate: stack images, list targets (variable bbox count).\"\"\"\n",
    "    imgs = torch.stack([b[0] for b in batch])\n",
    "    targets_p3 = torch.stack([b[1][0] for b in batch])\n",
    "    targets_p4 = torch.stack([b[1][1] for b in batch])\n",
    "    targets_p5 = torch.stack([b[1][2] for b in batch])\n",
    "    boxes = [b[2] for b in batch]      # list of variable-length tensors\n",
    "    labels = [b[3] for b in batch]     # list of variable-length tensors\n",
    "    return imgs, [targets_p3, targets_p4, targets_p5], boxes, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11913c74",
   "metadata": {
    "papermill": {
     "duration": 0.002665,
     "end_time": "2026-02-15T16:30:16.610148",
     "exception": false,
     "start_time": "2026-02-15T16:30:16.607483",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Visualization utilities\n",
    "\n",
    "The following helper functions let us inspect the pipeline output visually. The first function draws bounding boxes on an image tensor, and the second displays the objectness maps at each feature pyramid level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4776d072",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:16.616185Z",
     "iopub.status.busy": "2026-02-15T16:30:16.616022Z",
     "iopub.status.idle": "2026-02-15T16:30:16.620006Z",
     "shell.execute_reply": "2026-02-15T16:30:16.619501Z"
    },
    "papermill": {
     "duration": 0.007765,
     "end_time": "2026-02-15T16:30:16.620743",
     "exception": false,
     "start_time": "2026-02-15T16:30:16.612978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_sample(img_tensor, boxes, labels, category_names=None, ax=None):\n",
    "    \"\"\"Visualize an image with bounding boxes.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "    img = img_tensor.permute(1, 2, 0).numpy()\n",
    "    ax.imshow(img)\n",
    "\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, 80))\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        cx, cy, w, h = boxes[i].numpy()\n",
    "        # Convert from normalized to pixel\n",
    "        cx *= IMG_SIZE; cy *= IMG_SIZE; w *= IMG_SIZE; h *= IMG_SIZE\n",
    "        x1 = cx - w / 2\n",
    "        y1 = cy - h / 2\n",
    "\n",
    "        cls = int(labels[i])\n",
    "        color = colors[cls % len(colors)]\n",
    "        rect = patches.Rectangle((x1, y1), w, h, linewidth=2,\n",
    "                                  edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        name = category_names.get(cls, str(cls)) if category_names else str(cls)\n",
    "        ax.text(x1, y1 - 5, name, color='white', fontsize=8,\n",
    "                bbox=dict(boxstyle='round,pad=0.2', facecolor=color, alpha=0.7))\n",
    "\n",
    "    ax.axis('off')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f57f7b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:16.626106Z",
     "iopub.status.busy": "2026-02-15T16:30:16.625992Z",
     "iopub.status.idle": "2026-02-15T16:30:16.629095Z",
     "shell.execute_reply": "2026-02-15T16:30:16.628619Z"
    },
    "papermill": {
     "duration": 0.006265,
     "end_time": "2026-02-15T16:30:16.629453",
     "exception": false,
     "start_time": "2026-02-15T16:30:16.623188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_targets(targets, strides=[8, 16, 32]):\n",
    "    \"\"\"Visualize objectness maps at each scale.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    for i, (target, stride) in enumerate(zip(targets, strides)):\n",
    "        obj_map = target[:, :, 0] if isinstance(target, np.ndarray) else target[..., 0].numpy()\n",
    "        axes[i].imshow(obj_map, cmap='hot', interpolation='nearest')\n",
    "        axes[i].set_title(f'P{i+3} (stride={stride}, grid={obj_map.shape[0]}x{obj_map.shape[1]})')\n",
    "        axes[i].set_xlabel(f'{int(obj_map.sum())} objects assigned')\n",
    "\n",
    "    plt.suptitle('Multi-Scale Target Assignment (Objectness Maps)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b54eb7b",
   "metadata": {
    "papermill": {
     "duration": 0.002031,
     "end_time": "2026-02-15T16:30:16.633707",
     "exception": false,
     "start_time": "2026-02-15T16:30:16.631676",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading real COCO data via Hugging Face streaming\n",
    "\n",
    "Instead of creating synthetic images with colored rectangles, we stream real COCO images directly from [detection-datasets/coco](https://huggingface.co/datasets/detection-datasets/coco) on the Hugging Face Hub. This requires no local download â€” images are fetched on-the-fly.\n",
    "\n",
    "> **Data source**: Images streamed from [detection-datasets/coco](https://huggingface.co/datasets/detection-datasets/coco). See our [HF COCO streaming tutorial](/blog/tutorials/hf-coco-streaming) for details.\n",
    "\n",
    "The streaming dataset wraps the HF iterable as a PyTorch `IterableDataset`, converting annotations from COCO format (`[x, y, w, h]` with top-left origin) to YOLO format (`[cx, cy, w, h]` normalized, 0-indexed labels). It applies the same letterbox resize and multi-scale target encoding as the disk-based `YOLODataset`.\n",
    "\n",
    "**Note**: Mosaic augmentation requires random access to the dataset, which is incompatible with `IterableDataset`. The streaming demo skips mosaic; mosaic augmentation is already demonstrated above with the disk-based `YOLODataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe606536",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:16.638584Z",
     "iopub.status.busy": "2026-02-15T16:30:16.638442Z",
     "iopub.status.idle": "2026-02-15T16:30:30.499043Z",
     "shell.execute_reply": "2026-02-15T16:30:30.498410Z"
    },
    "papermill": {
     "duration": 13.863724,
     "end_time": "2026-02-15T16:30:30.499505",
     "exception": false,
     "start_time": "2026-02-15T16:30:16.635781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# COCO class names (80 categories, 0-indexed as provided by the HF dataset)\n",
    "COCO_NAMES = [\n",
    "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck',\n",
    "    'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench',\n",
    "    'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra',\n",
    "    'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "    'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n",
    "    'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',\n",
    "    'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
    "    'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "    'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "\n",
    "def transform_to_yolo(example):\n",
    "    \"\"\"Convert a single HF COCO example to YOLO format.\n",
    "\n",
    "    The HF dataset provides bounding boxes in COCO format [x, y, w, h] (pixels,\n",
    "    top-left corner) with 0-indexed category labels. We convert to YOLO format\n",
    "    [cx, cy, w, h] (normalized) with the same 0-indexed labels.\n",
    "    \"\"\"\n",
    "    img = np.array(example['image'].convert('RGB'))\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    bboxes = example['objects']['bbox']\n",
    "    cats = example['objects']['category']\n",
    "\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for bbox, cat_id in zip(bboxes, cats):\n",
    "        bx, by, bw, bh = bbox\n",
    "        if bw <= 0 or bh <= 0:\n",
    "            continue\n",
    "        cx = (bx + bw / 2) / w\n",
    "        cy = (by + bh / 2) / h\n",
    "        boxes.append([cx, cy, bw / w, bh / h])\n",
    "        labels.append(int(cat_id))\n",
    "\n",
    "    return {\n",
    "        'image': img,\n",
    "        'boxes': np.array(boxes, dtype=np.float32) if boxes else np.zeros((0, 4), dtype=np.float32),\n",
    "        'labels': np.array(labels, dtype=np.int64) if labels else np.zeros((0,), dtype=np.int64),\n",
    "    }\n",
    "\n",
    "\n",
    "class COCOStreamYOLODataset(IterableDataset):\n",
    "    \"\"\"Stream COCO from Hugging Face and yield YOLO-format training samples.\n",
    "\n",
    "    Each sample goes through letterbox resize and multi-scale target encoding,\n",
    "    identical to the disk-based YOLODataset above. Mosaic augmentation is skipped\n",
    "    because it requires random access, which is incompatible with streaming.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split='train', max_samples=None, img_size=640, num_classes=80):\n",
    "        self.split = split\n",
    "        self.max_samples = max_samples\n",
    "        self.img_size = img_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __iter__(self):\n",
    "        ds = load_dataset('detection-datasets/coco', split=self.split, streaming=True)\n",
    "\n",
    "        count = 0\n",
    "        for example in ds:\n",
    "            if self.max_samples and count >= self.max_samples:\n",
    "                break\n",
    "\n",
    "            parsed = transform_to_yolo(example)\n",
    "            img = parsed['image']\n",
    "            boxes = parsed['boxes']\n",
    "            labels = parsed['labels']\n",
    "\n",
    "            if len(boxes) == 0:\n",
    "                continue\n",
    "\n",
    "            # Letterbox resize (same as disk-based pipeline)\n",
    "            orig_h, orig_w = img.shape[:2]\n",
    "            img, scale, pad = letterbox_resize(img, self.img_size)\n",
    "\n",
    "            # Adjust boxes for letterbox\n",
    "            pixel_boxes = boxes.copy()\n",
    "            pixel_boxes[:, 0] *= orig_w\n",
    "            pixel_boxes[:, 1] *= orig_h\n",
    "            pixel_boxes[:, 2] *= orig_w\n",
    "            pixel_boxes[:, 3] *= orig_h\n",
    "            boxes = adjust_boxes_for_letterbox(pixel_boxes, scale, pad)\n",
    "            boxes[:, [0, 2]] /= self.img_size\n",
    "            boxes[:, [1, 3]] /= self.img_size\n",
    "\n",
    "            # Encode multi-scale targets\n",
    "            targets = encode_targets(boxes, labels, self.img_size, self.num_classes)\n",
    "\n",
    "            img_tensor = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "            target_tensors = [torch.from_numpy(t) for t in targets]\n",
    "\n",
    "            yield img_tensor, target_tensors, torch.from_numpy(boxes), torch.from_numpy(labels)\n",
    "            count += 1\n",
    "\n",
    "\n",
    "# Stream 16 real COCO images for demonstration\n",
    "stream_dataset = COCOStreamYOLODataset(split='train', max_samples=16)\n",
    "stream_loader = DataLoader(stream_dataset, batch_size=4, collate_fn=yolo_collate_fn, num_workers=0)\n",
    "\n",
    "print(\"Streaming real COCO images from Hugging Face...\")\n",
    "batch = next(iter(stream_loader))\n",
    "imgs, targets, boxes_list, labels_list = batch\n",
    "\n",
    "print(f\"Image batch shape: {imgs.shape}\")\n",
    "for i, t in enumerate(targets):\n",
    "    print(f\"Target P{i+3} shape: {t.shape}\")\n",
    "print(f\"Objects per image: {[len(b) for b in boxes_list]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba4e258",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:30.505461Z",
     "iopub.status.busy": "2026-02-15T16:30:30.505238Z",
     "iopub.status.idle": "2026-02-15T16:30:31.397614Z",
     "shell.execute_reply": "2026-02-15T16:30:31.396813Z"
    },
    "papermill": {
     "duration": 0.912789,
     "end_time": "2026-02-15T16:30:31.414941",
     "exception": false,
     "start_time": "2026-02-15T16:30:30.502152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize real COCO images with ground-truth boxes\n",
    "cat_names = {i: name for i, name in enumerate(COCO_NAMES)}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 16))\n",
    "for i in range(min(4, len(imgs))):\n",
    "    ax = axes[i // 2][i % 2]\n",
    "    visualize_sample(imgs[i], boxes_list[i], labels_list[i], cat_names, ax=ax)\n",
    "    ax.set_title(f'COCO Sample {i} ({len(boxes_list[i])} objects)')\n",
    "\n",
    "plt.suptitle('Real COCO Images via HF Streaming', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77031399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:31.478611Z",
     "iopub.status.busy": "2026-02-15T16:30:31.478441Z",
     "iopub.status.idle": "2026-02-15T16:30:31.721865Z",
     "shell.execute_reply": "2026-02-15T16:30:31.721246Z"
    },
    "papermill": {
     "duration": 0.276134,
     "end_time": "2026-02-15T16:30:31.722460",
     "exception": false,
     "start_time": "2026-02-15T16:30:31.446326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize target grids for first sample\n",
    "sample_targets = [t[0] for t in targets]  # first sample in batch\n",
    "visualize_targets(sample_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2cfdf4",
   "metadata": {
    "papermill": {
     "duration": 0.030657,
     "end_time": "2026-02-15T16:30:31.784684",
     "exception": false,
     "start_time": "2026-02-15T16:30:31.754027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DataLoader performance considerations\n",
    "\n",
    "When training on real data with thousands of images, DataLoader configuration has a significant impact on GPU utilization:\n",
    "\n",
    "- **`num_workers`** -- set this to the number of CPU cores available (typically 4-8). Each worker runs in a separate process and pre-loads batches in parallel. Setting this too high can cause memory issues.\n",
    "- **`pin_memory=True`** -- enables pinned (page-locked) memory for faster CPU-to-GPU transfers. Always use this when training on a GPU.\n",
    "- **`persistent_workers=True`** -- keeps worker processes alive between epochs, avoiding the overhead of re-spawning them. Requires `num_workers > 0`.\n",
    "- **`drop_last=True`** -- drops the final incomplete batch, which prevents shape mismatches in batch normalization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90db1ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:31.847543Z",
     "iopub.status.busy": "2026-02-15T16:30:31.847383Z",
     "iopub.status.idle": "2026-02-15T16:30:31.851319Z",
     "shell.execute_reply": "2026-02-15T16:30:31.850665Z"
    },
    "papermill": {
     "duration": 0.036348,
     "end_time": "2026-02-15T16:30:31.851837",
     "exception": false,
     "start_time": "2026-02-15T16:30:31.815489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Performance configuration for real training\n",
    "def create_train_loader(annotation_file, image_dir, batch_size=16, num_workers=4):\n",
    "    \"\"\"Create an optimized DataLoader for training.\"\"\"\n",
    "    dataset = YOLODataset(\n",
    "        annotation_file, image_dir,\n",
    "        img_size=IMG_SIZE,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        augment=True,\n",
    "        mosaic_prob=0.5\n",
    "    )\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=yolo_collate_fn,\n",
    "        drop_last=True,\n",
    "        persistent_workers=True if num_workers > 0 else False\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Data pipeline complete!\")\n",
    "print(f\"Input: COCO-format annotations + images\")\n",
    "print(f\"Output: {IMG_SIZE}x{IMG_SIZE} images with multi-scale targets\")\n",
    "print(f\"  P3: {GRID_SIZES[0]}x{GRID_SIZES[0]} (stride {STRIDES[0]}) - small objects\")\n",
    "print(f\"  P4: {GRID_SIZES[1]}x{GRID_SIZES[1]} (stride {STRIDES[1]}) - medium objects\")\n",
    "print(f\"  P5: {GRID_SIZES[2]}x{GRID_SIZES[2]} (stride {STRIDES[2]}) - large objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z98gd16xzpe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T16:30:31.914683Z",
     "iopub.status.busy": "2026-02-15T16:30:31.914521Z",
     "iopub.status.idle": "2026-02-15T16:30:31.917799Z",
     "shell.execute_reply": "2026-02-15T16:30:31.917343Z"
    },
    "papermill": {
     "duration": 0.035595,
     "end_time": "2026-02-15T16:30:31.918339",
     "exception": false,
     "start_time": "2026-02-15T16:30:31.882744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Streaming alternative: no local files needed\n",
    "def create_stream_train_loader(split='train', max_samples=None, batch_size=16):\n",
    "    \"\"\"Create a DataLoader that streams COCO from Hugging Face.\n",
    "\n",
    "    Unlike create_train_loader above, this requires no local annotation file\n",
    "    or image directory. Images are fetched on-the-fly from the HF Hub.\n",
    "    Mosaic augmentation is not available in streaming mode.\n",
    "    \"\"\"\n",
    "    dataset = COCOStreamYOLODataset(\n",
    "        split=split,\n",
    "        max_samples=max_samples,\n",
    "        img_size=IMG_SIZE,\n",
    "        num_classes=NUM_CLASSES,\n",
    "    )\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=yolo_collate_fn,\n",
    "        num_workers=0,  # streaming is single-threaded\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Streaming train loader factory ready.\")\n",
    "print(\"Usage: loader = create_stream_train_loader(max_samples=100)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae2b12f",
   "metadata": {
    "papermill": {
     "duration": 0.030395,
     "end_time": "2026-02-15T16:30:31.980870",
     "exception": false,
     "start_time": "2026-02-15T16:30:31.950475",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we built a complete COCO data pipeline for anchor-free YOLOv11 training. The key components are:\n",
    "\n",
    "1. **COCOParser** -- reads COCO JSON annotations, maps non-contiguous category IDs to a contiguous range, and groups annotations by image.\n",
    "2. **Letterbox resize** -- scales images to 640 x 640 while preserving aspect ratio with symmetric gray padding.\n",
    "3. **Mosaic augmentation** -- combines four training images into a single composite to increase object diversity and context variation.\n",
    "4. **Multi-scale target encoding** -- assigns each ground-truth box to the appropriate feature pyramid level (P3/P4/P5) and encodes objectness, center offsets, box dimensions, and class labels into dense grid targets.\n",
    "5. **YOLODataset + DataLoader** -- wraps everything into a PyTorch `Dataset` with a custom collate function that handles variable numbers of objects per image.\n",
    "\n",
    "**Next up:** In Notebook 2 we will build the YOLOv11 backbone network that processes these 640 x 640 images and produces the P3, P4, and P5 feature maps that our detection heads will operate on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18.762928,
   "end_time": "2026-02-15T16:30:32.730880",
   "environment_variables": {},
   "exception": null,
   "input_path": "notebooks/scene-understanding/object-detection/yolo/pytorch/01_coco_dataloader.ipynb",
   "output_path": "notebooks/scene-understanding/object-detection/yolo/pytorch/01_coco_dataloader-executed.ipynb",
   "parameters": {},
   "start_time": "2026-02-15T16:30:13.967952",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
