{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO Data Pipeline for Anchor-Free Detection\n",
    "\n",
    "*Notebook 1 of 5 in the YOLOv11 from-scratch series*\n",
    "\n",
    "---\n",
    "\n",
    "Modern YOLO detectors require a specialized data pipeline that goes well beyond simple image loading. The pipeline must handle several responsibilities:\n",
    "\n",
    "- **Parsing** COCO-format annotations and mapping non-contiguous category IDs to a contiguous range\n",
    "- **Resizing** images via letterboxing to preserve aspect ratio while fitting a fixed input resolution\n",
    "- **Augmenting** training data with techniques like mosaic augmentation to increase object diversity per sample\n",
    "- **Encoding** ground-truth bounding boxes into multi-scale target tensors suitable for anchor-free detection heads\n",
    "\n",
    "In this notebook we build a complete COCO data pipeline for YOLOv11 training. The pipeline produces:\n",
    "\n",
    "| Output | Grid Size | Stride | Object Scale |\n",
    "|--------|-----------|--------|--------------|\n",
    "| P3     | 80 x 80   | 8      | Small        |\n",
    "| P4     | 40 x 40   | 16     | Medium       |\n",
    "| P5     | 20 x 20   | 32     | Large        |\n",
    "\n",
    "All outputs assume a 640 x 640 input resolution. By the end of this notebook you will have a `DataLoader` that yields image tensors paired with multi-scale target grids ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Configuration\n",
    "IMG_SIZE = 640\n",
    "NUM_CLASSES = 80\n",
    "STRIDES = [8, 16, 32]  # P3, P4, P5\n",
    "GRID_SIZES = [IMG_SIZE // s for s in STRIDES]  # 80, 40, 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCO annotation format\n",
    "\n",
    "The COCO (Common Objects in Context) dataset uses a JSON annotation format with three top-level keys:\n",
    "\n",
    "- **`images`** -- a list of image metadata entries, each containing an `id`, `file_name`, `width`, and `height`.\n",
    "- **`annotations`** -- a list of object annotations. Each annotation links to an image via `image_id` and contains a `bbox` in **top-left `[x, y, width, height]`** format, a `category_id`, and an `iscrowd` flag.\n",
    "- **`categories`** -- a list of category definitions mapping `id` to `name`.\n",
    "\n",
    "One important detail: COCO category IDs are **not contiguous**. For example, category IDs might jump from 1 to 16. We need to build a mapping from the original IDs to a contiguous `0..N-1` range for use in classification targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOParser:\n",
    "    \"\"\"Parse COCO-format annotations.\"\"\"\n",
    "\n",
    "    def __init__(self, annotation_file: str, image_dir: str):\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            coco = json.load(f)\n",
    "\n",
    "        self.image_dir = image_dir\n",
    "        self.images = {img['id']: img for img in coco['images']}\n",
    "\n",
    "        # Build category mapping (COCO IDs are not contiguous)\n",
    "        cat_ids = sorted([c['id'] for c in coco['categories']])\n",
    "        self.cat_id_to_continuous = {cid: i for i, cid in enumerate(cat_ids)}\n",
    "        self.categories = {c['id']: c['name'] for c in coco['categories']}\n",
    "\n",
    "        # Group annotations by image\n",
    "        self.img_annotations = {}\n",
    "        for ann in coco['annotations']:\n",
    "            if ann.get('iscrowd', 0):\n",
    "                continue\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.img_annotations:\n",
    "                self.img_annotations[img_id] = []\n",
    "            self.img_annotations[img_id].append(ann)\n",
    "\n",
    "        # Only keep images that have annotations\n",
    "        self.img_ids = [iid for iid in self.images if iid in self.img_annotations]\n",
    "        print(f\"Loaded {len(self.img_ids)} images with \"\n",
    "              f\"{sum(len(v) for v in self.img_annotations.values())} annotations\")\n",
    "\n",
    "    def get_image_path(self, img_id: int) -> str:\n",
    "        return os.path.join(self.image_dir, self.images[img_id]['file_name'])\n",
    "\n",
    "    def get_annotations(self, img_id: int) -> List[Dict]:\n",
    "        return self.img_annotations.get(img_id, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letterbox resizing\n",
    "\n",
    "YOLO models expect a fixed square input (640 x 640). Naively resizing images to this shape would distort their aspect ratio, which can hurt detection accuracy -- especially for objects with extreme aspect ratios.\n",
    "\n",
    "**Letterboxing** solves this by:\n",
    "\n",
    "1. Scaling the image so its longest side matches the target size.\n",
    "2. Padding the shorter side symmetrically with a neutral gray value (114) to form a square.\n",
    "\n",
    "This preserves the original aspect ratio while fitting the model's input dimensions. The bounding box coordinates must be adjusted to account for both the scale factor and the padding offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox_resize(image: np.ndarray, target_size: int = 640\n",
    "                     ) -> Tuple[np.ndarray, float, Tuple[int, int]]:\n",
    "    \"\"\"Resize image with letterboxing (preserve aspect ratio, pad to square).\n",
    "\n",
    "    Returns:\n",
    "        resized_image: (target_size, target_size, 3) uint8 array\n",
    "        scale: resize scale factor\n",
    "        pad: (pad_w, pad_h) padding applied\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    scale = target_size / max(h, w)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "\n",
    "    resized = np.array(Image.fromarray(image).resize((new_w, new_h), Image.BILINEAR))\n",
    "\n",
    "    # Create padded image (gray padding = 114)\n",
    "    padded = np.full((target_size, target_size, 3), 114, dtype=np.uint8)\n",
    "    pad_w = (target_size - new_w) // 2\n",
    "    pad_h = (target_size - new_h) // 2\n",
    "    padded[pad_h:pad_h + new_h, pad_w:pad_w + new_w] = resized\n",
    "\n",
    "    return padded, scale, (pad_w, pad_h)\n",
    "\n",
    "\n",
    "def adjust_boxes_for_letterbox(boxes: np.ndarray, scale: float,\n",
    "                                pad: Tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"Adjust bounding boxes after letterbox resize.\n",
    "\n",
    "    Args:\n",
    "        boxes: (N, 4) in [x_center, y_center, w, h] format (original pixel coords)\n",
    "        scale: letterbox scale factor\n",
    "        pad: (pad_w, pad_h)\n",
    "    Returns:\n",
    "        adjusted: (N, 4) in [x_center, y_center, w, h] in letterboxed image coords\n",
    "    \"\"\"\n",
    "    adjusted = boxes.copy().astype(np.float32)\n",
    "    adjusted[:, 0] = boxes[:, 0] * scale + pad[0]  # x_center\n",
    "    adjusted[:, 1] = boxes[:, 1] * scale + pad[1]  # y_center\n",
    "    adjusted[:, 2] = boxes[:, 2] * scale            # width\n",
    "    adjusted[:, 3] = boxes[:, 3] * scale            # height\n",
    "    return adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mosaic augmentation\n",
    "\n",
    "Mosaic augmentation was introduced in YOLOv4 and remains a staple in modern YOLO training. The idea is simple but powerful: combine four randomly selected training images into a single composite image by placing each in one quadrant.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- **More objects per sample** -- the model sees objects from four images in a single forward pass, which improves gradient quality.\n",
    "- **Context diversity** -- objects appear against varied backgrounds and alongside different neighbors.\n",
    "- **Reduced batch size dependence** -- because each sample is richer, you can train effectively with smaller batches.\n",
    "- **Scale variation** -- objects end up at a wider range of scales than they would in isolated images.\n",
    "\n",
    "The mosaic center is randomized to prevent the model from learning a fixed spatial prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mosaic_augmentation(dataset, indices: List[int],\n",
    "                        img_size: int = 640\n",
    "                        ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Create a mosaic from 4 images.\n",
    "\n",
    "    Returns:\n",
    "        mosaic_img: (img_size, img_size, 3)\n",
    "        mosaic_boxes: (N, 4) [x_center, y_center, w, h] normalized to [0,1]\n",
    "        mosaic_labels: (N,) class indices\n",
    "    \"\"\"\n",
    "    cx, cy = img_size // 2, img_size // 2  # mosaic center\n",
    "    # Add random offset for variety\n",
    "    cx += random.randint(-img_size // 4, img_size // 4)\n",
    "    cy += random.randint(-img_size // 4, img_size // 4)\n",
    "\n",
    "    mosaic_img = np.full((img_size, img_size, 3), 114, dtype=np.uint8)\n",
    "    all_boxes = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        img, boxes, labels = dataset.load_raw(idx)\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        # Determine placement in mosaic quadrant\n",
    "        if i == 0:    # top-left\n",
    "            x1, y1, x2, y2 = max(cx - w, 0), max(cy - h, 0), cx, cy\n",
    "            crop_x1, crop_y1 = w - (x2 - x1), h - (y2 - y1)\n",
    "            crop_x2, crop_y2 = w, h\n",
    "        elif i == 1:  # top-right\n",
    "            x1, y1, x2, y2 = cx, max(cy - h, 0), min(cx + w, img_size), cy\n",
    "            crop_x1, crop_y1 = 0, h - (y2 - y1)\n",
    "            crop_x2, crop_y2 = x2 - x1, h\n",
    "        elif i == 2:  # bottom-left\n",
    "            x1, y1, x2, y2 = max(cx - w, 0), cy, cx, min(cy + h, img_size)\n",
    "            crop_x1, crop_y1 = w - (x2 - x1), 0\n",
    "            crop_x2, crop_y2 = w, y2 - y1\n",
    "        else:         # bottom-right\n",
    "            x1, y1, x2, y2 = cx, cy, min(cx + w, img_size), min(cy + h, img_size)\n",
    "            crop_x1, crop_y1 = 0, 0\n",
    "            crop_x2, crop_y2 = x2 - x1, y2 - y1\n",
    "\n",
    "        mosaic_img[y1:y2, x1:x2] = img[crop_y1:crop_y2, crop_x1:crop_x2]\n",
    "\n",
    "        # Adjust boxes: convert from normalized [0,1] to pixel coords in mosaic\n",
    "        if len(boxes) > 0:\n",
    "            pixel_boxes = boxes.copy()\n",
    "            pixel_boxes[:, 0] = boxes[:, 0] * w - crop_x1 + x1  # x_center\n",
    "            pixel_boxes[:, 1] = boxes[:, 1] * h - crop_y1 + y1  # y_center\n",
    "            pixel_boxes[:, 2] = boxes[:, 2] * w                   # width\n",
    "            pixel_boxes[:, 3] = boxes[:, 3] * h                   # height\n",
    "            all_boxes.append(pixel_boxes)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    if all_boxes:\n",
    "        all_boxes = np.concatenate(all_boxes, axis=0)\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "        # Clip to mosaic bounds and filter invalid\n",
    "        x1 = all_boxes[:, 0] - all_boxes[:, 2] / 2\n",
    "        y1 = all_boxes[:, 1] - all_boxes[:, 3] / 2\n",
    "        x2 = all_boxes[:, 0] + all_boxes[:, 2] / 2\n",
    "        y2 = all_boxes[:, 1] + all_boxes[:, 3] / 2\n",
    "        x1 = np.clip(x1, 0, img_size)\n",
    "        y1 = np.clip(y1, 0, img_size)\n",
    "        x2 = np.clip(x2, 0, img_size)\n",
    "        y2 = np.clip(y2, 0, img_size)\n",
    "\n",
    "        all_boxes[:, 2] = x2 - x1\n",
    "        all_boxes[:, 3] = y2 - y1\n",
    "        all_boxes[:, 0] = (x1 + x2) / 2\n",
    "        all_boxes[:, 1] = (y1 + y2) / 2\n",
    "\n",
    "        # Filter out tiny boxes\n",
    "        valid = (all_boxes[:, 2] > 2) & (all_boxes[:, 3] > 2)\n",
    "        all_boxes = all_boxes[valid]\n",
    "        all_labels = all_labels[valid]\n",
    "\n",
    "        # Normalize to [0, 1]\n",
    "        all_boxes[:, [0, 2]] /= img_size\n",
    "        all_boxes[:, [1, 3]] /= img_size\n",
    "    else:\n",
    "        all_boxes = np.zeros((0, 4), dtype=np.float32)\n",
    "        all_labels = np.zeros((0,), dtype=np.int64)\n",
    "\n",
    "    return mosaic_img, all_boxes, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-scale target encoding\n",
    "\n",
    "YOLOv11 uses an **anchor-free** detection paradigm. Instead of pre-defined anchor boxes, each grid cell directly predicts whether it contains an object center and, if so, the bounding box parameters.\n",
    "\n",
    "The target encoding works as follows:\n",
    "\n",
    "1. **Scale assignment** -- each ground-truth box is assigned to the feature pyramid level (P3, P4, or P5) whose receptive field best matches the box size. Small objects (up to 64 px) go to P3, medium objects (65-128 px) to P4, and large objects (129+ px) to P5.\n",
    "\n",
    "2. **Grid cell assignment** -- within the chosen scale, the grid cell that contains the box center is designated as the positive sample.\n",
    "\n",
    "3. **Target encoding** -- at the assigned grid cell, we store:\n",
    "   - **Objectness** = 1.0 (binary indicator that this cell is responsible for an object)\n",
    "   - **Center offsets** (cx_offset, cy_offset) -- the fractional position of the box center within the grid cell, both in [0, 1]\n",
    "   - **Box dimensions** (w, h) -- normalized by the image size\n",
    "   - **Class label** -- one-hot encoded across the number of classes\n",
    "\n",
    "The resulting target tensor at each scale has shape `(grid_h, grid_w, 5 + num_classes)` where the first 5 channels are `[objectness, cx_offset, cy_offset, w, h]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_targets(boxes: np.ndarray, labels: np.ndarray,\n",
    "                   img_size: int = 640, num_classes: int = 80,\n",
    "                   strides: List[int] = [8, 16, 32]) -> List[np.ndarray]:\n",
    "    \"\"\"Encode ground-truth boxes into multi-scale target tensors for anchor-free detection.\n",
    "\n",
    "    Args:\n",
    "        boxes: (N, 4) normalized [cx, cy, w, h] in [0, 1]\n",
    "        labels: (N,) class indices\n",
    "        strides: feature map strides\n",
    "\n",
    "    Returns:\n",
    "        targets: list of arrays, one per scale level\n",
    "            Each has shape (grid_h, grid_w, 5 + num_classes)\n",
    "            Channel layout: [obj, cx_offset, cy_offset, w, h, one_hot_classes...]\n",
    "    \"\"\"\n",
    "    targets = []\n",
    "    for stride in strides:\n",
    "        grid_size = img_size // stride\n",
    "        # obj(1) + box(4) + classes\n",
    "        target = np.zeros((grid_size, grid_size, 5 + num_classes), dtype=np.float32)\n",
    "        targets.append(target)\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        cx, cy, w, h = boxes[i]\n",
    "        cls = int(labels[i])\n",
    "\n",
    "        # Convert to pixel coords\n",
    "        cx_px = cx * img_size\n",
    "        cy_px = cy * img_size\n",
    "        w_px = w * img_size\n",
    "        h_px = h * img_size\n",
    "\n",
    "        # Assign to stride level based on box size\n",
    "        box_size = max(w_px, h_px)\n",
    "        if box_size <= 64:\n",
    "            level = 0   # P3, stride 8\n",
    "        elif box_size <= 128:\n",
    "            level = 1   # P4, stride 16\n",
    "        else:\n",
    "            level = 2   # P5, stride 32\n",
    "\n",
    "        stride = strides[level]\n",
    "        grid_size = img_size // stride\n",
    "\n",
    "        # Grid cell containing the box center\n",
    "        gx = int(cx_px / stride)\n",
    "        gy = int(cy_px / stride)\n",
    "        gx = min(gx, grid_size - 1)\n",
    "        gy = min(gy, grid_size - 1)\n",
    "\n",
    "        # Encode: objectness, center offset within cell, box size\n",
    "        targets[level][gy, gx, 0] = 1.0                # objectness\n",
    "        targets[level][gy, gx, 1] = cx_px / stride - gx  # cx offset [0,1]\n",
    "        targets[level][gy, gx, 2] = cy_px / stride - gy  # cy offset [0,1]\n",
    "        targets[level][gy, gx, 3] = w_px / img_size     # normalized width\n",
    "        targets[level][gy, gx, 4] = h_px / img_size     # normalized height\n",
    "        targets[level][gy, gx, 5 + cls] = 1.0           # one-hot class\n",
    "\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLODataset(Dataset):\n",
    "    \"\"\"COCO-format dataset for YOLOv11 training with mosaic augmentation.\"\"\"\n",
    "\n",
    "    def __init__(self, annotation_file: str, image_dir: str, img_size: int = 640,\n",
    "                 num_classes: int = 80, augment: bool = True, mosaic_prob: float = 0.5):\n",
    "        self.parser = COCOParser(annotation_file, image_dir)\n",
    "        self.img_size = img_size\n",
    "        self.num_classes = num_classes\n",
    "        self.augment = augment\n",
    "        self.mosaic_prob = mosaic_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.parser.img_ids)\n",
    "\n",
    "    def load_raw(self, idx: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Load image and annotations without augmentation.\"\"\"\n",
    "        img_id = self.parser.img_ids[idx]\n",
    "        img = np.array(Image.open(self.parser.get_image_path(img_id)).convert('RGB'))\n",
    "        anns = self.parser.get_annotations(img_id)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        h, w = img.shape[:2]\n",
    "        for ann in anns:\n",
    "            x, y, bw, bh = ann['bbox']  # COCO format: top-left x, y, w, h\n",
    "            # Convert to center format and normalize\n",
    "            cx = (x + bw / 2) / w\n",
    "            cy = (y + bh / 2) / h\n",
    "            bw = bw / w\n",
    "            bh = bh / h\n",
    "            if bw > 0 and bh > 0:\n",
    "                boxes.append([cx, cy, bw, bh])\n",
    "                labels.append(self.parser.cat_id_to_continuous[ann['category_id']])\n",
    "\n",
    "        boxes = np.array(boxes, dtype=np.float32) if boxes else np.zeros((0, 4), dtype=np.float32)\n",
    "        labels = np.array(labels, dtype=np.int64) if labels else np.zeros((0,), dtype=np.int64)\n",
    "        return img, boxes, labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.augment and random.random() < self.mosaic_prob:\n",
    "            indices = [idx] + [random.randint(0, len(self) - 1) for _ in range(3)]\n",
    "            img, boxes, labels = mosaic_augmentation(self, indices, self.img_size)\n",
    "        else:\n",
    "            img, boxes, labels = self.load_raw(idx)\n",
    "            img, scale, pad = letterbox_resize(img, self.img_size)\n",
    "            if len(boxes) > 0:\n",
    "                # Convert normalized boxes to pixel, adjust for letterbox, re-normalize\n",
    "                orig_img = np.array(Image.open(\n",
    "                    self.parser.get_image_path(self.parser.img_ids[idx])).convert('RGB'))\n",
    "                oh, ow = orig_img.shape[:2]\n",
    "                pixel_boxes = boxes.copy()\n",
    "                pixel_boxes[:, 0] *= ow\n",
    "                pixel_boxes[:, 1] *= oh\n",
    "                pixel_boxes[:, 2] *= ow\n",
    "                pixel_boxes[:, 3] *= oh\n",
    "                boxes = adjust_boxes_for_letterbox(pixel_boxes, scale, pad)\n",
    "                boxes[:, [0, 2]] /= self.img_size\n",
    "                boxes[:, [1, 3]] /= self.img_size\n",
    "\n",
    "        # Encode multi-scale targets\n",
    "        targets = encode_targets(boxes, labels, self.img_size, self.num_classes)\n",
    "\n",
    "        # To tensor\n",
    "        img_tensor = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "        target_tensors = [torch.from_numpy(t) for t in targets]\n",
    "\n",
    "        return img_tensor, target_tensors, torch.from_numpy(boxes), torch.from_numpy(labels)\n",
    "\n",
    "\n",
    "def yolo_collate_fn(batch):\n",
    "    \"\"\"Custom collate: stack images, list targets (variable bbox count).\"\"\"\n",
    "    imgs = torch.stack([b[0] for b in batch])\n",
    "    targets_p3 = torch.stack([b[1][0] for b in batch])\n",
    "    targets_p4 = torch.stack([b[1][1] for b in batch])\n",
    "    targets_p5 = torch.stack([b[1][2] for b in batch])\n",
    "    boxes = [b[2] for b in batch]      # list of variable-length tensors\n",
    "    labels = [b[3] for b in batch]     # list of variable-length tensors\n",
    "    return imgs, [targets_p3, targets_p4, targets_p5], boxes, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization utilities\n",
    "\n",
    "The following helper functions let us inspect the pipeline output visually. The first function draws bounding boxes on an image tensor, and the second displays the objectness maps at each feature pyramid level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample(img_tensor, boxes, labels, category_names=None, ax=None):\n",
    "    \"\"\"Visualize an image with bounding boxes.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "    img = img_tensor.permute(1, 2, 0).numpy()\n",
    "    ax.imshow(img)\n",
    "\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, 80))\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        cx, cy, w, h = boxes[i].numpy()\n",
    "        # Convert from normalized to pixel\n",
    "        cx *= IMG_SIZE; cy *= IMG_SIZE; w *= IMG_SIZE; h *= IMG_SIZE\n",
    "        x1 = cx - w / 2\n",
    "        y1 = cy - h / 2\n",
    "\n",
    "        cls = int(labels[i])\n",
    "        color = colors[cls % len(colors)]\n",
    "        rect = patches.Rectangle((x1, y1), w, h, linewidth=2,\n",
    "                                  edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        name = category_names.get(cls, str(cls)) if category_names else str(cls)\n",
    "        ax.text(x1, y1 - 5, name, color='white', fontsize=8,\n",
    "                bbox=dict(boxstyle='round,pad=0.2', facecolor=color, alpha=0.7))\n",
    "\n",
    "    ax.axis('off')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_targets(targets, strides=[8, 16, 32]):\n",
    "    \"\"\"Visualize objectness maps at each scale.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    for i, (target, stride) in enumerate(zip(targets, strides)):\n",
    "        obj_map = target[:, :, 0] if isinstance(target, np.ndarray) else target[..., 0].numpy()\n",
    "        axes[i].imshow(obj_map, cmap='hot', interpolation='nearest')\n",
    "        axes[i].set_title(f'P{i+3} (stride={stride}, grid={obj_map.shape[0]}x{obj_map.shape[1]})')\n",
    "        axes[i].set_xlabel(f'{int(obj_map.sum())} objects assigned')\n",
    "\n",
    "    plt.suptitle('Multi-Scale Target Assignment (Objectness Maps)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo with synthetic data\n",
    "\n",
    "Since we cannot assume the full COCO dataset is available in this environment, we create a small synthetic dataset in COCO format. This lets us exercise and visualize every stage of the pipeline -- parsing, letterboxing, mosaic augmentation, and target encoding -- without needing to download large files.\n",
    "\n",
    "The synthetic images contain randomly colored rectangles that serve as stand-in objects with valid bounding box annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic COCO-format data for demonstration\n",
    "def create_synthetic_coco(num_images=16, img_size=640, max_objects=5):\n",
    "    \"\"\"Generate synthetic COCO annotations for pipeline testing.\"\"\"\n",
    "    import tempfile\n",
    "\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    img_dir = os.path.join(tmp_dir, 'images')\n",
    "    os.makedirs(img_dir)\n",
    "\n",
    "    images = []\n",
    "    annotations = []\n",
    "    ann_id = 1\n",
    "\n",
    "    for img_id in range(1, num_images + 1):\n",
    "        # Create random image\n",
    "        img = np.random.randint(50, 200, (img_size, img_size, 3), dtype=np.uint8)\n",
    "        # Draw some colored rectangles as \"objects\"\n",
    "        n_obj = random.randint(1, max_objects)\n",
    "        for _ in range(n_obj):\n",
    "            x1 = random.randint(0, img_size - 50)\n",
    "            y1 = random.randint(0, img_size - 50)\n",
    "            w = random.randint(30, min(200, img_size - x1))\n",
    "            h = random.randint(30, min(200, img_size - y1))\n",
    "            color = [random.randint(0, 255) for _ in range(3)]\n",
    "            img[y1:y1+h, x1:x1+w] = color\n",
    "\n",
    "            annotations.append({\n",
    "                'id': ann_id,\n",
    "                'image_id': img_id,\n",
    "                'category_id': random.choice([1, 2, 3, 16, 17, 18]),\n",
    "                'bbox': [float(x1), float(y1), float(w), float(h)],\n",
    "                'area': float(w * h),\n",
    "                'iscrowd': 0\n",
    "            })\n",
    "            ann_id += 1\n",
    "\n",
    "        fname = f'{img_id:06d}.jpg'\n",
    "        Image.fromarray(img).save(os.path.join(img_dir, fname))\n",
    "        images.append({'id': img_id, 'file_name': fname,\n",
    "                       'width': img_size, 'height': img_size})\n",
    "\n",
    "    categories = [\n",
    "        {'id': 1, 'name': 'person'}, {'id': 2, 'name': 'bicycle'},\n",
    "        {'id': 3, 'name': 'car'}, {'id': 16, 'name': 'bird'},\n",
    "        {'id': 17, 'name': 'cat'}, {'id': 18, 'name': 'dog'}\n",
    "    ]\n",
    "\n",
    "    ann_file = os.path.join(tmp_dir, 'annotations.json')\n",
    "    with open(ann_file, 'w') as f:\n",
    "        json.dump({'images': images, 'annotations': annotations,\n",
    "                   'categories': categories}, f)\n",
    "\n",
    "    return ann_file, img_dir, categories\n",
    "\n",
    "\n",
    "# Generate and test\n",
    "ann_file, img_dir, categories = create_synthetic_coco()\n",
    "cat_names = {i: c['name']\n",
    "             for i, c in enumerate(sorted(categories, key=lambda c: c['id']))}\n",
    "\n",
    "dataset = YOLODataset(ann_file, img_dir, num_classes=6,\n",
    "                      augment=True, mosaic_prob=0.5)\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True,\n",
    "                    collate_fn=yolo_collate_fn, num_workers=0)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Number of batches: {len(loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch one batch and visualize\n",
    "batch = next(iter(loader))\n",
    "imgs, targets, boxes_list, labels_list = batch\n",
    "\n",
    "print(f\"Image batch shape: {imgs.shape}\")\n",
    "for i, t in enumerate(targets):\n",
    "    print(f\"Target P{i+3} shape: {t.shape}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 16))\n",
    "for i in range(min(4, len(imgs))):\n",
    "    ax = axes[i // 2][i % 2]\n",
    "    visualize_sample(imgs[i], boxes_list[i], labels_list[i], cat_names, ax=ax)\n",
    "    ax.set_title(f'Sample {i} ({len(boxes_list[i])} objects)')\n",
    "\n",
    "plt.suptitle('Data Pipeline Output', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target grids for first sample\n",
    "sample_targets = [t[0] for t in targets]  # first sample in batch\n",
    "visualize_targets(sample_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader performance considerations\n",
    "\n",
    "When training on real data with thousands of images, DataLoader configuration has a significant impact on GPU utilization:\n",
    "\n",
    "- **`num_workers`** -- set this to the number of CPU cores available (typically 4-8). Each worker runs in a separate process and pre-loads batches in parallel. Setting this too high can cause memory issues.\n",
    "- **`pin_memory=True`** -- enables pinned (page-locked) memory for faster CPU-to-GPU transfers. Always use this when training on a GPU.\n",
    "- **`persistent_workers=True`** -- keeps worker processes alive between epochs, avoiding the overhead of re-spawning them. Requires `num_workers > 0`.\n",
    "- **`drop_last=True`** -- drops the final incomplete batch, which prevents shape mismatches in batch normalization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance configuration for real training\n",
    "def create_train_loader(annotation_file, image_dir, batch_size=16, num_workers=4):\n",
    "    \"\"\"Create an optimized DataLoader for training.\"\"\"\n",
    "    dataset = YOLODataset(\n",
    "        annotation_file, image_dir,\n",
    "        img_size=IMG_SIZE,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        augment=True,\n",
    "        mosaic_prob=0.5\n",
    "    )\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=yolo_collate_fn,\n",
    "        drop_last=True,\n",
    "        persistent_workers=True if num_workers > 0 else False\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Data pipeline complete!\")\n",
    "print(f\"Input: COCO-format annotations + images\")\n",
    "print(f\"Output: {IMG_SIZE}x{IMG_SIZE} images with multi-scale targets\")\n",
    "print(f\"  P3: {GRID_SIZES[0]}x{GRID_SIZES[0]} (stride {STRIDES[0]}) - small objects\")\n",
    "print(f\"  P4: {GRID_SIZES[1]}x{GRID_SIZES[1]} (stride {STRIDES[1]}) - medium objects\")\n",
    "print(f\"  P5: {GRID_SIZES[2]}x{GRID_SIZES[2]} (stride {STRIDES[2]}) - large objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we built a complete COCO data pipeline for anchor-free YOLOv11 training. The key components are:\n",
    "\n",
    "1. **COCOParser** -- reads COCO JSON annotations, maps non-contiguous category IDs to a contiguous range, and groups annotations by image.\n",
    "2. **Letterbox resize** -- scales images to 640 x 640 while preserving aspect ratio with symmetric gray padding.\n",
    "3. **Mosaic augmentation** -- combines four training images into a single composite to increase object diversity and context variation.\n",
    "4. **Multi-scale target encoding** -- assigns each ground-truth box to the appropriate feature pyramid level (P3/P4/P5) and encodes objectness, center offsets, box dimensions, and class labels into dense grid targets.\n",
    "5. **YOLODataset + DataLoader** -- wraps everything into a PyTorch `Dataset` with a custom collate function that handles variable numbers of objects per image.\n",
    "\n",
    "**Next up:** In Notebook 2 we will build the YOLOv11 backbone network that processes these 640 x 640 images and produces the P3, P4, and P5 feature maps that our detection heads will operate on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
