{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ca76b54",
   "metadata": {
    "papermill": {
     "duration": 0.004965,
     "end_time": "2026-02-12T21:28:09.531863",
     "exception": false,
     "start_time": "2026-02-12T21:28:09.526898",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "title: MaskRCNN Inference\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Colab Environment Setup ---\n",
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    %pip install -q matplotlib seaborn scikit-learn scipy tqdm\n",
    "    print(\"Colab dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb73330e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:28:09.539775Z",
     "iopub.status.busy": "2026-02-12T21:28:09.539538Z",
     "iopub.status.idle": "2026-02-12T21:28:11.723409Z",
     "shell.execute_reply": "2026-02-12T21:28:11.722560Z"
    },
    "papermill": {
     "duration": 2.189166,
     "end_time": "2026-02-12T21:28:11.724626",
     "exception": false,
     "start_time": "2026-02-12T21:28:09.535460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms, models, ops, io\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import MaskRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from IPython.display import display, Image as IPImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9858106",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:28:11.728523Z",
     "iopub.status.busy": "2026-02-12T21:28:11.728284Z",
     "iopub.status.idle": "2026-02-12T21:28:16.952073Z",
     "shell.execute_reply": "2026-02-12T21:28:16.951466Z"
    },
    "papermill": {
     "duration": 5.226674,
     "end_time": "2026-02-12T21:28:16.952804",
     "exception": false,
     "start_time": "2026-02-12T21:28:11.726130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = models.detection.maskrcnn_resnet50_fpn_v2(pretrained=True)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4a70ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:28:16.959989Z",
     "iopub.status.busy": "2026-02-12T21:28:16.959827Z",
     "iopub.status.idle": "2026-02-12T21:28:16.963523Z",
     "shell.execute_reply": "2026-02-12T21:28:16.963072Z"
    },
    "papermill": {
     "duration": 0.008232,
     "end_time": "2026-02-12T21:28:16.964057",
     "exception": false,
     "start_time": "2026-02-12T21:28:16.955825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    \"__background__\",\n",
    "    \"person\",\n",
    "    \"bicycle\",\n",
    "    \"car\",\n",
    "    \"motorcycle\",\n",
    "    \"airplane\",\n",
    "    \"bus\",\n",
    "    \"train\",\n",
    "    \"truck\",\n",
    "    \"boat\",\n",
    "    \"traffic light\",\n",
    "    \"fire hydrant\",\n",
    "    \"N/A\",\n",
    "    \"stop sign\",\n",
    "    \"parking meter\",\n",
    "    \"bench\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"sheep\",\n",
    "    \"cow\",\n",
    "    \"elephant\",\n",
    "    \"bear\",\n",
    "    \"zebra\",\n",
    "    \"giraffe\",\n",
    "    \"N/A\",\n",
    "    \"backpack\",\n",
    "    \"umbrella\",\n",
    "    \"N/A\",\n",
    "    \"N/A\",\n",
    "    \"handbag\",\n",
    "    \"tie\",\n",
    "    \"suitcase\",\n",
    "    \"frisbee\",\n",
    "    \"skis\",\n",
    "    \"snowboard\",\n",
    "    \"sports ball\",\n",
    "    \"kite\",\n",
    "    \"baseball bat\",\n",
    "    \"baseball glove\",\n",
    "    \"skateboard\",\n",
    "    \"surfboard\",\n",
    "    \"tennis racket\",\n",
    "    \"bottle\",\n",
    "    \"N/A\",\n",
    "    \"wine glass\",\n",
    "    \"cup\",\n",
    "    \"fork\",\n",
    "    \"knife\",\n",
    "    \"spoon\",\n",
    "    \"bowl\",\n",
    "    \"banana\",\n",
    "    \"apple\",\n",
    "    \"sandwich\",\n",
    "    \"orange\",\n",
    "    \"broccoli\",\n",
    "    \"carrot\",\n",
    "    \"hot dog\",\n",
    "    \"pizza\",\n",
    "    \"donut\",\n",
    "    \"cake\",\n",
    "    \"chair\",\n",
    "    \"couch\",\n",
    "    \"potted plant\",\n",
    "    \"bed\",\n",
    "    \"N/A\",\n",
    "    \"dining table\",\n",
    "    \"N/A\",\n",
    "    \"N/A\",\n",
    "    \"toilet\",\n",
    "    \"N/A\",\n",
    "    \"tv\",\n",
    "    \"laptop\",\n",
    "    \"mouse\",\n",
    "    \"remote\",\n",
    "    \"keyboard\",\n",
    "    \"cell phone\",\n",
    "    \"microwave\",\n",
    "    \"oven\",\n",
    "    \"toaster\",\n",
    "    \"sink\",\n",
    "    \"refrigerator\",\n",
    "    \"N/A\",\n",
    "    \"book\",\n",
    "    \"clock\",\n",
    "    \"vase\",\n",
    "    \"scissors\",\n",
    "    \"teddy bear\",\n",
    "    \"hair drier\",\n",
    "    \"toothbrush\",\n",
    "]\n",
    "\n",
    "# Set a confidence threshold to filter weak detections\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "MASK_THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e858d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:28:16.969938Z",
     "iopub.status.busy": "2026-02-12T21:28:16.969780Z",
     "iopub.status.idle": "2026-02-12T21:28:16.973472Z",
     "shell.execute_reply": "2026-02-12T21:28:16.973003Z"
    },
    "papermill": {
     "duration": 0.007457,
     "end_time": "2026-02-12T21:28:16.973978",
     "exception": false,
     "start_time": "2026-02-12T21:28:16.966521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prediction(img_path, threshold=0.5):\n",
    "    img = Image.open(img_path)  # This is for local images\n",
    "    # transform = T.Compose([T.ToTensor()]) # Turn the image into a torch.tensor\n",
    "    eval_transform = get_transform(train=False)\n",
    "    img = eval_transform(img)\n",
    "    img = img.to_device(device)\n",
    "    pred = model([img])\n",
    "\n",
    "    # Now we need to extract the bounding boxes and masks\n",
    "    pred_score = list(pred[0][\"scores\"].detach().cpu().numpy())\n",
    "    pred_t = [pred_score.index(x) for x in pred_score if x > threshold][-1]\n",
    "    masks = (pred[0][\"masks\"] > MASK_THRESHOLD).squeeze().detach().cpu().numpy()\n",
    "    pred_class = [\n",
    "        COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0][\"labels\"].cpu().numpy())\n",
    "    ]\n",
    "    pred_boxes = [\n",
    "        [(i[0], i[1]), (i[2], i[3])]\n",
    "        for i in list(pred[0][\"boxes\"].detach().cpu().numpy())\n",
    "    ]\n",
    "    masks = masks[: pred_t + 1]\n",
    "    pred_boxes = pred_boxes[: pred_t + 1]\n",
    "    pred_class = pred_class[: pred_t + 1]\n",
    "    return masks, pred_boxes, pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2941ac6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:28:16.980174Z",
     "iopub.status.busy": "2026-02-12T21:28:16.980021Z",
     "iopub.status.idle": "2026-02-12T21:28:16.984541Z",
     "shell.execute_reply": "2026-02-12T21:28:16.984048Z"
    },
    "papermill": {
     "duration": 0.00852,
     "end_time": "2026-02-12T21:28:16.985203",
     "exception": false,
     "start_time": "2026-02-12T21:28:16.976683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.v2 as T\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def get_prediction(img_path, threshold=0.5):\n",
    "    img = Image.open(img_path).convert(\"RGB\")  # Ensure image is RGB\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    img = transform(img)  # Apply transformations (now a PyTorch tensor)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    img = img.to(device)  # Now it's a tensor, so .to(device) works\n",
    "\n",
    "    model.to(device)  # Ensure model is on the correct device\n",
    "    model.eval()\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        pred = model([img])\n",
    "\n",
    "    # Extract predictions\n",
    "    pred_score = pred[0][\"scores\"].detach().cpu().numpy()\n",
    "    pred_t = [i for i, x in enumerate(pred_score) if x > threshold]\n",
    "\n",
    "    if len(pred_t) == 0:\n",
    "        return [], [], []\n",
    "\n",
    "    pred_t = pred_t[-1]\n",
    "\n",
    "    masks = (pred[0][\"masks\"] > MASK_THRESHOLD).squeeze().detach().cpu().numpy()\n",
    "    pred_class = [\n",
    "        COCO_INSTANCE_CATEGORY_NAMES[i] for i in pred[0][\"labels\"].cpu().numpy()\n",
    "    ]\n",
    "    pred_boxes = [\n",
    "        [(i[0], i[1]), (i[2], i[3])] for i in pred[0][\"boxes\"].detach().cpu().numpy()\n",
    "    ]\n",
    "\n",
    "    masks = masks[: pred_t + 1]\n",
    "    pred_boxes = pred_boxes[: pred_t + 1]\n",
    "    pred_class = pred_class[: pred_t + 1]\n",
    "\n",
    "    return masks, pred_boxes, pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc8285c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:28:16.991204Z",
     "iopub.status.busy": "2026-02-12T21:28:16.991023Z",
     "iopub.status.idle": "2026-02-12T21:28:17.622977Z",
     "shell.execute_reply": "2026-02-12T21:28:17.622288Z"
    },
    "papermill": {
     "duration": 0.635884,
     "end_time": "2026-02-12T21:28:17.623510",
     "exception": false,
     "start_time": "2026-02-12T21:28:16.987626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Download a sample image if not present\n",
    "img_path = \"./input.jpg\"\n",
    "if not os.path.exists(img_path):\n",
    "    print(\"Downloading sample image...\")\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/pytorch/vision/main/gallery/assets/dog1.jpg\"\n",
    "    )\n",
    "    urllib.request.urlretrieve(url, img_path)\n",
    "\n",
    "# Read image\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Run inference\n",
    "model.eval()\n",
    "masks, boxes, pred_cls = get_prediction(img_path, threshold=CONFIDENCE_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d843bf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:28:17.630509Z",
     "iopub.status.busy": "2026-02-12T21:28:17.630361Z",
     "iopub.status.idle": "2026-02-12T21:28:17.785590Z",
     "shell.execute_reply": "2026-02-12T21:28:17.785002Z"
    },
    "papermill": {
     "duration": 0.15971,
     "end_time": "2026-02-12T21:28:17.786545",
     "exception": false,
     "start_time": "2026-02-12T21:28:17.626835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def random_color_masks(image):\n",
    "    # I will copy a list of colors here\n",
    "    colors = [\n",
    "        [0, 255, 0],\n",
    "        [0, 0, 255],\n",
    "        [255, 0, 0],\n",
    "        [0, 255, 255],\n",
    "        [255, 255, 0],\n",
    "        [255, 0, 255],\n",
    "        [80, 70, 180],\n",
    "        [250, 80, 190],\n",
    "        [245, 145, 50],\n",
    "        [70, 150, 250],\n",
    "        [50, 190, 190],\n",
    "    ]\n",
    "    r = np.zeros_like(image).astype(np.uint8)\n",
    "    g = np.zeros_like(image).astype(np.uint8)\n",
    "    b = np.zeros_like(image).astype(np.uint8)\n",
    "    r[image == 1], g[image == 1], b[image == 1] = colors[random.randrange(0, 10)]\n",
    "    colored_mask = np.stack([r, g, b], axis=2)\n",
    "    return colored_mask\n",
    "\n",
    "\n",
    "for i in range(len(masks)):\n",
    "    rgb_mask = random_color_masks(masks[i])\n",
    "    img = cv2.addWeighted(img, 1, rgb_mask, 0.5, 0)\n",
    "    pt1 = tuple(int(x) for x in boxes[i][0])\n",
    "    pt2 = tuple(int(x) for x in boxes[i][1])\n",
    "    cv2.rectangle(img, pt1, pt2, color=(0, 255, 0), thickness=1)\n",
    "    cv2.putText(\n",
    "        img, pred_cls[i], pt1, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), thickness=1\n",
    "    )\n",
    "\n",
    "plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.894462,
   "end_time": "2026-02-12T21:28:18.710123",
   "environment_variables": {},
   "exception": null,
   "input_path": "notebooks/scene-understanding/maskrcnn/maskrcnn_torchvision_inference.ipynb",
   "output_path": "notebooks/scene-understanding/maskrcnn/maskrcnn_torchvision_inference-executed.ipynb",
   "parameters": {},
   "start_time": "2026-02-12T21:28:08.815661",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
