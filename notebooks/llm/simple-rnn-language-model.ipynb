{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53ZwO7dzC8kj"
   },
   "source": [
    "---\n",
    "title: Example of an RNN Language Model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pantelis-nlp/tutorial-nlp-notebooks/blob/main/rnn_language_model.ipynb) -->\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our aim is to predict the next character given a set of previous characters from our data string. For our RNN  implementation, we would take a sequence of length 25 characters as inputs to predict the next character.\n",
    "\n",
    "The notation used here was introduced first [here](http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture10.pdf). This minimal character-level Vanilla RNN model was first written by Andrej Karpathy (@karpathy) and was decorated with the forward and backprop equations by students of the CS-GY-6613 course as part of an asignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qaJ0WRAhG0cA"
   },
   "outputs": [],
   "source": [
    "data = 'Chios island is crescent or kidney shaped, 50 km (31 mi) long from north to south, and 29 km (18 mi) at its widest, covering an area of 842.289 km2 (325.210 sq mi).[2] The terrain is mountainous and arid, with a ridge of mountains running the length of the island. The two largest of these mountains, Pelineon (1,297 m (4,255 ft)) and Epos (1,188 m (3,898 ft)), are situated in the north of the island. The center of the island is divided between east and west by a range of smaller peaks, known as Provatas.'\n",
    "\n",
    "# you can also replace the data with any other txt file you want to use\n",
    "# data = open('input.txt', 'r').read() # should be simple plain text file - you can use any (small) file in txt format from the web or type your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mf_EEtaQYKH1"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fAPuTnEF8Xj",
    "outputId": "4f2fe837-1c9d-462c-a3e3-f7952a768ac6"
   },
   "outputs": [],
   "source": [
    "# creating a vocabulary of unique characters\n",
    "chars = list(set(data))                                                   \n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gSr5CNISzpvN",
    "outputId": "d28850ec-3aca-41f5-e821-82d1f75d8c2b"
   },
   "outputs": [],
   "source": [
    "# Data pre-processing\n",
    "# creating a dictionary, mapping characters to index and index to characters\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "print(char_to_ix)\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "print(ix_to_char)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WnOTeTaF-wBu"
   },
   "source": [
    "<img src='https://drive.google.com/uc?id=12ha59vACcd8eCEPAQdbZ4-axPGPKnn7F' width=\"700\">\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Inputs to RNN**\n",
    "- $x_1$ to $x_{25}$ is the input sequence of 25 characters, one character given as input to RNN at each time step\n",
    "\n",
    "\n",
    "**Hidden state of RNN**\n",
    "- The state consists of a single 'hidden' vector h\n",
    "- At every time step, a recurrence function $f_W$ with parameters $W_{xh}$, $W_{hh}$ and $b_h$ is applied to the input $x_t$ and the output from the previous hidden state $h_{t-1}$, to generate $h_t$\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad h_t = f_W (h_{t-1},x_t)$\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad \\; \\; \\; \\; = \\tanh (W_{hh}h_{t-1} + W_{xh}x_t + b_h)$\n",
    "\n",
    "**Outputs of the RNN**\n",
    "\n",
    "- $\\hat y_T$ is the character that our network would predict after $T=25$ time steps\n",
    "- At each time step, a $o_t$ is calculated as\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad o_t = W_{hy}h_t + b_y$\n",
    "\n",
    "- The softmax of $o_t$ is the set of probabilities of occurance of each unique character in the input data\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad \\hat y_t  = \\mathtt{softmax}(o_t)$\n",
    "\n",
    "- At each time step, from $t=1$ to $25$, loss is calculated from the set of predicted probabilities. \n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad L_t = \\mathtt{CE}(\\hat y_t, y_t)$\n",
    "\n",
    "$ \\qquad $ where the $y_t$ is the next character to the input sequence in the data string\n",
    "\n",
    "- The total loss is the sum of all the losses from the previously unrolled steps\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad L = ∑_{t=0}^{24}L_t$\n",
    "\n",
    "All the weights $W_{xh}$, $W_{hh}$, $b_h$, $W_{hy}$ and $b_y$ are reused at each time step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIu4DFm1QSV5"
   },
   "source": [
    "**Hyperparameters**\n",
    "\n",
    "- the size of hidden state of neurons\n",
    "- the sequence length or the time steps to unroll, which is 25 in our case\n",
    "- optimizer we use here is Adagrad\n",
    "- the learning rate for Adagrad optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZPu7_N4PoIZ"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100           # size of hidden state (number of RNN simple neurons)\n",
    "seq_length = 25               # number of time steps to unroll the RNN for, taking 25 previous characters to predict the next\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jukBMNnIPr2q"
   },
   "source": [
    "**Dimensions of tensors**\n",
    "\n",
    "Input:\n",
    "- Each character from the data string is pre-processed before being fed then into the RNN\n",
    "- From each sequence of 25 characters (for 25 time-steps) from the data string, we create an 'inputs' list of tokenized integer values \n",
    "- Each character is converted to an integer token index using 'char_to_ix' function, which maps each character to a number between 0 and 42 (as there are 43 unique characters in our data)\n",
    "- The integer tokens from the 'inputs' list are then one-hot encoded in 1-of-k representations, ie, into vectors of size 43 (k=43 unique characters in our data), which are fed as inputs to the RNN\n",
    "\n",
    "$ \\qquad $ => dimension of input $x_t =$ (43,1)\n",
    "\n",
    "Targets ($y_t$):\n",
    "- For each input in the 'inputs' list, we create a 'target' list consisting of the subsequent character's integer token\n",
    "- Our targets list, which is used during the cross-entropy loss calculation, is of length 25\n",
    "\n",
    "Predicted output:\n",
    "- The predicted outputs are the probabilities of the next characters\n",
    "- Since k=43 unique characters, the unnormalized logits for next chars is \n",
    "\n",
    "$ \\qquad $ => dimension of output $o_t =$ (43,1)\n",
    "\n",
    "- The $softmax(o_t)$ gives the class probabilities for next characters\n",
    "\n",
    "- The probabilities are then converted into one-hot encoded vectors using $\\arg \\max$\n",
    "\n",
    "- The one-hot encoded vectors are converted into integer tokens and then to a single character using 'ix_to_char' function\n",
    "\n",
    "Hidden layers:\n",
    "- Since we have chosen 100 neurons in the hidden layer,\n",
    "\n",
    "$ \\qquad $ => dimension of hidden state $h_t =$ (100,1)\n",
    "\n",
    "Model parameters:\n",
    "\n",
    "- Given the hidden_size=100,  input x dimension=(43,1) and output y dimension=(43,1):\n",
    "\n",
    "$ \\qquad $ => dimension of $W_{xh} =$ (100,43), \n",
    "\n",
    "$ \\qquad $ => dimension of $W_{hh} =$ (100,100), \n",
    "\n",
    "$ \\qquad $ => dimension of $b_{h} =$ (100,1), \n",
    "\n",
    "$ \\qquad $ => dimension of $W_{hy} =$ (43,100), \n",
    "\n",
    "$ \\qquad $ => dimension of $b_{y} =$ (43,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ixo26FFyYOGc"
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "# we set the initial values of the weights randomly from a normal distribution and set all the bias to zero\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01   # input to hidden, shape = (hidden_size, vocab_size) = (100,43)\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01  # hidden to hidden, shape = (hidden_size, hidden_size) = (100,100)\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01   # hidden to output, shape = (vocab_size, hidden_size) = (43,100)\n",
    "bh = np.zeros((hidden_size, 1))       # hidden bias, shape  = (hidden_size, 1) = (100,1)\n",
    "by = np.zeros((vocab_size, 1))         # output bias, shape  = (vocab_size, 1) = (43,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zGtoRZLakf-Y"
   },
   "source": [
    "**Forward Pass**\n",
    "\n",
    "- Forward through entire sequence $x_1$ to $x_{25}$ to compute loss\n",
    "\n",
    "- Calculate hidden states at each time step\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad h_t = tanh (W_{hh}h_{t-1} + W_{xh}x_t + b_h)$\n",
    "\n",
    "- Calculate output $y_t$ \n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad o_t = W_{hy}h_t + b_y$\n",
    "\n",
    "- The softmax of $o_t$ is the set of probabilities of occurance of each unique character in the input data\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad \\hat y_t = softmax(o_t)$\n",
    "\n",
    "- Calculate loss at each time step \n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad L_t = Cross\\;Entropy(\\hat y_t, y_t)$\n",
    "\n",
    "- Calculate the total loss, which is the negative log likelihood of our model\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad L = ∑_{t=0}^{24}L_t$\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad \\; \\; \\; \\; = - ∑_t log \\; p_{model} (y_t | x_1,...,x_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9J9bXiVlVH3"
   },
   "source": [
    "**Backpropogation Through Time**\n",
    "\n",
    "- Backward through entire sequence to compute gradient\n",
    "- The nodes include parameters $W_{xh}$, $W_{hh}$, $b_h$, $W_{hy}$ and $b_y$ \n",
    "- The inputs and outputs of nodes are $x_t$, $h_t$, $y_t$, $p_t$ and $L_t$ at time-step $t$\n",
    "- We'll use the suffix $(i)$ to indicate the $i^{th}$ sample\n",
    "\n",
    "*Gradients on the internal nodes:*\n",
    "\n",
    "- We'll be computing the gradients recursively starting with the nodes immediately preceding the final loss\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad \\frac{\\partial L}{\\partial L_t}  = 1 $\n",
    "\n",
    "- The gradient with respect to the softmax layer would be:\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad \\frac{\\partial L}{\\partial y_{(i)t}}  = p_{(i)t} -1 $\n",
    "\n",
    "- At t=25, the gradient with respect to the hidden layer would be:\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad \\frac{\\partial L}{\\partial h_{t=25}}  = {W_{hy}}^T \\frac{\\partial L}{\\partial y_{t=25}} $\n",
    "\n",
    "- We can now iterate backwards from t=24 down to t=1:\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad \\frac{\\partial L}{\\partial h_{t}}  =  \\bigg(\\frac{\\partial h_{t+1}}{\\partial h_{t}}\\bigg)^T \\frac{\\partial L}{\\partial h_{t+1}} + \\bigg(\\frac{\\partial y_{t}}{\\partial h_{t}}\\bigg)^T \\frac{\\partial L}{\\partial y_{t}}  $\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad \\qquad = (W_{hh})^T \\; diag\\bigg(1-(h_{t+1})^2\\bigg) \\frac{\\partial L}{\\partial h_{t+1}}  + (W_{hy})^T \\frac{\\partial L}{\\partial y_{t}} $\n",
    "\n",
    "$ \\qquad \\qquad $ where $diag\\bigg(1-(h_{t+1})^2\\bigg)$ indicates the diagonal matrix containing the elements $1-(h_{(i)t+1})^2$\n",
    "\n",
    "*Gradients on the parameter nodes:*\n",
    "\n",
    "- Gradients with respect to $W_{hy}$:\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad \\frac{\\partial L}{\\partial W_{hy}} = \\sum_t  \\sum_i \\frac{\\partial L}{\\partial y_{(i)t}} \\frac{\\partial y_{(i)t}}{\\partial W_{hy}} $\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad  \\qquad = \\sum_t  \\frac{\\partial L}{\\partial y_t} (h_t)^T$\n",
    "\n",
    "- Gradients with respect to $b_y$:\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad \\frac{\\partial L}{\\partial b_{y}} = \\sum_t \\bigg(\\frac{\\partial y_t}{\\partial b_{y}}\\bigg)^T \\frac{\\partial L}{\\partial y_{t}} $\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad  \\qquad = \\sum_t \\frac{\\partial L}{\\partial y_{t}} $\n",
    "\n",
    "- Gradients with respect to $W_{hh}$:\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad \\frac{\\partial L}{\\partial W_{hh}} = \\sum_t  \\sum_i \\frac{\\partial L}{\\partial h_{(i)t}} \\frac{\\partial h_{(i)t}}{\\partial W_{hh}} $\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad  \\qquad = \\sum_t diag\\bigg(1-(h_{t})^2\\bigg)  \\frac{\\partial L}{\\partial h_t} (h_{t-1})^T$\n",
    "\n",
    "- Gradients with respect to $b_h$:\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad \\frac{\\partial L}{\\partial b_{h}} = \\sum_t \\bigg(\\frac{\\partial h_t}{\\partial b_{h}}\\bigg)^T \\frac{\\partial L}{\\partial h_{t}} $\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad  \\qquad = \\sum_t diag\\bigg(1-(h_{t})^2\\bigg)  \\frac{\\partial L}{\\partial h_{t}} $\n",
    "\n",
    "- Gradients with respect to $W_{xh}$:\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad \\frac{\\partial L}{\\partial W_{xh}} = \\sum_t  \\sum_i \\frac{\\partial L}{\\partial h_{(i)t}} \\frac{\\partial h_{(i)t}}{\\partial W_{xh}} $\n",
    "\n",
    "$ \\qquad \\qquad \\qquad \\qquad  \\qquad = \\sum_t diag\\bigg(1-(h_{t})^2\\bigg)  \\frac{\\partial L}{\\partial h_t} (x_t)^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XU236iBMYtWS"
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  perform forward and backward pass\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "\n",
    "  xs, hs, os, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "\n",
    "  # forward pass: compute loss going forward\n",
    "  for t in range(len(inputs)):                                         # looping for t timesteps, which is the size of the length of inputs\n",
    "    xs[t] = np.zeros((vocab_size,1))                                   # xs = one-hot encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)    # hs_t = tanh(W_hh.hs_t-1 + W_xh.xs_t + b_h) -> hidden state\n",
    "    os[t] = np.dot(Why, hs[t]) + by                                    # os = W_hy.hs_t + b_y -> unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(os[t]) / np.sum(np.exp(os[t]))                      # ps = softmax(os) -> probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0])                               # cross-entropy loss\n",
    "\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)      # create numpy arrays for right size for the weights\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)                                    # create numpy arrays for right size for the biasses\n",
    "  dhnext = np.zeros_like(hs[0])                                                      # h_{t-1} for the first iteration is set to all zeros\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1                      # backprop into y by taking gradient for softmax (http://cs231n.github.io/neural-networks-case-study/#grad)\n",
    "    dWhy += np.dot(dy, hs[t].T)              # gradient for Why\n",
    "    dby += dy                                # gradient for by\n",
    "    dh = np.dot(Why.T, dy) + dhnext          # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh         # backprop through tanh nonlinearity\n",
    "    dbh += dhraw                             # gradient for bh\n",
    "    dWxh += np.dot(dhraw, xs[t].T)           # gradient for Wxh\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)         # gradient for Whh\n",
    "    dhnext = np.dot(Whh.T, dhraw)            # calculate h_t-1 for the next iteration\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam)                              # clip gradients to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJM73XLoY0q3"
   },
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  predicts probabilities for each character \n",
    "  returns the set of predicted indices with the highest probabilities\n",
    "  \"\"\"\n",
    "  # at test-time sample characters one at a time, feed back to model for next character prediction\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1                                              # x = one-hot encode the input for seed_ix letter in 1-of-k representation\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    # predicting the next character\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)         # h_t = tanh(W_hh.h_t-1 + W_xh.x_t + b_h) -> hidden state                    \n",
    "    y = np.dot(Why, h) + by                                   # y = W_hy.h_t + b_y -> unnormalized log probabilities for next chars\n",
    "    p = np.exp(y) / np.sum(np.exp(y))                         # p = softmax(y) -> probabilities for next chars\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())     # p.ravel gives the probabilities of each entry, with the maximum ix at argmax\n",
    "    x = np.zeros((vocab_size, 1))  \n",
    "    x[ix] = 1                                                 # convert probabilities to one-hot encoded vectors in 1-of-k representation\n",
    "    ixes.append(ix)\n",
    "  return ixes                                                 # return all the indices to convert them into characters and print the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qkOdfAJPY81L",
    "outputId": "fdd47001-7ecb-44fa-992e-f6cd9a353a22"
   },
   "outputs": [],
   "source": [
    "# p-data pointer, n-iteration counter\n",
    "n, p = 0, 0                        # setting both to zero in the beginning\n",
    "\n",
    "# memory variables for Adagrad, initialized to all zeros\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)    \n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) \n",
    "\n",
    "# loss at time instance 0\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length \n",
    "\n",
    "# while True:\n",
    "# running for 80000 epochs\n",
    "for i in range(80000):\n",
    "\n",
    "  # Data pre-processing to prepare inputs and targets\n",
    "  if p+seq_length+1 >= len(data) or n == 0:                        # sweeping from left to right in steps seq_length=25 long\n",
    "    hprev = np.zeros((hidden_size,1))                              # reset RNN memory\n",
    "    p = 0                                                          # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]         # inputs are tokens each of length seq_length=25\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]    # targets are the tokens of the subsequent characters for each input sequence\n",
    "\n",
    "  # Model testing\n",
    "  if n % 1000 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)                   # sample from the model and predict characters every 1000 iterations\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)           # convert tokens into characters and add it to the list of previous predictions\n",
    "    print('----\\n %s \\n----' % (txt, ))                         # print model predictions\n",
    "\n",
    "  # Model training\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)   # forward seq_length characters through the net and fetch gradient\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001                            # RNN adds all the losses from the previously unrolled steps\n",
    "  if n % 1000 == 0: print('iter %d, loss: %f' % (n, smooth_loss))             # print progress\n",
    "  \n",
    "  # parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8)              # adagrad parameter update\n",
    "\n",
    "  p += seq_length                                                       # move data pointer\n",
    "  n += 1                                                                # iteration counter "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d6993cb2f9ce9a59d5d7380609d9cb5192a9dedd2735a011418ad9e827eb538"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}