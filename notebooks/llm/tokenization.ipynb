{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Text Tokenization\n",
    "subtitle: focusing on GPT tokenizers\n",
    "---\n",
    "\n",
    "\n",
    "In earlier chapters we have limited the discussion to tokenizers that either produce a list of words or a list of characters. Its very important though to understand the connection that exists between tokenization and modeling for various NLP tasks.  \n",
    "\n",
    "**Model fine-tuning**\n",
    "\n",
    "The last few years we have seen the rise of pre-trained language models such as BERT, GPT-2, GPT-3, etc. These models are trained on large amounts of text data and are then used to perform downstream tasks such as text classification, text generation, etc. When you finetune a language model, you have to feed a pretrained model with the same tokenizer type that it was trained on. \n",
    "\n",
    "**Model training from scratch**\n",
    "\n",
    "If a language model is not available in the language you are interested in, or if your corpus is very different from the one your language model was trained on, you will most likely want to retrain the model from scratch using a tokenizer adapted to your data. That will require training a new tokenizer on your dataset. Follow [this guide](https://huggingface.co/course/chapter6/2?fw=pt) to learn how to do that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer types\n",
    "Lets review the main tokenizers so we highlight such connection for each type specifically.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word level tokenization\n",
    "\n",
    "The word-level tokenizers are intuitive, however, they suffer from the problem of unknown words, tagged as Out Of Vocabulary (OOV) words. They also tend to result into large vocabulary sizes, assigning different tokens to \"can't\" and \"cannot\". They also have issues with abbreviations eg. \"U.S.A\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "tokenizer = nlp.tokenizer\n",
    "\n",
    "text = \"I have a new GPU!\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character level tokenization\n",
    "\n",
    "Character-level tokenization are more flexible as they bypass the OOV issue, however to capture the context of each word we need to use much longer sequences and this results in loss of performance.  \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte Pair Encoding (BPE)\n",
    "\n",
    "In [BPE](https://arxiv.org/abs/1508.07909), one token can correspond to a character, an entire word or more, or anything in between and on average a token corresponds to 0.7 words. The idea behind BPE is to tokenize at word level frequently occuring words and at subword level the rarer words. GPT-3 uses a variant of BPE. \n",
    "\n",
    "![](images/bpe-example.png)\n",
    "\n",
    "Let see an example a tokenizer in action. We wull use the HuggingFace Tokenizers API and the GPT2 tokenizer. Note that this is called the _encoder_ as it is used to encode text into tokens. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokens = tokenizer.tokenize(\"she sells seashells by the seashore\")\n",
    "print(tokens)\n",
    "print(tokenizer.convert_tokens_to_string(tokens))\n",
    "print(tokenizer.convert_tokens_to_ids(tokens))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are perplexed about the `Ġ` please note that this character is what the encoder produces for the space character. The encoder code doesn't like spaces, so they replace spaces and other whitespace characters with other unicode bytes.  The encoder [more specifically](https://github.com/openai/gpt-2/issues/80) takes all control and whitespace characters in code points 0-255 and shifts them up by 256 to make them printable. So space (code point 32) becomes Ġ (code point 288)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we have another subword tokenization from the BERT tokenizer known as [WordPiece](aperswithcode.com/method/wordpiece)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.tokenize(\"I have a new GPU!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"##\" means that the rest of the token should be attached to the previous one, without space (for decoding or reversal of the tokenization).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [tiktokenizer webapp](https://tiktokenizer.vercel.app/?model=gpt2) runs various tokenizers in your browser. Its is a great tool to understand the impact of tokenizers on a language model complexity. Copy and paste the following code into the text box. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# A simple Python program to calculate the area of a rectangle\n",
    "\n",
    "# Function to calculate the area of a rectangle\n",
    "def calculate_rectangle_area(length, width):\n",
    "  \"\"\"\n",
    "  Calculates the area of a rectangle.\n",
    "\n",
    "  Args:\n",
    "    length: The length of the rectangle.\n",
    "    width: The width of the rectangle.\n",
    "\n",
    "  Returns:\n",
    "    The area of the rectangle.\n",
    "  \"\"\"\n",
    "  area = length * width\n",
    "  return area\n",
    "\n",
    "# Get input from the user\n",
    "length = float(input(\"Enter the length of the rectangle: \"))\n",
    "width = float(input(\"Enter the width of the rectangle: \"))\n",
    "\n",
    "# Calculate the area\n",
    "area = calculate_rectangle_area(length, width)\n",
    "\n",
    "# Print the result\n",
    "print(\"The area of the rectangle is:\", area)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe\n",
    "\tsrc=\"https://tiktokenizer.vercel.app/?model=gpt2\"\n",
    "\tframeborder=\"0\"\n",
    "\twidth=\"850\"\n",
    "\theight=\"550\"\n",
    "></iframe>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to test two tokenizers `cl100k_base` that has a `100,000` vocabulary size and `gpt2` that has approximately a `50,000` vocabulary size. The first one is used by the GPT-3.5 and GPT-4 models, while the second one is used by the GPT-2 model.\n",
    " \n",
    "Select `gpt2` from the list of tokenizers and observe the number of tokens produced. For the code above, this would be 186 tokens. The same text would result in 149 tokens with the `cl100k_base` tokenizer. Why is that and what is the impact of this on the model complexity?\n",
    "\n",
    "1. `gpt2` needs more tokens to express the same words due to its less expressive vocabulary. This is exactly the same thing that happens if you dont know a word eg `intricate` you will use a longer phrase to express the same thing eg. very complicated`. \n",
    "\n",
    "2. `gpt2` will however result into a smaller model size since the vocabulary size is smaller, as evident by the transformer architecture. On the other hand, the increased token size will result into a larger requirement for context memory. The same python code will occupy more memory in the `gpt2` coding agent and the agent is more likely to be slower to respond since the longer sequences result into increased attention and memory requirements and also more likely to hallucinate since the model is more likely to hallucinate when its context reaches its limits. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}