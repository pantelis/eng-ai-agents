{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KmEyadzTtGxY"
   },
   "source": [
    "# Attention in RNN NMT Workshop\n",
    "https://nlpdemystified.org<br>\n",
    "https://github.com/nitinpunjabi/nlp-demystified<br><br>\n",
    "\n",
    "Course module for this demo: https://www.nlpdemystified.org/course/seq2seq-and-attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uOVYaAveQJia"
   },
   "source": [
    "**IMPORTANT**<br>\n",
    "In Colab enable **GPU acceleration** by going to *Runtime > Change Runtime Type*. Keep in mind that, on certain tiers, you're not guaranteed GPU access depending on usage history and current load.\n",
    "<br><br>\n",
    "Also, if you're running this in the cloud rather than a local Jupyter server on your machine, then the notebook will *timeout* after a period of inactivity.\n",
    "<br><br>\n",
    "Refer to this link on how to run Colab notebooks locally on your machine to avoid this issue:<br>\n",
    "https://research.google.com/colaboratory/local-runtimes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWyjB-YNwTG_"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "\n",
    "#from google.colab import files\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fN1DbrqyQtpV"
   },
   "source": [
    "# Recurrence-based Seq2Seq Neural Machine Translation **WITHOUT** Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvLQOElRW0MW"
   },
   "source": [
    "Our first model will use just two LSTMs (one encoder and one decoder) to translate Hungarian to English. We're going with Hungarian here because it's a particularly challenging language to tackle with few resources.\n",
    "<br><br>\n",
    "The dataset we'll use comes from **Tatoeba**, a collection of sentence translations in a variety of languages sourced from volunteers:\n",
    "<br>\n",
    "https://tatoeba.org/en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZ3_Nbokp-0W"
   },
   "source": [
    "The **NLP Demystified** repo has a lightly processed copy that's already been split into train/validation/test sets and pre-shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9K6qTnn2zFY2"
   },
   "outputs": [],
   "source": [
    "# Download the training set.\n",
    "!wget https://raw.githubusercontent.com/nitinpunjabi/nlp-demystified/main/datasets/hun_eng_pairs/hun_eng_pairs_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTWIYr1SzFY3"
   },
   "outputs": [],
   "source": [
    "with open('hun_eng_pairs_train.txt') as file:\n",
    "  train = [line.rstrip() for line in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gl-PYX7rej4r"
   },
   "source": [
    "Each entry consists of a Hungarian sentence followed by its English translation, separated by a **\\<sep\\>** delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3k8goMfzXdA"
   },
   "outputs": [],
   "source": [
    "train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayx92dzBQecH"
   },
   "source": [
    "This is a relatively tiny dataset for neural machine translation, but we'll see how far we can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wh7zTszrTEK2"
   },
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5InhndWgw32G"
   },
   "outputs": [],
   "source": [
    "# Separate the input (Hungarian) and target (English) sentences into separate lists.\n",
    "SEPARATOR = '<sep>'\n",
    "train_input, train_target = map(list, zip(*[pair.split(SEPARATOR) for pair in train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XGZLCXKE6QM"
   },
   "outputs": [],
   "source": [
    "print(train_input[:3])\n",
    "print(train_target[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o06kZA4tkYPV"
   },
   "source": [
    "Since we're dealing with a source language that uses accented characters, it's important to apply *Unicode normalization* \n",
    "<br><br>\n",
    "In the example below, two different sets of Unicode yield the same character visually. The first Unicode is for an accented 'a', while the second Unicode combines an 'a' with an accent mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1hp9vFAbk749"
   },
   "outputs": [],
   "source": [
    "print(\"\\u00E1\", \"\\u0061\\u0301\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5add_L2ngre"
   },
   "source": [
    "Though these characters look the same to us reading them, they'll be treated differently by a model. So to avoid this, the following function normalizes any accented characters into the same set of Unicode, and then replaces them with their ASCII equivalents.<br>\n",
    "https://docs.python.org/3/library/unicodedata.html\n",
    "<br><br>\n",
    "Here's an informative article on the importance of Unicode normalization and how to go about it (including what _NFD_ means):<br>\n",
    "https://towardsdatascience.com/what-on-earth-is-unicode-normalization-56c005c55ad0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxNbQzN5XI1D"
   },
   "outputs": [],
   "source": [
    "# Unicode normalization\n",
    "def normalize_unicode(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHL9FrkqwhuZ"
   },
   "source": [
    "We're building a **word**-based translation model, but we still want to keep punctuation and treat them as separate tokens, so we'll insert a space between any relevant punctuation and the characters around them. This way, our tokenizer (which won't filter out punctuation) will output punctuation marks as separate tokens.\n",
    "\n",
    "This function does that and unicode normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHjmr9BtaJyv"
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(s):\n",
    "  s = normalize_unicode(s)\n",
    "  s = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", s)\n",
    "  s = re.sub(r'[\" \"]+', \" \", s)\n",
    "  s = s.strip()\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQ8QPyDhXIvU"
   },
   "outputs": [],
   "source": [
    "# Preprocess both the source and target sentences.\n",
    "train_preprocessed_input = [preprocess_sentence(s) for s in train_input]\n",
    "train_preprocessed_target = [preprocess_sentence(s) for s in train_target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqPCyKOFkzy0"
   },
   "source": [
    "After preprocessing, the unicode should be normalized and there should be spaces on either side of any punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZRl1nciXIoI"
   },
   "outputs": [],
   "source": [
    "train_preprocessed_input[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HON19ezK04I5"
   },
   "source": [
    "Like the language model we built in the [previous demo](https://github.com/nitinpunjabi/nlp-demystified/blob/main/notebooks/nlpdemystified_recurrent_neural_networks.ipynb), we'll use **Teacher Forcing** with our translation model (specifically, the decoder). This begins by placing a  start-of-sentence tag (_\\<sos\\>_) and end-of-sentence tag (_\\<eos\\>_) at the beginning and end of each target sentence, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIh6ztc6HjwS"
   },
   "outputs": [],
   "source": [
    "def tag_target_sentences(sentences):\n",
    "  tagged_sentences = map(lambda s: (' ').join(['<sos>', s, '<eos>']), sentences)\n",
    "  return list(tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orS_vjpWHlqZ"
   },
   "outputs": [],
   "source": [
    "train_tagged_preprocessed_target = tag_target_sentences(train_preprocessed_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DeF-zDEXJM1l"
   },
   "outputs": [],
   "source": [
    "train_tagged_preprocessed_target[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gVyHPLd3yhQ"
   },
   "source": [
    "Next, we'll tokenize our input and target sentences, taking care to keep relevant punctuation.<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "<br><br>\n",
    "Note that we're also including an out-of-vocabulary token (_\\<unk\\>_) in the tokenizer initialization. At inference time, if the tokenizer encounters a word it didn't see during the initial fit on the training data, that word will be replaced with _\\<unk\\>_ and the translation system will need to cope with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUKSKBBdGe1T"
   },
   "outputs": [],
   "source": [
    "# Tokenizer for the Hungarian input sentences. Note how we're not filtering punctuation.\n",
    "source_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<unk>', filters='\"#$%&()*+-/:;=@[\\\\]^_`{|}~\\t\\n')\n",
    "source_tokenizer.fit_on_texts(train_preprocessed_input)\n",
    "source_tokenizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYZxoraaGrC_"
   },
   "outputs": [],
   "source": [
    "source_vocab_size = len(source_tokenizer.word_index) + 1\n",
    "print(source_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cH61k4h6GxKH"
   },
   "outputs": [],
   "source": [
    "# Tokenizer for the English target sentences.\n",
    "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<unk>', filters='\"#$%&()*+-/:;=@[\\\\]^_`{|}~\\t\\n')\n",
    "target_tokenizer.fit_on_texts(train_tagged_preprocessed_target)\n",
    "target_tokenizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0pQSlygHJvz"
   },
   "outputs": [],
   "source": [
    "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
    "print(target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4P8Lo14MDk2f"
   },
   "source": [
    "Next, we'll vectorize the input and target sentences just like we did in the last few demos.<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqhEKYYfLbUx"
   },
   "outputs": [],
   "source": [
    "train_encoder_inputs = source_tokenizer.texts_to_sequences(train_preprocessed_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUzomY0BNVPN"
   },
   "outputs": [],
   "source": [
    "print(train_encoder_inputs[:3])\n",
    "print(source_tokenizer.sequences_to_texts(train_encoder_inputs[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMbFHSf7D8wv"
   },
   "source": [
    "For teacher forcing, we'll create two copies of each vectorized **target** sentence, with the second copy shifted over by one.\n",
    "\n",
    "The function below takes a collection of sentences, vectorizes them, then returns two copies of each. The first will include every token except the last, the second will include every token except the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6ZKc-aCBQOo"
   },
   "outputs": [],
   "source": [
    "def generate_decoder_inputs_targets(sentences, tokenizer):\n",
    "  seqs = tokenizer.texts_to_sequences(sentences)\n",
    "  decoder_inputs = [s[:-1] for s in seqs] # Drop the last token in the sentence.\n",
    "  decoder_targets = [s[1:] for s in seqs] # Drop the first token in the sentence.\n",
    "\n",
    "  return decoder_inputs, decoder_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RELD4y5XN8AE"
   },
   "outputs": [],
   "source": [
    "train_decoder_inputs, train_decoder_targets = generate_decoder_inputs_targets(train_tagged_preprocessed_target, \n",
    "                                                                              target_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AT_gZyWwEY5J"
   },
   "source": [
    "Each token of a *decoder_input* sentence will be fed to the decoder as the **next** expected token, and each token of a *decoder_target* sentence will be used to calculate the loss against the decoder's actual output; exactly as we covered in the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMxvdK1KOCeO"
   },
   "outputs": [],
   "source": [
    "print(train_decoder_inputs[0], train_decoder_targets[0])\n",
    "\n",
    "print(target_tokenizer.sequences_to_texts(train_decoder_inputs[:1]), \n",
    "      target_tokenizer.sequences_to_texts(train_decoder_targets[:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USDP1n_2Fb63"
   },
   "source": [
    "Exactly as we did in the previous [RNN demo](https://github.com/nitinpunjabi/nlp-demystified/blob/main/notebooks/nlpdemystified_recurrent_neural_networks.ipynb), we'll pad all sequences to a uniform length.<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kgv8kSVYXE4u"
   },
   "outputs": [],
   "source": [
    "max_encoding_len = len(max(train_encoder_inputs, key=len))\n",
    "max_encoding_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_t_DMykQtmQQ"
   },
   "outputs": [],
   "source": [
    "max_decoding_len = len(max(train_decoder_inputs, key=len))\n",
    "max_decoding_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzJdEAei2BEw"
   },
   "outputs": [],
   "source": [
    "padded_train_encoder_inputs = pad_sequences(train_encoder_inputs, max_encoding_len, padding='post', truncating='post')\n",
    "padded_train_decoder_inputs = pad_sequences(train_decoder_inputs, max_decoding_len, padding='post', truncating='post')\n",
    "padded_train_decoder_targets = pad_sequences(train_decoder_targets, max_decoding_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIikywhbt1Xs"
   },
   "outputs": [],
   "source": [
    "print(padded_train_encoder_inputs[0])\n",
    "print(padded_train_decoder_inputs[0])\n",
    "print(padded_train_decoder_targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nz_a2DT8NVVa"
   },
   "source": [
    "When converting a padded sequence back to text, padding is replaced with the out-of-vocabulary *\\<unk\\>* token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "330US5M0V-5A"
   },
   "outputs": [],
   "source": [
    "target_tokenizer.sequences_to_texts([padded_train_decoder_inputs[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAhD3d_2Nnqr"
   },
   "source": [
    "The training dataset is now ready, and we can follow the same preprocessing steps to prepare the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XybzLmgmzvNO"
   },
   "outputs": [],
   "source": [
    "# Download validation set pairs.\n",
    "!wget https://raw.githubusercontent.com/nitinpunjabi/nlp-demystified/main/datasets/hun_eng_pairs/hun_eng_pairs_val.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mF8AOZMQzvNP"
   },
   "outputs": [],
   "source": [
    "with open('hun_eng_pairs_val.txt') as file:\n",
    "  val = [line.rstrip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTY0nUIKfKzq"
   },
   "outputs": [],
   "source": [
    "def process_dataset(dataset):\n",
    "\n",
    "  # Split the Hungarian and English sentences into separate lists.\n",
    "  input, output = map(list, zip(*[pair.split(SEPARATOR) for pair in dataset]))\n",
    "\n",
    "  # Unicode normalization and inserting spaces around punctuation.\n",
    "  preprocessed_input = [preprocess_sentence(s) for s in input]\n",
    "  preprocessed_output = [preprocess_sentence(s) for s in output]\n",
    "\n",
    "  # Tag target sentences with <sos> and <eos> tokens.\n",
    "  tagged_preprocessed_output = tag_target_sentences(preprocessed_output)\n",
    "\n",
    "  # Vectorize encoder source sentences.\n",
    "  encoder_inputs = source_tokenizer.texts_to_sequences(preprocessed_input)\n",
    "\n",
    "  # Vectorize and create decoder input and target sentences.\n",
    "  decoder_inputs, decoder_targets = generate_decoder_inputs_targets(tagged_preprocessed_output, \n",
    "                                                                    target_tokenizer)\n",
    "  \n",
    "  # Pad all collections.\n",
    "  padded_encoder_inputs = pad_sequences(encoder_inputs, max_encoding_len, padding='post', truncating='post')\n",
    "  padded_decoder_inputs = pad_sequences(decoder_inputs, max_decoding_len, padding='post', truncating='post')\n",
    "  padded_decoder_targets = pad_sequences(decoder_targets, max_decoding_len, padding='post', truncating='post')\n",
    "\n",
    "  return padded_encoder_inputs, padded_decoder_inputs, padded_decoder_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXyGfl0TXEKD"
   },
   "outputs": [],
   "source": [
    "# Process validation dataset\n",
    "padded_val_encoder_inputs, padded_val_decoder_inputs, padded_val_decoder_targets = process_dataset(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XthOgq70_OMK"
   },
   "source": [
    "We're now ready to build our translation model. These are the parameters we'll use (feel free to try different values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "boKqI_J6gpWz"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "default_dropout=0.2\n",
    "batch_size = 32\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li_LsdIBOJ9p"
   },
   "source": [
    "Unlike the models we've built up to this point, this model will have **two** inputs:<br>\n",
    "1. The encoder receives the source sentences (Hungarian) and generates the initial state inputs for the decoder.\n",
    "2. The decoder receives the decoder input sentences (English) for teacher forcing.\n",
    "<br>\n",
    "\n",
    "Since that's the case, we won't use the Keras **Sequential API**. Rather, we'll use the **Functional API** which will give us more flexibility in expressing our model layers as graphs. Fortunately, the work remains pretty intuitive as long as we keep in mind what's happening behind the scenes.\n",
    "<br>\n",
    "\n",
    "https://keras.io/api/models/sequential/<br>\n",
    "https://www.tensorflow.org/guide/keras/functional/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qg2A5kYBR-72"
   },
   "source": [
    "The code in the following cell specifies the encoder. If you haven't used the **Functional API** before, the most important thing to realize here is this: Each layer is like a _node_ in a graph. Each time we pass the output of one layer to the next, it creates a link between the two _nodes_ and allows data to flow through. Once the whole training model is built, we'll visualize it for greater clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oK3Cs08XXEGv"
   },
   "outputs": [],
   "source": [
    "# The initial encoder input layer which will take in padded sequences. We're specifying\n",
    "# a shape of None here but you can specify it upfront if you want since we\n",
    "# know what the max encoding length is.\n",
    "encoder_inputs = layers.Input(shape=[None], name='encoder_inputs')\n",
    "\n",
    "# The embedding layer. Similar to what we did in the RNN demo.\n",
    "encoder_embeddings = layers.Embedding(source_vocab_size, \n",
    "                                      embedding_dim,\n",
    "                                      mask_zero=True,\n",
    "                                      name='encoder_embeddings')\n",
    "\n",
    "# Passing the input layer output to the embedding layer creates a link between the\n",
    "# two. Input sequences will now flow into the embedding layer which will output\n",
    "# a sequence of embeddings.\n",
    "encoder_embedding_output = encoder_embeddings(encoder_inputs)\n",
    "\n",
    "\n",
    "# We're not using any kind of attention mechanism in this model, so setting only\n",
    "# return_state to True is enough. return_sequences remains False.\n",
    "encoder_lstm = layers.LSTM(hidden_dim, \n",
    "                           return_state=True, \n",
    "                           dropout=default_dropout, \n",
    "                           name='encoder_lstm')\n",
    "\n",
    "# Passing the embedding layer output to the LSTM layer creates another link.\n",
    "# IMPORTANT: The LSTM always returns three values. When return_sequences is\n",
    "# False, encoder_outputs and state_h are the SAME. When return_sequences is\n",
    "# True, encoder_outputs contains the encoder hidden states from each time step.\n",
    "#\n",
    "# Side note: we won't be using encoder_outputs here so that variable can be \n",
    "# replaced with a _ if preferred.\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding_output)\n",
    "\n",
    "# The final hidden and cell/context states from the encoder will be the the\n",
    "# initial states for the decoder.\n",
    "encoder_states = (state_h, state_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfzEZs8AZI9p"
   },
   "source": [
    "The code for specifying the decoder is similar except for three additions:\n",
    "1. The LSTM has *return_sequences* set to *True* since we'll need the hidden state outputted at each timestep (similar to our PoS tagger and language model from the [RNN demo](https://github.com/nitinpunjabi/nlp-demystified/blob/main/notebooks/nlpdemystified_recurrent_neural_networks.ipynb)).\n",
    "\n",
    "2. The decoder's LSTM takes an *initial_state*, the value for which is from the encoder.\n",
    "\n",
    "3. There's a *softmax* layer in the end to generate a probability distribution over the target (English) vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1A4I3gBUXEDl"
   },
   "outputs": [],
   "source": [
    "decoder_inputs = layers.Input(shape=[None], name='decoder_inputs')\n",
    "\n",
    "\n",
    "decoder_embeddings = layers.Embedding(target_vocab_size, \n",
    "                                      embedding_dim, \n",
    "                                      mask_zero=True,\n",
    "                                      name='decoder_embeddings')\n",
    "\n",
    "\n",
    "decoder_embedding_output = decoder_embeddings(decoder_inputs)\n",
    "\n",
    "# Return sequences set to True.\n",
    "decoder_lstm = layers.LSTM(hidden_dim,\n",
    "                           return_sequences=True,\n",
    "                           return_state=True,\n",
    "                           dropout=default_dropout,\n",
    "                           name='decoder_lstm')\n",
    "\n",
    "\n",
    "# Set the decoder's initial state to the encoder's final output states. Since\n",
    "# return_sequences is set to True, decoder_outputs is going to be a collection of\n",
    "# the decoder's hidden state at each timestep. Also note that since we don't need\n",
    "# the decoder's final hidden output and cell states, those are just set to _.\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding_output, initial_state=encoder_states)\n",
    "\n",
    "# Have a softmax layer in the end to create a probability distribution for the output word.\n",
    "decoder_dense = layers.Dense(target_vocab_size, activation='softmax', name='decoder_dense')\n",
    "\n",
    "# The probability distribution for the output word.\n",
    "y_proba = decoder_dense(decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssRFKO4kccti"
   },
   "source": [
    "We can now create our model and specify that it has **two** inputs and one output.\n",
    "<br><br>\n",
    "Note that *accuracy* is a crude metric for translation models and just serves as a proxy for how our model is doing. As we covered in the slides, the popular performance metric is BLEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6UmSwvBRp95K"
   },
   "outputs": [],
   "source": [
    "# Note how the model is taking two inputs in an array.\n",
    "model = tf.keras.Model([encoder_inputs, decoder_inputs], y_proba, name='hun_eng_seq2seq_nmt_no_attention')\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',  metrics='sparse_categorical_accuracy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNl1yOJahz_P"
   },
   "source": [
    "We can visualize our model to get a better idea of the flow we've built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2y7bTLoHMiDE"
   },
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='hun_eng_seq2seq_nmt_no_attention.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpznSJt-dhyz"
   },
   "source": [
    "I personally found keeping the right matrix dimensions in mind more challenging and tricky than the theoretical concepts. So here are the dimensions we're working with in this training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Do3ANrLdeRzj"
   },
   "outputs": [],
   "source": [
    "print('encoder_inputs layer\\n input dimension {}\\n output dimension: {}'.format((batch_size, max_encoding_len), (batch_size, max_encoding_len)))\n",
    "print()\n",
    "print('encoder_embeddings layer\\n input dimension {}\\n output dimension: {}'.format((batch_size, max_encoding_len), (batch_size, max_encoding_len, embedding_dim)))\n",
    "print()\n",
    "print('encoder_lstm layer\\n input dimension {}\\n output dimension: {}'.format((batch_size, max_encoding_len, embedding_dim), [(batch_size, hidden_dim), (batch_size, hidden_dim), (batch_size, hidden_dim)]))\n",
    "print()\n",
    "print()\n",
    "print('decoder_inputs layer\\n input dimension {}\\n output dimension: {}'.format((batch_size, max_decoding_len), (batch_size, max_decoding_len)))\n",
    "print()\n",
    "print('decoder_embeddings layer\\n input dimension {}\\n output dimension: {}'.format((batch_size, max_decoding_len), (batch_size, max_decoding_len, embedding_dim)))\n",
    "print()\n",
    "print('decoder_lstm layer\\n input dimension {}\\n output dimension: {}'.format([(batch_size, max_decoding_len, embedding_dim), (batch_size, hidden_dim), (batch_size, hidden_dim)], [(batch_size, max_decoding_len, hidden_dim), (batch_size, hidden_dim), (batch_size, hidden_dim)]))\n",
    "print()\n",
    "print('decoder_dense layer(softmax)\\n input dimension {}\\n output dimension: {}'.format((batch_size, max_decoding_len, hidden_dim), (batch_size, max_decoding_len, target_vocab_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zy_BUgVuiYLo"
   },
   "source": [
    "Because this model takes a while to train, we're using model checkpoints to save the weights after every epoch. This way, if something goes wrong with our system during training, we can reload the last set of weights from the checkpoint, and resume training from there.\n",
    "https://keras.io/api/callbacks/model_checkpoint/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vepIWKQYp2q5"
   },
   "outputs": [],
   "source": [
    "# Saving this to a folder on my local machine.\n",
    "filepath=\"./HunEngNMTNoAttention/training1/cp.ckpt\"\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=filepath,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fa2VKH-jmU0"
   },
   "source": [
    "We'll also use **early stopping** as we have recently.\n",
    "<br><br>\n",
    "The *fit* method below is commmented out because I trained the model ahead of time and saved it. I also saved the tokenizers.\n",
    "<br><br>\n",
    "If you want to train it yourself, feel free to uncomment and execute it. Keep in mind that because of the random weight initialization, your trained model's output will likely differ from mine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4H6Gg10ofnu"
   },
   "outputs": [],
   "source": [
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# history = model.fit([padded_train_encoder_inputs, padded_train_decoder_inputs], padded_train_decoder_targets,\n",
    "#                      batch_size=batch_size,\n",
    "#                      epochs=epochs,\n",
    "#                      validation_data=([padded_val_encoder_inputs, padded_val_decoder_inputs], padded_val_decoder_targets),\n",
    "#                      callbacks=[cp_callback, es_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUFCwtx7klZ2"
   },
   "source": [
    "The model I previously trained exited at epoch 12 with these metrics.\n",
    "\n",
    "> Epoch 12: saving model to ./HunEngNMTNoAttention/training1/cp.ckpt\n",
    "2771/2771 [==============================] - 105s 38ms/step \n",
    "- loss: 0.1315 - sparse_categorical_accuracy: 0.8450 \n",
    "- val_loss: 0.3996 - val_sparse_categorical_accuracy: 0.6832\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RHDpipylWyR"
   },
   "source": [
    "If you choose to train your own model, the following functions save the model and tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCZRcpLR18vG"
   },
   "outputs": [],
   "source": [
    "###### Save the model.\n",
    "# model.save('hun_eng_s2s_nmt_no_attention')\n",
    "\n",
    "\n",
    "###### Zip and download the model.\n",
    "# !zip -r ./hun_eng_s2s_nmt_no_attention.zip ./hun_eng_s2s_nmt_no_attention\n",
    "# files.download(\"./hun_eng_s2s_nmt_no_attention.zip\")\n",
    "\n",
    "\n",
    "###### Save the tokenizers as JSON files. The resulting files can be downloaded by left-clicking on them.\n",
    "# source_tokenizer_json = source_tokenizer.to_json()\n",
    "# with io.open('source_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "#   f.write(json.dumps(source_tokenizer_json, ensure_ascii=False))\n",
    "\n",
    "# target_tokenizer_json = target_tokenizer.to_json()\n",
    "# with io.open('target_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "#   f.write(json.dumps(target_tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjqDKLYY1Wyh"
   },
   "source": [
    "At this point, we can retrieve the saved model and saved tokenizers and try them on the **test** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSAzAy6durpU"
   },
   "outputs": [],
   "source": [
    "# Retrieve the tokenizers.\n",
    "!wget https://github.com/nitinpunjabi/nlp-demystified/raw/main/models/nmt_no_attention/hun_eng_s2s_nmt_no_attention_tokenizers.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TZYjVFRurhc"
   },
   "outputs": [],
   "source": [
    "!unzip -o hun_eng_s2s_nmt_no_attention_tokenizers.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-vWynGLnQXW"
   },
   "source": [
    "The tokenizers can be loaded using the *tokenizer_from_json* method.<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/tokenizer_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7S8c2BMhurYT"
   },
   "outputs": [],
   "source": [
    "with open('source_tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    source_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)\n",
    "\n",
    "with open('target_tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    target_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXo-xKdB60IL"
   },
   "outputs": [],
   "source": [
    "# Retrieve the model.\n",
    "!wget https://github.com/nitinpunjabi/nlp-demystified/raw/main/models/nmt_no_attention/hun_eng_s2s_nmt_no_attention_model.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GutiVtIH60Dj"
   },
   "outputs": [],
   "source": [
    "!unzip -o hun_eng_s2s_nmt_no_attention_model.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CiP3Xatc6z-0"
   },
   "outputs": [],
   "source": [
    "# Load the model.\n",
    "model = tf.keras.models.load_model('hun_eng_s2s_nmt_no_attention')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCFkCbRZnsig"
   },
   "source": [
    "The *test* dataset contains sentences (and most certainly words) unseen by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-JAKiUuxz4V"
   },
   "outputs": [],
   "source": [
    "# Retrieve the test dataset.\n",
    "!wget https://raw.githubusercontent.com/nitinpunjabi/nlp-demystified/main/datasets/hun_eng_pairs/hun_eng_pairs_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyYokpwfxz05"
   },
   "outputs": [],
   "source": [
    "with open('hun_eng_pairs_test.txt') as file:\n",
    "  test = [line.rstrip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6xyrNgzxzw5"
   },
   "outputs": [],
   "source": [
    "test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5HV85X36xztV"
   },
   "outputs": [],
   "source": [
    "# Preprocess test dataset\n",
    "padded_test_encoder_inputs, padded_test_decoder_inputs, padded_test_decoder_targets = process_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SS294ZqzDy-"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set.\n",
    "model.evaluate([padded_test_encoder_inputs, padded_test_decoder_inputs], padded_test_decoder_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnPNakv9q-tq"
   },
   "source": [
    "Our training model uses teacher forcing, but that won't be the case for inference. So we'll take the trained layers from our training model, and create **separate**, stand-alone encoder and decoder models. This will give us much greater control over how the output is created as we'll see. (e.g. feeding the current output as the input to the next time step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ru9EZBCDCuwg"
   },
   "outputs": [],
   "source": [
    "# These are the layers of our trained model.\n",
    "[layer.name for layer in model.layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBJ7VlGTvp7b"
   },
   "source": [
    "Creating a stand-alone encoder is just a matter of retrieving the trained layers by name and re-creating the graph.\n",
    "\n",
    "The major difference here is the last line where we're calling *tf.keras.Model* to create a stand-alone encoder with *encoder_inputs* as the input and *encoder_states* as the output.<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/Model#get_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADvS6K_pC2Vs"
   },
   "outputs": [],
   "source": [
    "encoder_inputs = model.get_layer('encoder_inputs').input\n",
    "\n",
    "encoder_embedding_layer = model.get_layer('encoder_embeddings')\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_inputs)\n",
    "\n",
    "encoder_lstm = model.get_layer('encoder_lstm')\n",
    "\n",
    "_, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embeddings)\n",
    "\n",
    "encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "# Our stand-alone encoder model. encoder_inputs is the input to the encoder,\n",
    "# and encoder_states is the expected output.\n",
    "encoder_model_no_attention = tf.keras.Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ISTyd0lC2bc"
   },
   "outputs": [],
   "source": [
    "plot_model(encoder_model_no_attention, to_file='encoder_model_no_attention_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "on_bkG5Hwfy8"
   },
   "source": [
    "Creating the decoder is similar. The major difference here is that there are two additional inputs to the decoder representing its LSTM hidden state (*decoder_input_state_h*) and cell state (*decoder_input_state_c*), respectively.\n",
    "<br><br>\n",
    "They are there because the encoder and decoder are now separate models, so we'll manually take the encoder's output (its final states) and use them as the decoder's initial state. From there, at each time step, we'll take the decoder's state outputs and feed them to the next time step.\n",
    "<br><br>\n",
    "The stand-alone decoder now takes as input:\n",
    "- Either the '\\<sos\\>' token (at the beginning) or the output from the previous timestep. Either way, it'll now take only one token at a time.\n",
    "- Hidden and cell states, either from the encoder initially or from the decoder's last time step.\n",
    "<br>\n",
    "\n",
    "And it outputs a probability distribution for the current output, and new hidden and cell states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewbqiTU9HV5W"
   },
   "outputs": [],
   "source": [
    "decoder_inputs = model.get_layer('decoder_inputs').input\n",
    "\n",
    "decoder_embedding_layer = model.get_layer('decoder_embeddings')\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "# Inputs to represent the decoder's LSTM hidden and cell states. We'll populate \n",
    "# these manually using the encoder's output for the initial state.\n",
    "decoder_input_state_h = tf.keras.Input(shape=(hidden_dim,), name='decoder_input_state_h')\n",
    "decoder_input_state_c = tf.keras.Input(shape=(hidden_dim,), name='decoder_input_state_c')\n",
    "decoder_input_states = [decoder_input_state_h, decoder_input_state_c]\n",
    "\n",
    "decoder_lstm = model.get_layer('decoder_lstm')\n",
    "\n",
    "decoder_sequence_outputs, decoder_output_state_h, decoder_output_state_c = decoder_lstm(\n",
    "    decoder_embeddings, initial_state=decoder_input_states\n",
    ")\n",
    "\n",
    "# Update hidden and cell states for the next time step.\n",
    "decoder_output_states = [decoder_output_state_h, decoder_output_state_c]\n",
    "\n",
    "decoder_dense = model.get_layer('decoder_dense')\n",
    "y_proba = decoder_dense(decoder_sequence_outputs)\n",
    "\n",
    "decoder_model_no_attention = tf.keras.Model(\n",
    "    [decoder_inputs] + decoder_input_states, \n",
    "    [y_proba] + decoder_output_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOd62g9GHKSL"
   },
   "outputs": [],
   "source": [
    "plot_model(decoder_model_no_attention, to_file='decoder_model_no_attention_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A56MoHNuCrVj"
   },
   "source": [
    "The method below translates a sentence from the source language to the target language. It encodes the source sentence as usual, then feeds the encoder's state outputs and the \\<sos\\> token into the decoder.\n",
    "<br><br>\n",
    "The decoder's outputs (the resulting word and its hidden/cell states) are then fed back to the decoder at the next time step. This continues until either a max word limit is reached or an \\<eos\\> token is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIxop6k943xw"
   },
   "outputs": [],
   "source": [
    "def translate_without_attention(sentence: str, \n",
    "                                source_tokenizer, encoder,\n",
    "                                target_tokenizer, decoder,\n",
    "                                max_translated_len = 30):\n",
    "\n",
    "  # Vectorize the source sentence and run it through the encoder.    \n",
    "  input_seq = source_tokenizer.texts_to_sequences([sentence])\n",
    "\n",
    "  # Get the tokenized sentence to see if there are any unknown tokens.\n",
    "  tokenized_sentence = source_tokenizer.sequences_to_texts(input_seq)\n",
    "\n",
    "  states = encoder.predict(input_seq)  \n",
    "\n",
    "  current_word = '<sos>'\n",
    "  decoded_sentence = []\n",
    "\n",
    "  while len(decoded_sentence) < max_translated_len:\n",
    "    \n",
    "    # Set the next input word for the decoder.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = target_tokenizer.word_index[current_word]\n",
    "    \n",
    "    # Determine the next word.\n",
    "    target_y_proba, h, c = decoder.predict([target_seq] + states)\n",
    "    target_token_index = np.argmax(target_y_proba[0, -1, :])\n",
    "    current_word = target_tokenizer.index_word[target_token_index]\n",
    "\n",
    "    if (current_word == '<eos>'):\n",
    "      break\n",
    "\n",
    "    decoded_sentence.append(current_word)\n",
    "    states = [h, c]\n",
    "  \n",
    "  return tokenized_sentence[0], ' '.join(decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3x_0AzAEkzV"
   },
   "source": [
    "To test it out, we'll sample a bunch of sentences from the *test* dataset and translate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NfACOuj43_0"
   },
   "outputs": [],
   "source": [
    "# random.seed is just here to re-create results.\n",
    "random.seed(1)\n",
    "sentences = random.sample(test, 15)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qv0unQFOLTY"
   },
   "outputs": [],
   "source": [
    "def translate_sentences(sentences, translation_func, source_tokenizer, encoder,\n",
    "                        target_tokenizer, decoder):\n",
    "  translations = {'Tokenized Original': [], 'Reference': [], 'Translation': []}\n",
    "\n",
    "  for s in sentences:\n",
    "    source, target = s.split(SEPARATOR)\n",
    "    source = preprocess_sentence(source)\n",
    "    tokenized_sentence, translated = translation_func(source, source_tokenizer, encoder,\n",
    "                                                      target_tokenizer, decoder)\n",
    "\n",
    "    translations['Tokenized Original'].append(tokenized_sentence)\n",
    "    translations['Reference'].append(target)\n",
    "    translations['Translation'].append(translated)\n",
    "  \n",
    "  return translations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEW5jxQZatjo"
   },
   "source": [
    "We'll load the results into a Pandas **DataFrame** for easier viewing.<br>\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hi7YJuG7NDv3"
   },
   "outputs": [],
   "source": [
    "translations_no_attention = pd.DataFrame(translate_sentences(sentences, translate_without_attention,\n",
    "                                                             source_tokenizer, encoder_model_no_attention,\n",
    "                                                             target_tokenizer, decoder_model_no_attention))\n",
    "translations_no_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdW6h9ndUdnq"
   },
   "source": [
    "So performance here tends to be mixed with a few translations which are off, a few which match the reference exactly, and a few interesting ones where the translation doesn't match the reference but conveys the same idea. But overall, pretty good.\n",
    "<br><br>\n",
    "There are several limitations we're working with:\n",
    "- Hungarian is a difficult language because of its flexible structure and high expressiveness. The conjugation style leads to several English words often mapping to one word in Hungarian. Even Google Translate tends to have a high error rate with Hungarian.\n",
    "- This is a small dataset and to build something robust requires much more data. It also isn't very wide and diverse.\n",
    "- We're using a simple model with one LSTM for the encoder, and one LSTM for the decoder. In contrast, when Google Translate was still fully recurrence-based, it used an eight-layer LSTM for the encoder, and an eight-layer LSTM for the decoder (though in this case, I still think the data is the primary limitation).\n",
    "- Translation remains a tricky and hard problem. For examples of issues that arise in translation, check out the video from Rasa in the *References* section at the end of the notebook.\n",
    "<br><br>\n",
    "Still, we managed to get some decent and interesting results with a simple approach. Next, let's learn how to add an attention layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fLKFfuYHiKo"
   },
   "source": [
    "# Recurrence-based Seq2Seq Neural Machine Translation **WITH** Luong Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVewBNtmWiET"
   },
   "source": [
    "All the preprocessing steps and tokenizers we already have can carry over here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-Sx3_U-Fcwn"
   },
   "source": [
    "This time around, we'll decouple the encoder and decoder from the beginning (i.e. won't have to build separate models for training and inference). And rather than using the **Functional API**, we'll use **subclassing** instead.<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/Model<br>\n",
    "https://www.tensorflow.org/guide/keras/custom_layers_and_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_m8XDQaYwrT"
   },
   "source": [
    "In the constructor, we declare our layers. The encoder flow remains much the same. The big difference with this encoder's LSTM layer is that it has *return_sequences* set to *True*. This means the encoder will now output a hidden state at each time step which will be used by the attention mechanism. And there is no masking because masks don't flow through custom layers (such as the attention mechanism), so we'll handle the masking ourselves in a custom loss function.\n",
    "<br><br>\n",
    "The overriden *call* method contains the operations of our model. i.e. it describes how the model should turns inputs into outputs.<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/Model#call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4EItNLFWcJR"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # No masking here. We'll handle it ourselves.\n",
    "        self.embedding = layers.Embedding(source_vocab_size, \n",
    "                                          embedding_dim,\n",
    "                                          name='encoder_embedding_layer')\n",
    "        \n",
    "        # return_sequences is set to True this time.\n",
    "        self.lstm = layers.LSTM(hidden_dim, \n",
    "                                return_sequences=True, \n",
    "                                return_state=True,\n",
    "                                name='encoder_lstm')\n",
    "\n",
    "    def call(self, input):\n",
    "        embeddings = self.embedding(input)\n",
    "        \n",
    "        # output_seq will hold the encoder's hidden states from each time step.\n",
    "        output_seq, state_h, state_c = self.lstm(embeddings)\n",
    "\n",
    "        return output_seq, state_h, state_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7P2-2pcbyDw"
   },
   "source": [
    "We can get a sense of the encoder's outputs by instantiating a dummy encoder and passing it some input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2ZuJoief7y7"
   },
   "outputs": [],
   "source": [
    "test_encoder = Encoder(source_vocab_size, embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NCsHuXk1Co_"
   },
   "source": [
    "The encoder will receive a batch of sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0e94USvf7uY"
   },
   "outputs": [],
   "source": [
    "test_encoder_batch = padded_train_encoder_inputs[:3]\n",
    "print(test_encoder_batch.shape)\n",
    "test_encoder_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhDRVE5A0ZsL"
   },
   "source": [
    "The encoder's overriden *call* method is invoked by *\\_\\_call\\_\\_* (dunder method), which is why we're executing the encoder instance as if it's a function.<br>\n",
    "https://stackoverflow.com/questions/9663562/what-is-the-difference-between-init-and-call<br>\n",
    "https://docs.python.org/3/reference/datamodel.html#object.__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyKGEvZwf7pN"
   },
   "outputs": [],
   "source": [
    "test_encoder_outputs, state_h, state_c = test_encoder(test_encoder_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ircW0Dr7f7jF"
   },
   "outputs": [],
   "source": [
    "print(test_encoder_outputs.shape)\n",
    "print(state_h.shape)\n",
    "print(state_c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MphLH6MGIYu"
   },
   "source": [
    "Before we create the attention class, let's walk through a simple example of multiplicative (dot product) attention. The one we'll eventually create will be almost identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaJhzHd3cVCV"
   },
   "source": [
    "Suppose the following is the sequence of hidden states from the encoder after processing a single four-token sentence. Each hidden state is of dimension three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlhkOO9VCVwm"
   },
   "outputs": [],
   "source": [
    "# Sample encoder LSTM output for single sequence of length 4.\n",
    "encoder_out = tf.constant([[1., 2., 3.],\n",
    "                           [2., 3., 4.],\n",
    "                           [3., 4., 5.],\n",
    "                           [4., 5. ,6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXOySklECVwn"
   },
   "outputs": [],
   "source": [
    "print('encoder_out shape: {}'.format(encoder_out.shape))\n",
    "print('Number of timesteps: {}'.format(encoder_out.shape[0]))\n",
    "print('Number of hidden dimensions: {}'.format(encoder_out.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cX4VCRZdcmcu"
   },
   "source": [
    "And suppose the following is the hidden state from the decoder at a particular time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vu_uNTT3CVwn"
   },
   "outputs": [],
   "source": [
    "# Sample decoder LSTM output for a single timestep.\n",
    "decoder_out = tf.constant([[1., 3., 5.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4pB-6iYCVwo"
   },
   "outputs": [],
   "source": [
    "print('decoder_out shape: {}'.format(decoder_out.shape))\n",
    "print('Number of timesteps: {}'.format(decoder_out.shape[0]))\n",
    "print('Number of hidden dimensions: {}'.format(decoder_out.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrR3i4x5dp6g"
   },
   "source": [
    "In order to get the attention scores, we need to perform a dot product between the decoder hidden state and the encoder hidden states. To do that, the encoder hidden states need to be transposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDhkjNqMCVwo"
   },
   "outputs": [],
   "source": [
    "tf.transpose(encoder_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kaxvmd9nd9ZW"
   },
   "source": [
    "The *tf.matmul* function can perform the transpose and the dot product in one step to yield the attention scores.<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/linalg/matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PD976tUBCVwo"
   },
   "outputs": [],
   "source": [
    "attention_scores = tf.matmul(decoder_out, encoder_out, transpose_b=True)\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObcBIy2TePKA"
   },
   "source": [
    "We can then apply a softmax to the attention scores to get the attention weights.<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "872rlQQ2IPxF"
   },
   "outputs": [],
   "source": [
    "attention_weights = tf.keras.activations.softmax(attention_scores, axis=-1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ov8sQe8_ecM6"
   },
   "source": [
    "Finally, we can create the context vector using the attention weights and the encoder outputs. Once we have our context vector we can use it in our decoder. Refer to this module's video/slides for a refresher if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuErfLozCVwp"
   },
   "outputs": [],
   "source": [
    "context = tf.matmul(attention_weights, encoder_out)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJ-qLZoIoS52"
   },
   "source": [
    "The attention class below follows the same steps. The only difference is the encoder outputs go through a dense layer first.\n",
    "<br><br>\n",
    "For input, the attention class takes in all the encoder hidden states and the current decoder hidden state. For output, it returns the attention weights (which could be useful for other purposes) and the context vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrEh7sCSGfcO"
   },
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.Model):\n",
    "  def __init__(self, hidden_dim):\n",
    "    super(LuongAttention, self).__init__()\n",
    "\n",
    "    self.w = layers.Dense(hidden_dim, name='encoder_outputs_dense')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    encoder_output_seq, decoder_output = inputs\n",
    "    z = self.w(encoder_output_seq)\n",
    "    attention_scores = tf.matmul(decoder_output, z, transpose_b=True)\n",
    "    attention_weights = tf.keras.activations.softmax(attention_scores, axis=-1)\n",
    "    context = tf.matmul(attention_weights, encoder_output_seq)\n",
    "\n",
    "    return attention_weights, context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EFPHR55TvWM"
   },
   "source": [
    "The decoder remains mostly the same except:\n",
    "1. The embedding layer doesn't have masking.\n",
    "2. There's an additional attention step in the flow.\n",
    "3. The combined context and decoder output vector goes through a dense layer *w*. Refer to this module's video/slides if a refresher is needed.\n",
    "4. The final dense layer has no softmax activation. Rather, we'll calculate the loss directly on the logits.\n",
    "<br>\n",
    "\n",
    "For inputs, the decoder receives:\n",
    "1. the token(s) for the current step. During training with teacher forcing, that would be the next expected token(s). During inference, that would be the token(s) generated from the last time step.\n",
    "2. all the encoder's hidden states.\n",
    "3. whatever the decoder should take as its current hidden and cell states.\n",
    "<br>\n",
    "\n",
    "For outputs, the decoder returns the:\n",
    "1. logits.\n",
    "2. last LSTM hidden and cell states.\n",
    "3. attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTyggGkiGfcP"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.embedding_layer = layers.Embedding(vocab_size,\n",
    "                                            embedding_dim,\n",
    "                                            name='decoder_embedding_layer')\n",
    "    \n",
    "    self.lstm = layers.LSTM(hidden_dim,\n",
    "                            return_sequences=True,\n",
    "                            return_state=True,\n",
    "                            name='decoder_lstm')\n",
    "\n",
    "    self.attention = LuongAttention(hidden_dim)\n",
    "\n",
    "    self.w = tf.keras.layers.Dense(hidden_dim, activation='tanh', name='attended_outputs_dense')\n",
    "    \n",
    "    self.dense = layers.Dense(vocab_size, name='decoder_dense')\n",
    "\n",
    "\n",
    "  def call(self, inputs):\n",
    "    decoder_input, encoder_output_seq, lstm_state = inputs\n",
    "    embeddings = self.embedding_layer(decoder_input)\n",
    "\n",
    "    decoder_output, state_h, state_c = self.lstm(embeddings, initial_state=lstm_state)\n",
    "   \n",
    "    weights, context = self.attention([encoder_output_seq, decoder_output])\n",
    "\n",
    "    decoder_output_with_attention = self.w(tf.concat(\n",
    "        [tf.squeeze(context, 1), tf.squeeze(decoder_output, 1)], -1))\n",
    "\n",
    "    logits = self.dense(decoder_output_with_attention)\n",
    "\n",
    "    return logits, state_h, state_c, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZVLZ2-7Jc_z"
   },
   "source": [
    "We can get a sense of the decoder's inputs and outputs for a single training time step with a test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKq5hd1uqtys"
   },
   "outputs": [],
   "source": [
    "test_decoder = Decoder(target_vocab_size, embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd1La69R08Bd"
   },
   "source": [
    "Suppose this is a batch of input sequences for the decoder (the sequences used for teacher forcing)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KKvm4q75bCH"
   },
   "outputs": [],
   "source": [
    "test_decoder_batch = padded_train_decoder_inputs[:3]\n",
    "print(test_decoder_batch.shape)\n",
    "test_decoder_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHjrOfuF1KYD"
   },
   "source": [
    "...and let's say we're currently on the second timestep (index 1).These would be the next inputs to the decoder for each sequence in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5i1_Mqr1Tat"
   },
   "outputs": [],
   "source": [
    "test_decoder_batch[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUearRVJ1Zol"
   },
   "source": [
    "But we need to modify this to be *three sequences of one element* which we can do with *expand_dims*.<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/expand_dims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJq_jIFtsLne"
   },
   "outputs": [],
   "source": [
    "next_decoder_inputs = tf.expand_dims(test_decoder_batch[:, 1], 1)\n",
    "next_decoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7C51h6hsxoD"
   },
   "outputs": [],
   "source": [
    "# Initial values for state_h and state_c are from the encoder.\n",
    "test_decoder_logits, state_h, state_c, test_decoder_weights = test_decoder(\n",
    "    [\n",
    "      next_decoder_inputs,\n",
    "      test_encoder_outputs,\n",
    "      [state_h, state_c]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUZJveextNNq"
   },
   "outputs": [],
   "source": [
    "print(test_decoder_logits.shape)\n",
    "print(test_decoder_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__J8yEd13Zsk"
   },
   "source": [
    "We're going to use a custom loss function because our sequences contain padding, and we don't want predictions on padding to contribute to the loss.\n",
    "<br><br>\n",
    "This custom loss function is simply a wrapper around a **sparse categorical crossentropy** loss, but with a mask of 1s and 0s. Any target element of 0 (i.e. a padding value) will get a mask value of 0, everything else will get a mask of 1, and only target values corresponding to a mask value of 1 will be used for loss calculation.\n",
    "<br><br>\n",
    "Refer to this page to get a sense of what the *targets* and *logits* parameters look like:<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3TxqKmoCYO8"
   },
   "outputs": [],
   "source": [
    "def loss_func(targets, logits):\n",
    "  ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "  mask = tf.cast(tf.math.not_equal(targets, 0), tf.float32)\n",
    "\n",
    "  return ce_loss(targets, logits, sample_weight=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_NmJkYf6vCi"
   },
   "source": [
    "We'll use Tensorflow's **Dataset** to batch our data like we did in the previous model on RNNs.<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/data/Dataset<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOrLxlgayH36"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((padded_train_encoder_inputs, \n",
    "                                              padded_train_decoder_inputs, \n",
    "                                              padded_train_decoder_targets)).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcl0QIsxi4BG"
   },
   "source": [
    "Finally, this is our training function. It'll be called for each batch of data.\n",
    "<br><br>\n",
    "*train_step* doesn't have to be in a class, but by putting it inside a *Model* subclass, we can leverage the *fit* method rather than writing our own training loop.\n",
    "<br><br>\n",
    "*train_step* is an overridden method called by the *fit* method.<br>\n",
    "https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit\n",
    "<br><br>\n",
    "Any operation within the _GradientTape_ context is recorded, and the variables involved are watched. We can then have Tensorflow's autodiff calculate the gradients of the variables with respect to the loss, and backpropagate it through the operations involved.\n",
    "https://www.tensorflow.org/api_docs/python/tf/GradientTape<br>\n",
    "https://www.tensorflow.org/guide/autodiff\n",
    "<br><br>\n",
    "*train_step* is wrapped in a *@tf.function* annotation for a bit of a performance boost.<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/function<br>\n",
    "https://www.tensorflow.org/guide/function<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8Ef_eWXjWMn"
   },
   "outputs": [],
   "source": [
    "class TranslatorTrainer(tf.keras.Model):\n",
    "  def __init__(self, encoder, decoder):\n",
    "    super(TranslatorTrainer, self).__init__()\n",
    "\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "\n",
    "  # This method will be called by model.fit for each batch.\n",
    "  @tf.function\n",
    "  def train_step(self, inputs):\n",
    "      loss = 0.\n",
    "\n",
    "      encoder_input_seq, decoder_input_seq, decoder_target_seq = inputs\n",
    "      \n",
    "      with tf.GradientTape() as tape:\n",
    "          encoder_output_seq, state_h, state_c = self.encoder(encoder_input_seq)\n",
    "  \n",
    "          # We need to create a loop to iterate through the target sequences\n",
    "          for i in range(decoder_target_seq.shape[1]):\n",
    "\n",
    "              # Input to the decoder must have shape of (batch_size, length)\n",
    "              # so we need to expand one dimension (just like in the previous example).\n",
    "              next_decoder_input = tf.expand_dims(decoder_input_seq[:, i], 1)\n",
    "              logits, state_h, state_c, _ = self.decoder(\n",
    "                  [next_decoder_input, encoder_output_seq, (state_h, state_c)])\n",
    "\n",
    "              # The loss is now accumulated through the whole batch\n",
    "              loss += self.loss(decoder_target_seq[:, i], logits)\n",
    "\n",
    "      # Update the parameters and the optimizer\n",
    "      variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "      gradients = tape.gradient(loss, variables)\n",
    "      self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "      return {'loss': loss / decoder_target_seq.shape[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNm8-EeI7LTi"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(source_vocab_size, embedding_dim, hidden_dim)\n",
    "decoder = Decoder(target_vocab_size, embedding_dim, hidden_dim)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "translator_trainer = TranslatorTrainer(encoder, decoder)\n",
    "translator_trainer.compile(optimizer=optimizer, loss=loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqH3IU1bIiaI"
   },
   "source": [
    "We now have everything to train our encoder and decoder.\n",
    "<br><br>\n",
    "The following *fit* call is commented out because the encoder and decoder were trained prior to save time. If you want to train them yourself, feel free to uncomment and execute the method.<br><br>\n",
    "**Note**: Because of the random weight initialization, your trained model's output will likely differ from mine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BkIaWicIH4wl"
   },
   "outputs": [],
   "source": [
    "epochs = 12\n",
    "# translator_trainer.fit(dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4IUBkFZbfaE"
   },
   "source": [
    "Functions to save, zip, and download the weights of the trained encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tYGhcB2zAAN"
   },
   "outputs": [],
   "source": [
    "# encoder.save_weights('attention_encoder_weights_with_dropout_ckpt')\n",
    "# decoder.save_weights('attention_decoder_weights_with_dropout_ckpt')\n",
    "\n",
    "# !zip -r ./attention_weights.zip ./attention_weights\n",
    "\n",
    "# files.download('./attention_weights.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQpwVj8DGYSe"
   },
   "source": [
    "We'll download and load the previously trained attention-based encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1vgMTwHe0z0"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/nitinpunjabi/nlp-demystified/raw/main/models/nmt_with_attention/attention_weights.zip\n",
    "!unzip -o attention_weights.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7jv1O45e0eS"
   },
   "outputs": [],
   "source": [
    "encoder.load_weights('attention_weights/attention_encoder_weights_ckpt')\n",
    "decoder.load_weights('attention_weights/attention_decoder_weights_ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gri3tSndJLJ"
   },
   "source": [
    "The *translate_with_attention* method is similar to the method without attention, except it involves padding since the LSTMs didn't use masking, and also using the encoder's hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlSSNZzItoTK"
   },
   "outputs": [],
   "source": [
    "def translate_with_attention(sentence: str, \n",
    "                             source_tokenizer, encoder,\n",
    "                             target_tokenizer, decoder,\n",
    "                             max_translated_len = 30):\n",
    "    input_seq = source_tokenizer.texts_to_sequences([sentence])\n",
    "    tokenized = source_tokenizer.sequences_to_texts(input_seq)\n",
    "\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_encoding_len, padding='post')\n",
    "    encoder_output, state_h, state_c  = encoder.predict(input_seq)\n",
    "\n",
    "    current_word = '<sos>'\n",
    "    decoded_sentence = []\n",
    "\n",
    "    while len(decoded_sentence) < max_translated_len:\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = target_tokenizer.word_index[current_word]\n",
    "\n",
    "        logits, state_h, state_c, _ = decoder.predict([target_seq, encoder_output, (state_h, state_c)])\n",
    "        current_token_index = np.argmax(logits[0])\n",
    "\n",
    "        current_word = target_tokenizer.index_word[current_token_index]\n",
    "\n",
    "        if (current_word == '<eos>'):\n",
    "          break\n",
    "\n",
    "        decoded_sentence.append(current_word)\n",
    "\n",
    "    return tokenized[0], ' '.join(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQmveQcudsfH"
   },
   "source": [
    "We'll translate the same randomly sampled sentences from before and compare the results against the translations without attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRNTbJTWF7Os"
   },
   "outputs": [],
   "source": [
    "shorter_translations_w_attention = pd.DataFrame(translate_sentences(sentences, translate_with_attention,\n",
    "                                                                    source_tokenizer, encoder,\n",
    "                                                                    target_tokenizer, decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wA4-w-f0I7Hi"
   },
   "outputs": [],
   "source": [
    "shorter_translations_w_attention.rename(columns={'Translation': 'Translation W/ Attention'}, inplace=True)\n",
    "shorter_translations_w_attention['Translation W/O Attention'] = translations_no_attention['Translation']\n",
    "shorter_translations_w_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmSA22oMd2G2"
   },
   "source": [
    "In addition, let's take a look at results of translating the longest sentences in the training set to see how good they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNYUJXkCHnJL"
   },
   "outputs": [],
   "source": [
    "pairs = train.copy()\n",
    "pairs.sort(key=lambda s: len(s))\n",
    "longer_sentences = pairs[-10:]\n",
    "longer_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJR2kC7YJ3-9"
   },
   "outputs": [],
   "source": [
    "longer_translations_wo_attention = translate_sentences(longer_sentences, translate_without_attention, \n",
    "                                                       source_tokenizer, encoder_model_no_attention,\n",
    "                                                       target_tokenizer, decoder_model_no_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJhewvH5HmvW"
   },
   "outputs": [],
   "source": [
    "longer_translations_with_attention = pd.DataFrame(translate_sentences(longer_sentences, translate_with_attention,\n",
    "                                                                      source_tokenizer, encoder,\n",
    "                                                                      target_tokenizer, decoder))\n",
    "longer_translations_with_attention.rename(columns={'Translation': 'Translation W/ Attention'}, inplace=True)\n",
    "longer_translations_with_attention['Translation W/O Attention'] = longer_translations_wo_attention['Translation']\n",
    "longer_translations_with_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTnDWYbReE7y"
   },
   "source": [
    "Ok, so clearly the encoder/decoder with attention gets further along before losing signal and it often manages to translate words later in the sentence.<br><br>\n",
    "But still, there are clear limitiations. For one thing, we need *much* more data. For another, most industrial translation systems use more complex models. For example, Google's NMT system (when it was based entirely on LSTMs) used eight layers for both the encoder and decoder.<br><br>\n",
    "But beyond that, we can use a more powerful model which we'll explore in the next module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppjikZ88pzt4"
   },
   "source": [
    "# Further Exploration\n",
    "1. Download another translation collection from [Tatoeba](https://tatoeba.org/en/downloads) and try using a more complex model (e.g. with more layers) to see if you get better performance.\n",
    "<br><br>\n",
    "2. Seq2Seq is a general problem approach. Try turning a non-language sequence into another sequence. For example, turn simple arithmetic problems (e.g. 2 + 1 * 3) into its respective answer. Generating your own dataset for such a scenario is easy.\n",
    "<br><br>\n",
    "3. [BLEU](https://en.wikipedia.org/wiki/BLEU) is a popular metric for evaluating translation quality. Now that you know how BLEU works (see video/slides) and have a bunch of machine-generated translations along with their human-provided references, try calculating BLEU for the results.\n",
    "<br><br>\n",
    "4. Implement Beam Search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MdGADaZK2Dd"
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MF0jaZAdX_z"
   },
   "source": [
    "Challenges in machine tanslation from RASA:\n",
    "https://www.youtube.com/watch?v=94K4LU_Pe8Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTN-sObh7vlr"
   },
   "source": [
    "Sequence to Sequence Learning with Neural Networks\n",
    "\n",
    "https://arxiv.org/abs/1409.3215\n",
    "\n",
    "\n",
    "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\n",
    "https://arxiv.org/abs/1406.1078\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeN6jB6R7BKs"
   },
   "source": [
    "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\n",
    "- 8 encoder and decoder layers\n",
    "- Uses attention mechanism\n",
    "\n",
    " https://arxiv.org/abs/1609.08144\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Neural Machine Translation by Jointly Learning to Align and Translate\n",
    " https://arxiv.org/abs/1409.0473\n",
    "\n",
    "\n",
    "Effective Approaches to Attention-based Neural Machine Translation\n",
    "https://arxiv.org/abs/1508.04025\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}