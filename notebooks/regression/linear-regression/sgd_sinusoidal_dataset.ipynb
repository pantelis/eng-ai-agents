{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# rye add --git https://github.com/ctgk/PRML.git prml\n",
    "\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# Apply the default theme\n",
    "sns.set_theme()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import prml\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from prml.preprocess import GaussianFeature, PolynomialFeature, SigmoidalFeature\n",
    "from prml.linear import (\n",
    "    BayesianRegression,\n",
    "    EmpiricalBayesRegression,\n",
    "    LinearRegression,\n",
    "    RidgeRegression,\n",
    ")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def create_toy_data(func, sample_size, std, domain=[0, 1]):\n",
    "    rng = np.random.default_rng()\n",
    "    x = np.linspace(domain[0], domain[1], sample_size)\n",
    "    # x = rng.uniform(0, 1, sample_size)\n",
    "    np.random.shuffle(x)\n",
    "\n",
    "    y = func(x) + rng.normal(scale=std, size=x.shape)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def sinusoidal(x):\n",
    "    return np.sin(2 * np.pi * x)\n",
    "\n",
    "\n",
    "m = 20\n",
    "x, y = create_toy_data(sinusoidal, m, 0.25)\n",
    "\n",
    "# Reshape x to work with sklearn (needed if x is a 1D array)\n",
    "x = x.reshape(-1, 1)\n",
    "\n",
    "# Split dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Print the shapes of the splits\n",
    "print(\"Training set size:\", x_train.shape, y_train.shape)\n",
    "print(\"Test set size:\", x_test.shape, y_test.shape)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "plt.figure(figsize=[10, 8])\n",
    "plt.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")\n",
    "plt.scatter(x_test, y_test, facecolor=\"none\", edgecolor=\"r\", label=\"test data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "M = 9\n",
    "\n",
    "feature = PolynomialFeature(M)\n",
    "\n",
    "X_train = feature.transform(x_train)\n",
    "X_test = feature.transform(x_test)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=[10, 8])\n",
    "plt.plot(model.w)\n",
    "plt.xlabel(\"index of $w$\")\n",
    "plt.ylabel(\"$w$\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=[10, 8])\n",
    "\n",
    "# training data\n",
    "plt.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")\n",
    "# test data\n",
    "plt.scatter(x_test, y_test, facecolor=\"none\", edgecolor=\"r\", s=50, label=\"test data\")\n",
    "\n",
    "# M=9 polynomial regression hypothesis\n",
    "x_range = np.linspace(X_train.min(), X_train.max(), 100).reshape(-1, 1)\n",
    "X_range = feature.transform(x_range)\n",
    "y_hat_range, y_hat_range_std = model.predict(X_range, return_std=True)\n",
    "\n",
    "plt.plot(x_range, y_hat_range, label=\"Least Squares Prediction\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$ or $\\hat{y}$\")\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import test\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# SGD Loop with polynomial features and mini-batch support\n",
    "def sgd_loop(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    w,\n",
    "    learning_rate,\n",
    "    epochs,\n",
    "    lambda_reg,\n",
    "    n_samples,\n",
    "    batch_size=1,\n",
    "):\n",
    "    # Arrays to store loss values for visualization\n",
    "    losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    # SGD Loop\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, n_samples, batch_size):  # Iterate in mini-batches\n",
    "            # Select a mini-batch of `batch_size`\n",
    "            batch_indices = np.random.choice(n_samples, batch_size, replace=False)\n",
    "            xi = X_train[batch_indices]\n",
    "            yi = y_train[batch_indices]\n",
    "\n",
    "            # Prediction\n",
    "            y_pred = np.dot(xi, w)\n",
    "\n",
    "            # Compute error\n",
    "            error = y_pred - yi\n",
    "\n",
    "            # Compute gradients (Mean of batch gradients)\n",
    "            dw = (2 / batch_size) * (xi.T @ error).flatten() + 2 * lambda_reg * w\n",
    "\n",
    "            # Update weights\n",
    "            w -= learning_rate * dw\n",
    "\n",
    "        # Compute training loss\n",
    "        y_hat_train = X_train @ w\n",
    "        loss = np.mean((y_hat_train - y_train) ** 2) + lambda_reg * np.sum(w**2)\n",
    "        losses.append(loss)\n",
    "\n",
    "        # Compute test loss\n",
    "        y_hat_test = X_test @ w\n",
    "        test_loss = np.mean((y_hat_test - y_test) ** 2) + lambda_reg * np.sum(w**2)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "    # Plot loss vs epochs\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(epochs), losses, label=\"Training Loss\", color=\"blue\")\n",
    "    plt.plot(range(epochs), test_losses, label=\"Test Loss\", color=\"red\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training & Test Loss vs Epochs\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Start of plotting regression curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    x_range = np.linspace(X_train.min(), X_train.max(), 100).reshape(-1, 1)\n",
    "\n",
    "    # computation\n",
    "    hypothesis = np.zeros_like(x_range)\n",
    "    for i in range(len(w)):\n",
    "        hypothesis += w[i] * (x_range**i)  # Add each term: w_i * x^i\n",
    "\n",
    "    # Plot training and testing data points\n",
    "    plt.scatter(x_train, y_train, color=\"blue\", label=\"Training Data\")\n",
    "    plt.scatter(x_test, y_test, color=\"red\", label=\"Test Data\")\n",
    "\n",
    "    # plot our regression curve\n",
    "    plt.plot(x_range, hypothesis, color=\"green\", label=\"Regularized Hypothesis\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(\"Regularized 9-degree Polynomial Regression Function\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    min_loss = min(losses)\n",
    "    min_index = losses.index(min_loss)\n",
    "    min_test_loss = min(test_losses)\n",
    "    min_test_index = test_losses.index(min_test_loss)\n",
    "\n",
    "    # Print final parameters\n",
    "    print(\"Final weights:\", w.flatten())\n",
    "    print(f\"Smallest Loss: {min_loss} loss at index {min_index}\")\n",
    "    print(f\"Smallest Test Loss: {min_test_loss} loss at index {min_test_index}\")\n",
    "\n",
    "    return min(test_losses)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "n_features = X_train.shape[1]  # Number of polynomial features\n",
    "\n",
    "# Replace this vector with the optimal trial vector\n",
    "w = np.array(\n",
    "    [\n",
    "        0.3821733,\n",
    "        0.3096256,\n",
    "        -1.87535451,\n",
    "        -0.41147161,\n",
    "        0.48204058,\n",
    "        -0.47616227,\n",
    "        -0.91596271,\n",
    "        0.33230879,\n",
    "        0.92623104,\n",
    "        1.34749729,\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_loss = sgd_loop(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    w=w,\n",
    "    learning_rate=0.01,\n",
    "    epochs=10000,\n",
    "    lambda_reg=0.02,\n",
    "    n_samples=len(y_train),\n",
    "    batch_size=5,\n",
    ")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Sample lambda_reg from Optuna (log-uniform for better scaling)\n",
    "    lambda_reg = trial.suggest_loguniform(\"lambda_reg\", 1e-4, 1.0)  # Define the search space\n",
    "\n",
    "    # batch_size = trial.suggest_uniform(\n",
    "    #     \"batch_size\", 1, 10\n",
    "    # )  # Define the search space\n",
    "\n",
    "    # Initialize weights\n",
    "    w = np.random.randn(10)\n",
    "\n",
    "    # fix batch size to the size of the training set\n",
    "    batch_size = len(y_train)\n",
    "\n",
    "    # Run SGD\n",
    "    min_test_loss = sgd_loop(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        w=w,\n",
    "        learning_rate=0.01,\n",
    "        epochs=10000,\n",
    "        lambda_reg=lambda_reg,\n",
    "        n_samples=len(y_train),\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    return min_test_loss\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set direction to \"minimize\" as we want to minimize the objective\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "n_trials = 50\n",
    "\n",
    "study.optimize(objective, n_trials=50)  # Run the optimization for 100 trials\n",
    "\n",
    "# Best lambda_reg\n",
    "best_lambda_reg = study.best_params[\"lambda_reg\"]\n",
    "print(f\"Best lambda_reg: {best_lambda_reg}\")\n",
    "print(f\"Best test loss: {study.best_value}\")\n",
    "\n",
    "# Best Batch size\n",
    "best_lambda_reg = study.best_params[\"lambda_reg\"]\n",
    "print(f\"Best lambda_reg: {best_lambda_reg}\")\n",
    "print(f\"Best test loss: {study.best_value}\")\n",
    "\n",
    "# Plot optimization history\n",
    "optuna.visualization.matplotlib.plot_optimization_history(study)\n",
    "plt.show()\n"
   ],
   "outputs": []
  }
 ]
}