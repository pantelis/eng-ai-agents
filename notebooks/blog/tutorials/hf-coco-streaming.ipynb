{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b9f3266",
   "metadata": {
    "papermill": {
     "duration": 0.003599,
     "end_time": "2026-02-15T14:43:15.561201",
     "exception": false,
     "start_time": "2026-02-15T14:43:15.557602",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Stream COCO for object detection training with Hugging Face datasets\n",
    "\n",
    "The COCO dataset is a cornerstone benchmark for object detection, but at ~20 GB it takes\n",
    "significant time and disk space to download. **Hugging Face datasets streaming** lets you\n",
    "train on COCO without downloading the full dataset — images are fetched on-the-fly as your\n",
    "training loop requests them.\n",
    "\n",
    "In this tutorial you will:\n",
    "\n",
    "1. Stream COCO from the [`detection-datasets/coco`](https://huggingface.co/datasets/detection-datasets/coco) repository\n",
    "2. Build a PyTorch `DataLoader` that works with the streaming `IterableDataset`\n",
    "3. Fine-tune a Faster R-CNN model for 100 training steps\n",
    "4. Run inference and visualize predictions with bounding box overlays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Colab Environment Setup ---\n",
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    %pip install -q matplotlib datasets torch torchvision\n",
    "    print(\"Colab dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e3d90",
   "metadata": {
    "papermill": {
     "duration": 0.003172,
     "end_time": "2026-02-15T14:43:15.567500",
     "exception": false,
     "start_time": "2026-02-15T14:43:15.564328",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "Install the required packages (if not already present):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c1cdde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T14:43:15.575684Z",
     "iopub.status.busy": "2026-02-15T14:43:15.575416Z",
     "iopub.status.idle": "2026-02-15T14:43:16.504626Z",
     "shell.execute_reply": "2026-02-15T14:43:16.502893Z"
    },
    "papermill": {
     "duration": 0.935289,
     "end_time": "2026-02-15T14:43:16.505945",
     "exception": false,
     "start_time": "2026-02-15T14:43:15.570656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -q datasets torch torchvision matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2b0926",
   "metadata": {
    "papermill": {
     "duration": 0.002682,
     "end_time": "2026-02-15T14:43:16.512791",
     "exception": false,
     "start_time": "2026-02-15T14:43:16.510109",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load COCO with streaming\n",
    "\n",
    "With `streaming=True`, no data is downloaded upfront. The dataset returns an\n",
    "`IterableDataset` that fetches examples on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c3c0cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T14:43:16.517354Z",
     "iopub.status.busy": "2026-02-15T14:43:16.517206Z",
     "iopub.status.idle": "2026-02-15T14:43:21.064891Z",
     "shell.execute_reply": "2026-02-15T14:43:21.064116Z"
    },
    "papermill": {
     "duration": 4.550776,
     "end_time": "2026-02-15T14:43:21.065479",
     "exception": false,
     "start_time": "2026-02-15T14:43:16.514703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"detection-datasets/coco\", split=\"train\", streaming=True)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94446e7a",
   "metadata": {
    "papermill": {
     "duration": 0.002361,
     "end_time": "2026-02-15T14:43:21.070655",
     "exception": false,
     "start_time": "2026-02-15T14:43:21.068294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's peek at the schema by grabbing one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59c401d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T14:43:21.075308Z",
     "iopub.status.busy": "2026-02-15T14:43:21.075054Z",
     "iopub.status.idle": "2026-02-15T14:43:32.959119Z",
     "shell.execute_reply": "2026-02-15T14:43:32.958613Z"
    },
    "papermill": {
     "duration": 11.887193,
     "end_time": "2026-02-15T14:43:32.959744",
     "exception": false,
     "start_time": "2026-02-15T14:43:21.072551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = next(iter(ds))\n",
    "print(\"Keys:\", list(sample.keys()))\n",
    "print(\"Image size:\", sample[\"image\"].size)\n",
    "print(\"Number of objects:\", len(sample[\"objects\"][\"bbox\"]))\n",
    "print(\"First bbox (COCO xywh):\", sample[\"objects\"][\"bbox\"][0])\n",
    "print(\"Categories:\", sample[\"objects\"][\"category\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f540b1b",
   "metadata": {
    "papermill": {
     "duration": 0.001776,
     "end_time": "2026-02-15T14:43:32.963703",
     "exception": false,
     "start_time": "2026-02-15T14:43:32.961927",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocess for detection\n",
    "\n",
    "Faster R-CNN expects:\n",
    "- Images as `float32` tensors in `[0, 1]` range\n",
    "- Targets as a list of dicts with `boxes` (xyxy format) and `labels`\n",
    "\n",
    "COCO bounding boxes are in `[x, y, width, height]` format, so we convert to\n",
    "`[x1, y1, x2, y2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4488071",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T14:43:32.968017Z",
     "iopub.status.busy": "2026-02-15T14:43:32.967864Z",
     "iopub.status.idle": "2026-02-15T14:43:34.471192Z",
     "shell.execute_reply": "2026-02-15T14:43:34.470483Z"
    },
    "papermill": {
     "duration": 1.506765,
     "end_time": "2026-02-15T14:43:34.472132",
     "exception": false,
     "start_time": "2026-02-15T14:43:32.965367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": "import torch\nfrom torchvision.transforms import functional as tv_f\n\nRESIZE = 640\n\n\ndef transform_example(example):\n    \"\"\"Convert a single HF dataset example to Faster R-CNN format.\"\"\"\n    img = example[\"image\"].convert(\"RGB\")\n    orig_w, orig_h = img.size\n\n    # Resize image\n    img = tv_f.resize(img, [RESIZE, RESIZE])\n    img_tensor = tv_f.to_tensor(img)  # [C, H, W] in [0, 1]\n\n    # Scale factors for bbox adjustment\n    sx = RESIZE / orig_w\n    sy = RESIZE / orig_h\n\n    # Convert COCO xywh -> xyxy and scale\n    boxes = []\n    for bbox in example[\"objects\"][\"bbox\"]:\n        x, y, w, h = bbox\n        boxes.append([x * sx, y * sy, (x + w) * sx, (y + h) * sy])\n\n    # Category IDs (shift by +1 since 0 is background in torchvision)\n    labels = [cat + 1 for cat in example[\"objects\"][\"category\"]]\n\n    target = {\n        \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n        \"labels\": torch.tensor(labels, dtype=torch.int64),\n    }\n    return img_tensor, target"
  },
  {
   "cell_type": "markdown",
   "id": "f8dc49b7",
   "metadata": {
    "papermill": {
     "duration": 0.001851,
     "end_time": "2026-02-15T14:43:34.476297",
     "exception": false,
     "start_time": "2026-02-15T14:43:34.474446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Build a streaming DataLoader\n",
    "\n",
    "Since `IterableDataset` from HF `datasets` inherits from `torch.utils.data.IterableDataset`,\n",
    "we can pass it directly to a `DataLoader`. We use a custom collate function because detection\n",
    "targets have variable-length box lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866821ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T14:43:34.480834Z",
     "iopub.status.busy": "2026-02-15T14:43:34.480520Z",
     "iopub.status.idle": "2026-02-15T14:43:45.861035Z",
     "shell.execute_reply": "2026-02-15T14:43:45.860541Z"
    },
    "papermill": {
     "duration": 11.383664,
     "end_time": "2026-02-15T14:43:45.861632",
     "exception": false,
     "start_time": "2026-02-15T14:43:34.477968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class COCOStreamDataset(torch.utils.data.IterableDataset):\n",
    "    \"\"\"Wraps the HF streaming dataset with detection preprocessing.\"\"\"\n",
    "\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.hf_dataset = hf_dataset\n",
    "\n",
    "    def __iter__(self):\n",
    "        for example in self.hf_dataset:\n",
    "            img, target = transform_example(example)\n",
    "            # Skip examples with no boxes\n",
    "            if target[\"boxes\"].numel() > 0:\n",
    "                yield img, target\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate — detection targets are variable-length.\"\"\"\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    return images, targets\n",
    "\n",
    "\n",
    "stream_ds = COCOStreamDataset(ds.shuffle(seed=42, buffer_size=1000))\n",
    "dataloader = DataLoader(stream_ds, batch_size=4, collate_fn=collate_fn)\n",
    "\n",
    "# Quick sanity check\n",
    "images, targets = next(iter(dataloader))\n",
    "print(f\"Batch: {len(images)} images\")\n",
    "print(f\"First image shape: {images[0].shape}\")\n",
    "print(f\"First target boxes: {targets[0]['boxes'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02103a41",
   "metadata": {
    "papermill": {
     "duration": 0.001995,
     "end_time": "2026-02-15T14:43:45.865872",
     "exception": false,
     "start_time": "2026-02-15T14:43:45.863877",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Visualize a batch\n",
    "\n",
    "Let's draw bounding boxes on a batch of images to verify the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43791ee4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T14:43:45.870341Z",
     "iopub.status.busy": "2026-02-15T14:43:45.870204Z",
     "iopub.status.idle": "2026-02-15T14:43:46.667609Z",
     "shell.execute_reply": "2026-02-15T14:43:46.667032Z"
    },
    "papermill": {
     "duration": 0.806845,
     "end_time": "2026-02-15T14:43:46.674418",
     "exception": false,
     "start_time": "2026-02-15T14:43:45.867573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import patches\n",
    "\n",
    "\n",
    "def show_batch(images, targets, max_images=4):\n",
    "    \"\"\"Display images with bounding box overlays.\"\"\"\n",
    "    fig, axes = plt.subplots(1, min(len(images), max_images), figsize=(16, 5))\n",
    "    if not isinstance(axes, np.ndarray):\n",
    "        axes = [axes]\n",
    "\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, 12))\n",
    "\n",
    "    for ax, img, tgt in zip(axes, images, targets):\n",
    "        ax.imshow(img.permute(1, 2, 0).numpy())\n",
    "        for i, (box, label) in enumerate(zip(tgt[\"boxes\"], tgt[\"labels\"])):\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            color = colors[label.item() % len(colors)]\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1),\n",
    "                x2 - x1,\n",
    "                y2 - y1,\n",
    "                linewidth=2,\n",
    "                edgecolor=color,\n",
    "                facecolor=\"none\",\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "        ax.set_title(f\"{len(tgt['boxes'])} objects\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_batch(images, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b389e0a",
   "metadata": {
    "papermill": {
     "duration": 0.013171,
     "end_time": "2026-02-15T14:43:46.701837",
     "exception": false,
     "start_time": "2026-02-15T14:43:46.688666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train with Faster R-CNN\n",
    "\n",
    "We use `fasterrcnn_resnet50_fpn_v2` pretrained on COCO and fine-tune for 100 steps\n",
    "as a demonstration. In a real scenario you would train for many more steps and\n",
    "evaluate on the validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d3a1ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T14:43:46.729428Z",
     "iopub.status.busy": "2026-02-15T14:43:46.729101Z",
     "iopub.status.idle": "2026-02-15T14:45:32.206308Z",
     "shell.execute_reply": "2026-02-15T14:45:32.205791Z"
    },
    "papermill": {
     "duration": 105.504631,
     "end_time": "2026-02-15T14:45:32.220088",
     "exception": false,
     "start_time": "2026-02-15T14:43:46.715457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": "from torchvision.models.detection import (\n    FasterRCNN_ResNet50_FPN_V2_Weights,\n    fasterrcnn_resnet50_fpn_v2,\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nmodel = fasterrcnn_resnet50_fpn_v2(weights=FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT)\nmodel.to(device)\nmodel.train()\n\noptimizer = torch.optim.SGD(\n    model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005\n)\n\nNUM_STEPS = 100\nLOG_EVERY = 10\n\n# Rebuild the dataloader for training (fresh iterator)\ntrain_ds = COCOStreamDataset(\n    load_dataset(\"detection-datasets/coco\", split=\"train\", streaming=True).shuffle(\n        seed=42, buffer_size=1000\n    )\n)\ntrain_loader = DataLoader(train_ds, batch_size=4, collate_fn=collate_fn)\n\nrunning_loss = 0.0\nfor step, (imgs, tgts) in enumerate(train_loader):\n    if step >= NUM_STEPS:\n        break\n\n    imgs_dev = [img.to(device) for img in imgs]\n    tgts_dev = [{k: v.to(device) for k, v in t.items()} for t in tgts]\n\n    loss_dict = model(imgs_dev, tgts_dev)\n    losses = sum(loss for loss in loss_dict.values())\n\n    optimizer.zero_grad()\n    losses.backward()\n    optimizer.step()\n\n    running_loss += losses.item()\n    if (step + 1) % LOG_EVERY == 0:\n        avg_loss = running_loss / LOG_EVERY\n        print(f\"Step [{step + 1}/{NUM_STEPS}]  Loss: {avg_loss:.4f}\")\n        running_loss = 0.0\n\nprint(\"Training complete.\")"
  },
  {
   "cell_type": "markdown",
   "id": "a4152322",
   "metadata": {
    "papermill": {
     "duration": 0.0137,
     "end_time": "2026-02-15T14:45:32.247269",
     "exception": false,
     "start_time": "2026-02-15T14:45:32.233569",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run inference\n",
    "\n",
    "Switch to eval mode and visualize predictions on a few streamed images.\n",
    "We keep predictions with confidence > 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5abfbf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T14:45:32.275191Z",
     "iopub.status.busy": "2026-02-15T14:45:32.275021Z",
     "iopub.status.idle": "2026-02-15T14:45:32.281057Z",
     "shell.execute_reply": "2026-02-15T14:45:32.280537Z"
    },
    "papermill": {
     "duration": 0.021019,
     "end_time": "2026-02-15T14:45:32.281773",
     "exception": false,
     "start_time": "2026-02-15T14:45:32.260754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# COCO category names (91 categories, index 0 = background)\n",
    "COCO_NAMES = [\n",
    "    \"__background__\",\n",
    "    \"person\",\n",
    "    \"bicycle\",\n",
    "    \"car\",\n",
    "    \"motorcycle\",\n",
    "    \"airplane\",\n",
    "    \"bus\",\n",
    "    \"train\",\n",
    "    \"truck\",\n",
    "    \"boat\",\n",
    "    \"traffic light\",\n",
    "    \"fire hydrant\",\n",
    "    \"N/A\",\n",
    "    \"stop sign\",\n",
    "    \"parking meter\",\n",
    "    \"bench\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"sheep\",\n",
    "    \"cow\",\n",
    "    \"elephant\",\n",
    "    \"bear\",\n",
    "    \"zebra\",\n",
    "    \"giraffe\",\n",
    "    \"N/A\",\n",
    "    \"backpack\",\n",
    "    \"umbrella\",\n",
    "    \"N/A\",\n",
    "    \"N/A\",\n",
    "    \"handbag\",\n",
    "    \"tie\",\n",
    "    \"suitcase\",\n",
    "    \"frisbee\",\n",
    "    \"skis\",\n",
    "    \"snowboard\",\n",
    "    \"sports ball\",\n",
    "    \"kite\",\n",
    "    \"baseball bat\",\n",
    "    \"baseball glove\",\n",
    "    \"skateboard\",\n",
    "    \"surfboard\",\n",
    "    \"tennis racket\",\n",
    "    \"bottle\",\n",
    "    \"N/A\",\n",
    "    \"wine glass\",\n",
    "    \"cup\",\n",
    "    \"fork\",\n",
    "    \"knife\",\n",
    "    \"spoon\",\n",
    "    \"bowl\",\n",
    "    \"banana\",\n",
    "    \"apple\",\n",
    "    \"sandwich\",\n",
    "    \"orange\",\n",
    "    \"broccoli\",\n",
    "    \"carrot\",\n",
    "    \"hot dog\",\n",
    "    \"pizza\",\n",
    "    \"donut\",\n",
    "    \"cake\",\n",
    "    \"chair\",\n",
    "    \"couch\",\n",
    "    \"potted plant\",\n",
    "    \"bed\",\n",
    "    \"N/A\",\n",
    "    \"dining table\",\n",
    "    \"N/A\",\n",
    "    \"N/A\",\n",
    "    \"toilet\",\n",
    "    \"N/A\",\n",
    "    \"tv\",\n",
    "    \"laptop\",\n",
    "    \"mouse\",\n",
    "    \"remote\",\n",
    "    \"keyboard\",\n",
    "    \"cell phone\",\n",
    "    \"microwave\",\n",
    "    \"oven\",\n",
    "    \"toaster\",\n",
    "    \"sink\",\n",
    "    \"refrigerator\",\n",
    "    \"N/A\",\n",
    "    \"book\",\n",
    "    \"clock\",\n",
    "    \"vase\",\n",
    "    \"scissors\",\n",
    "    \"teddy bear\",\n",
    "    \"hair drier\",\n",
    "    \"toothbrush\",\n",
    "]\n",
    "\n",
    "\n",
    "def show_predictions(images, predictions, score_threshold=0.5, max_images=4):\n",
    "    \"\"\"Display images with predicted bounding boxes and labels.\"\"\"\n",
    "    fig, axes = plt.subplots(1, min(len(images), max_images), figsize=(16, 5))\n",
    "    if not isinstance(axes, np.ndarray):\n",
    "        axes = [axes]\n",
    "\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, 20))\n",
    "\n",
    "    for ax, img, pred in zip(axes, images, predictions):\n",
    "        ax.imshow(img.cpu().permute(1, 2, 0).numpy())\n",
    "        keep = pred[\"scores\"] > score_threshold\n",
    "        boxes = pred[\"boxes\"][keep]\n",
    "        labels = pred[\"labels\"][keep]\n",
    "        scores = pred[\"scores\"][keep]\n",
    "\n",
    "        for box, label, score in zip(boxes, labels, scores):\n",
    "            x1, y1, x2, y2 = box.cpu().tolist()\n",
    "            color = colors[label.item() % len(colors)]\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1),\n",
    "                x2 - x1,\n",
    "                y2 - y1,\n",
    "                linewidth=2,\n",
    "                edgecolor=color,\n",
    "                facecolor=\"none\",\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            name = (\n",
    "                COCO_NAMES[label.item()]\n",
    "                if label.item() < len(COCO_NAMES)\n",
    "                else str(label.item())\n",
    "            )\n",
    "            ax.text(\n",
    "                x1,\n",
    "                y1 - 5,\n",
    "                f\"{name} {score:.2f}\",\n",
    "                color=\"white\",\n",
    "                fontsize=8,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=color, alpha=0.8),\n",
    "            )\n",
    "        ax.set_title(f\"{len(boxes)} detections\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824a4676",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T14:45:32.311057Z",
     "iopub.status.busy": "2026-02-15T14:45:32.310842Z",
     "iopub.status.idle": "2026-02-15T14:45:43.412310Z",
     "shell.execute_reply": "2026-02-15T14:45:43.411330Z"
    },
    "papermill": {
     "duration": 11.121955,
     "end_time": "2026-02-15T14:45:43.417185",
     "exception": false,
     "start_time": "2026-02-15T14:45:32.295230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stream a few validation images for inference\n",
    "val_ds = COCOStreamDataset(\n",
    "    load_dataset(\"detection-datasets/coco\", split=\"val\", streaming=True)\n",
    ")\n",
    "val_loader = DataLoader(val_ds, batch_size=4, collate_fn=collate_fn)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_images, val_targets = next(iter(val_loader))\n",
    "    val_images_device = [img.to(device) for img in val_images]\n",
    "    predictions = model(val_images_device)\n",
    "\n",
    "show_predictions(val_images_device, predictions, score_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbff47c9",
   "metadata": {
    "papermill": {
     "duration": 0.025375,
     "end_time": "2026-02-15T14:45:43.468105",
     "exception": false,
     "start_time": "2026-02-15T14:45:43.442730",
     "status": "completed"
    },
    "tags": []
   },
   "source": "## Next steps\n\n- **Scale up training**: increase `NUM_STEPS`, add a learning rate scheduler, and evaluate\n  on the full validation split with mAP metrics.\n- **Try YOLOv11**: explore our YOLOv11 from-scratch notebooks for a different detection architecture built entirely in PyTorch.\n- **Explore HF streaming**: the\n  [Hugging Face datasets streaming guide](https://huggingface.co/docs/datasets/stream)\n  covers advanced features like multi-worker loading, shuffling strategies, and checkpoint\n  resumption with `StatefulDataLoader`."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 150.097043,
   "end_time": "2026-02-15T14:45:44.913637",
   "environment_variables": {},
   "exception": null,
   "input_path": "notebooks/blog/tutorials/hf-coco-streaming.ipynb",
   "output_path": "notebooks/blog/tutorials/hf-coco-streaming-executed.ipynb",
   "parameters": {},
   "start_time": "2026-02-15T14:43:14.816594",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
