{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f71844",
   "metadata": {
    "papermill": {
     "duration": 0.002918,
     "end_time": "2026-02-12T21:11:48.404687",
     "exception": false,
     "start_time": "2026-02-12T21:11:48.401769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "title: SARSA Gridworld Example\n",
    "sidebar: sidebar-rl\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c67c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Colab Environment Setup ---\n",
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    %pip install -q matplotlib seaborn scikit-learn scipy tqdm\n",
    "    print(\"Colab dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab202aa",
   "metadata": {
    "papermill": {
     "duration": 0.0026,
     "end_time": "2026-02-12T21:11:48.410147",
     "exception": false,
     "start_time": "2026-02-12T21:11:48.407547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![sarsa-gridworld](sarsa-gridworld.png)\n",
    "*SARSA Gridworld*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a52d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:11:48.421638Z",
     "iopub.status.busy": "2026-02-12T21:11:48.420805Z",
     "iopub.status.idle": "2026-02-12T21:11:50.100843Z",
     "shell.execute_reply": "2026-02-12T21:11:50.100034Z"
    },
    "papermill": {
     "duration": 1.688485,
     "end_time": "2026-02-12T21:11:50.101966",
     "exception": false,
     "start_time": "2026-02-12T21:11:48.413481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"  # Suppress warnings\n",
    "sys.path.insert(\n",
    "    0,\n",
    "    os.path.join(\n",
    "        os.path.dirname(os.path.abspath(\"__file__\")),\n",
    "        \"notebooks\",\n",
    "        \"reinforcement-learning\",\n",
    "        \"control\",\n",
    "        \"sarsa\",\n",
    "        \"environment\",\n",
    "    ),\n",
    ")\n",
    "from environment import Env\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "\n",
    "    _wandb_ok = bool(os.environ.get(\"WANDB_API_KEY\"))\n",
    "except ImportError:\n",
    "    wandb = None\n",
    "    _wandb_ok = False\n",
    "\n",
    "\n",
    "# SARSA agent learns every time step from the sample <s, a, r, s', a'>\n",
    "class SARSAgent:\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "        self.learning_rate = 0.01\n",
    "        self.discount_factor = 0.9\n",
    "        self.epsilon = 0.1\n",
    "        self.q_table = defaultdict(lambda: [0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "    # with sample <s, a, r, s', a'>, learns new q function\n",
    "    def learn(self, state, action, reward, next_state, next_action):\n",
    "        current_q = self.q_table[state][action]\n",
    "        next_state_q = self.q_table[next_state][next_action]\n",
    "        new_q = current_q + self.learning_rate * (\n",
    "            reward + self.discount_factor * next_state_q - current_q\n",
    "        )\n",
    "        self.q_table[state][action] = new_q\n",
    "\n",
    "    # get action for the state according to the q function table\n",
    "    # agent pick action of epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # take random action\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # take action according to the q function table\n",
    "            state_action = self.q_table[state]\n",
    "            action = self.arg_max(state_action)\n",
    "        return action\n",
    "\n",
    "    @staticmethod\n",
    "    def arg_max(state_action):\n",
    "        max_index_list = []\n",
    "        max_value = state_action[0]\n",
    "        for index, value in enumerate(state_action):\n",
    "            if value > max_value:\n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "            elif value == max_value:\n",
    "                max_index_list.append(index)\n",
    "        return random.choice(max_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b3e195",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:11:50.105094Z",
     "iopub.status.busy": "2026-02-12T21:11:50.104865Z",
     "iopub.status.idle": "2026-02-12T21:13:50.660283Z",
     "shell.execute_reply": "2026-02-12T21:13:50.659659Z"
    },
    "papermill": {
     "duration": 120.557629,
     "end_time": "2026-02-12T21:13:50.660801",
     "exception": false,
     "start_time": "2026-02-12T21:11:50.103172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    agent = SARSAgent(actions=list(range(env.n_actions)))\n",
    "\n",
    "    # Init W&B run\n",
    "    _wb_run = None\n",
    "    if _wandb_ok and wandb is not None:\n",
    "        try:\n",
    "            _wb_run = wandb.init(\n",
    "                settings=wandb.Settings(init_timeout=120),\n",
    "                project=\"eng-ai-agents\",\n",
    "                entity=\"pantelis\",\n",
    "                id=\"train-sarsa-gridworld\",\n",
    "                resume=\"allow\",\n",
    "                name=\"sarsa-gridworld\",\n",
    "                group=\"reinforcement-learning\",\n",
    "                tags=[\"reinforcement-learning\"],\n",
    "                job_type=\"training\",\n",
    "                config={\n",
    "                    \"learning_rate\": agent.learning_rate,\n",
    "                    \"discount_factor\": agent.discount_factor,\n",
    "                    \"epsilon\": agent.epsilon,\n",
    "                    \"episodes\": 10000,\n",
    "                },\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"W&B init failed (non-fatal): {e}\")\n",
    "            _wb_run = None\n",
    "\n",
    "    try:\n",
    "        for episode in tqdm(range(10000), desc=\"Training Episodes\"):\n",
    "            state = env.reset()\n",
    "            action = agent.get_action(str(state))\n",
    "            episode_reward = 0.0\n",
    "\n",
    "            while True:\n",
    "                next_state, reward, done = env.step(action)\n",
    "                next_action = agent.get_action(str(next_state))\n",
    "\n",
    "                agent.learn(str(state), action, reward, str(next_state), next_action)\n",
    "\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Log every 100 episodes\n",
    "            if _wb_run is not None and episode % 100 == 0:\n",
    "                _wb_run.log({\"episode\": episode, \"episode_reward\": episode_reward})\n",
    "    finally:\n",
    "        if _wb_run is not None:\n",
    "            _wb_run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845cf620",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:13:50.665009Z",
     "iopub.status.busy": "2026-02-12T21:13:50.664862Z",
     "iopub.status.idle": "2026-02-12T21:13:50.772386Z",
     "shell.execute_reply": "2026-02-12T21:13:50.771942Z"
    },
    "papermill": {
     "duration": 0.110574,
     "end_time": "2026-02-12T21:13:50.772973",
     "exception": false,
     "start_time": "2026-02-12T21:13:50.662399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_q_values(q_table):\n",
    "    # Create a grid to store the maximum Q-value for each state\n",
    "    q_values_grid = np.zeros((5, 5))\n",
    "\n",
    "    for state, actions in q_table.items():\n",
    "        try:\n",
    "            state_coords = eval(state)  # Convert string back to list\n",
    "            q_values_grid[state_coords[0], state_coords[1]] = max(actions)\n",
    "        except (SyntaxError, TypeError, IndexError):\n",
    "            # Skip invalid states\n",
    "            continue\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(q_values_grid, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "    plt.title(\"Heatmap of Maximum Q-Values for Each State\")\n",
    "    plt.xlabel(\"X Coordinate\")\n",
    "    plt.ylabel(\"Y Coordinate\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_q_values(agent.q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 123.597801,
   "end_time": "2026-02-12T21:13:51.293144",
   "environment_variables": {},
   "exception": null,
   "input_path": "notebooks/reinforcement-learning/control/sarsa/sarsa_gridworld.ipynb",
   "output_path": "notebooks/reinforcement-learning/control/sarsa/sarsa_gridworld-executed.ipynb",
   "parameters": {},
   "start_time": "2026-02-12T21:11:47.695343",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
